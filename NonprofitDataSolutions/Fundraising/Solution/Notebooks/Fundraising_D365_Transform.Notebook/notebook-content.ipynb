{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507d53b-ac97-449c-a9aa-6658e3d4647a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run <Fundraising_D365_Config>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3efff8-9f65-48c6-afd0-4e14ed0cfff9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Initialize Synchronizers\n",
    "\n",
    "Create sync instances once for reuse across multiple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da365c47-59bd-4a9b-9dc2-360102ca917a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optionset synchronizer (used for all optionset syncs)\n",
    "optionset_sync = Dynamics365OptionsetSync(\n",
    "    source_id=source_id,\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name\n",
    ")\n",
    "\n",
    "# Initialize data synchronizer (used for all data table syncs)\n",
    "data_sync = Dynamics365DataSync(\n",
    "    source_id=source_id,\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0ff92-4f7b-4450-8784-1b878a412d44",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "## Constants\n",
    "\n",
    "Business constants and configuration values used throughout the transformations.\n",
    "Centralized to avoid magic numbers and improve maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69228d67-0428-42b5-b2cd-81d7f92e2a82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUSINESS CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "# --- ConstituentType ---\n",
    "CONSTITUENT_TYPE_INDIVIDUAL = \"Individual\"\n",
    "CONSTITUENT_TYPE_ORGANIZATION = \"Organization\"\n",
    "CONSTITUENT_TYPE_HOUSEHOLD = \"Household\"\n",
    "CONSTITUENT_TYPE_SYNTHETIC_ID = \"-2\"  # SourceSystemId for synthetic Individual record\n",
    "\n",
    "# --- Channel Names (exact match required with Silver Channel.Name) ---\n",
    "CHANNEL_EMAIL = \"Email\"\n",
    "CHANNEL_LETTER = \"Letter\"\n",
    "CHANNEL_PHONE_CALL = \"Phone Call\"\n",
    "\n",
    "# --- Activity Party Participation Types (from Dynamics 365 activityparty.participationtypemask) ---\n",
    "# Reference: https://learn.microsoft.com/en-us/dynamics365/customerengagement/on-premises/developer/entities/activityparty\n",
    "PARTICIPATION_TYPE_FROM = 1       # Sender\n",
    "PARTICIPATION_TYPE_TO = 2         # Recipient\n",
    "PARTICIPATION_TYPE_CC = 3         # CC recipient\n",
    "PARTICIPATION_TYPE_BCC = 4        # BCC recipient\n",
    "PARTICIPATION_TYPES_SENDER_RECIPIENT = [PARTICIPATION_TYPE_FROM, PARTICIPATION_TYPE_TO]\n",
    "PARTICIPATION_TYPES_ALL_RECIPIENTS = [PARTICIPATION_TYPE_FROM, PARTICIPATION_TYPE_TO, PARTICIPATION_TYPE_CC, PARTICIPATION_TYPE_BCC]\n",
    "\n",
    "# --- OpportunityStage State Codes (Dynamics 365 opportunity.statecode optionset) ---\n",
    "OPPORTUNITY_STATE_OPEN = 0\n",
    "OPPORTUNITY_STATE_WON = 1\n",
    "OPPORTUNITY_STATE_LOST = 2\n",
    "OPPORTUNITY_STAGE_NAME_OPEN = \"Open\"\n",
    "OPPORTUNITY_STAGE_NAME_WON = \"Won\"\n",
    "OPPORTUNITY_STAGE_NAME_LOST = \"Lost\"\n",
    "\n",
    "# --- Account Type (Dynamics 365 account.msnfp_accounttype optionset) ---\n",
    "ACCOUNT_TYPE_HOUSEHOLD = 844060000  # Household account\n",
    "ACCOUNT_TYPE_ORGANIZATION = 844060001  # Organization account\n",
    "\n",
    "# --- OpportunitySalesProcess State Codes (for workflow deduplication) ---\n",
    "SALES_PROCESS_STATE_ACTIVE = 0  # Active workflow process\n",
    "\n",
    "# --- Data Constraints ---\n",
    "MAX_NAME_LENGTH = 500  # Maximum string length for Name fields (varchar constraint)\n",
    "\n",
    "print(\"✅ Constants loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46430b6-73af-4902-884a-4030e05188be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Optionsets (Dimension Tables)\n",
    "\n",
    "Synchronize Dynamics 365 optionsets to Silver dimension tables using full compare-and-sync (INSERT/UPDATE/DELETE).\n",
    "\n",
    "**Pattern:** Use single `optionset_sync` instance for all optionset operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28325bfc-53bf-4b20-b2b9-82c17ee4e4b8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Define the optionsets to sync\n",
    "# Format: (entity_name, optionset_name, target_table, target_primary_key)\n",
    "optionsets_to_sync = [\n",
    "    (\"campaign\",        \"typecode\",        \"CampaignType\",     \"CampaignTypeId\"),\n",
    "    (\"campaignactivity\",\"channeltypecode\", \"Channel\",          \"ChannelId\"),\n",
    "    (\"contact\",         \"gendercode\",      \"Gender\",           \"GenderId\"),\n",
    "    (\"opportunity\",     \"statecode\",       \"OpportunityStage\", \"OpportunityStageId\"),\n",
    "]\n",
    "\n",
    "# Run the sync\n",
    "for entity, optionset, table, pk in optionsets_to_sync:\n",
    "    try:\n",
    "        print(f\"▶️  {entity}.{optionset} → {table} starting…\")\n",
    "        optionset_sync.sync_optionset(\n",
    "            entity_name=entity,\n",
    "            optionset_name=optionset,\n",
    "            target_table=table,\n",
    "            target_primary_key=pk,\n",
    "            # is_global=False,\n",
    "            # transform_func=None\n",
    "        )\n",
    "        print(f\"✅ Synchronized {entity}.{optionset} → {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {entity}.{optionset} → {table} failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efa857-436d-4699-84fb-2c825084dd0d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "## ConstituentType - Custom Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f4add-9ba2-4b0d-838f-54d313352efd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, lower, trim, when, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def transform_constituenttype(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add synthetic 'Individual' type and deduplicate by name (case-insensitive).\n",
    "    \n",
    "    WHY: Contacts need ConstituentType but don't have msnfp_accounttype optionset.\n",
    "         If Bronze contains real 'Individual', prefer it over synthetic.\n",
    "    \"\"\"\n",
    "    synthetic_individual = df.sparkSession.createDataFrame(\n",
    "        [(CONSTITUENT_TYPE_INDIVIDUAL, CONSTITUENT_TYPE_SYNTHETIC_ID)], \n",
    "        [\"Name\", \"SourceSystemId\"]\n",
    "    )\n",
    "    \n",
    "    unioned = df.unionByName(synthetic_individual, allowMissingColumns=True)\n",
    "    \n",
    "    # Deduplicate: prefer real records over synthetic\n",
    "    window = Window.partitionBy(lower(trim(col(\"Name\")))).orderBy(\n",
    "        when(col(\"SourceSystemId\") == CONSTITUENT_TYPE_SYNTHETIC_ID, lit(1)).otherwise(lit(0)),\n",
    "        col(\"Name\")\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        unioned\n",
    "        .withColumn(\"rank\", row_number().over(window))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .drop(\"rank\")\n",
    "    )\n",
    "\n",
    "optionset_sync.sync_optionset(\n",
    "    entity_name=\"\",\n",
    "    optionset_name=\"msnfp_accounttype\",\n",
    "    target_table=\"ConstituentType\",\n",
    "    target_primary_key=\"ConstituentTypeId\",\n",
    "    is_global=True,\n",
    "    transform_func=transform_constituenttype\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2280dbb-2507-48a4-8578-d626925255fd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Activity Helper Functions\n",
    "\n",
    "Shared helper functions for activity transformations (Letter, Phonecall, Email).\n",
    "These are project-specific business logic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218bf86f-e47c-4f2e-a5c1-6e04eab52bbb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, lower, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def resolve_campaign_for_activity(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve Campaign for activity via regardingobjectid.\n",
    "    \n",
    "    Business logic: regardingobjectid can point directly to campaign,\n",
    "    or to campaignactivity which then points to campaign.\n",
    "    \"\"\"\n",
    "    # Read Bronze campaign activity using framework\n",
    "    campaignactivity = data_sync.reader.read_bronze_table(\n",
    "        \"campaignactivity\",\n",
    "        columns={\n",
    "            \"Id\": \"ca_Id\",\n",
    "            \"regardingobjectid\": \"ca_regardingobjectid\",\n",
    "            \"regardingobjectid_entitytype\": \"ca_regarding_entity\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Read Bronze campaign using framework\n",
    "    campaign_bronze = data_sync.reader.read_bronze_table(\"campaign\", columns={\"Id\": \"c_Id\"})\n",
    "    \n",
    "    # Join campaign activity (not all activities regard campaignactivity)\n",
    "    with_ca = df.join(\n",
    "        campaignactivity,\n",
    "        (lower(col(\"regardingobjectid_entitytype\")) == lit(\"campaignactivity\")) &\n",
    "        (col(\"regardingobjectid\") == col(\"ca_Id\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Join campaign via campaignactivity\n",
    "    with_c = with_ca.join(\n",
    "        campaign_bronze,\n",
    "        (lower(col(\"ca_regarding_entity\")) == lit(\"campaign\")) &\n",
    "        (col(\"ca_regardingobjectid\") == col(\"c_Id\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Determine Campaign SourceSystemId (direct or via campaignactivity)\n",
    "    resolved = with_c.withColumn(\n",
    "        \"CampaignSourceSystemId\",\n",
    "        when(lower(col(\"regardingobjectid_entitytype\")) == \"campaign\", col(\"regardingobjectid\"))\n",
    "        .when(\n",
    "            lower(col(\"regardingobjectid_entitytype\")) == \"campaignactivity\",\n",
    "            when(lower(col(\"ca_regarding_entity\")) == \"campaign\", col(\"c_Id\"))\n",
    "        )\n",
    "    ).drop(\"regardingobjectid\", \"regardingobjectid_entitytype\", \"ca_Id\", \"ca_regardingobjectid\", \"ca_regarding_entity\", \"c_Id\")\n",
    "    \n",
    "    # Resolve Campaign SourceSystemId → Silver CampaignId\n",
    "    campaign_mapping = (\n",
    "        get_silver_table(\"SourceSystemIdMapping\")\n",
    "        .filter((col(\"SourceId\") == lit(data_sync.source_id)) & \n",
    "                (col(\"SourceTable\") == \"Campaign\"))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").alias(\"CampSSId\"),\n",
    "            col(\"SilverRecordId\").alias(\"CampaignId\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return resolved.join(\n",
    "        campaign_mapping,\n",
    "        col(\"CampaignSourceSystemId\").cast(StringType()) == col(\"CampSSId\"),\n",
    "        \"left\"\n",
    "    ).drop(\"CampaignSourceSystemId\", \"CampSSId\")\n",
    "\n",
    "\n",
    "def lookup_channel_by_name(channel_name: str) -> DataFrame:\n",
    "    \"\"\"Get Channel record by name.\"\"\"\n",
    "    return (\n",
    "        get_silver_table(\"Channel\")\n",
    "        .filter(lower(trim(col(\"Name\"))) == lit(channel_name.lower()))\n",
    "        .select(\"ChannelId\")\n",
    "        .limit(1)\n",
    "    )\n",
    "\n",
    "\n",
    "def resolve_activity_party_constituent(df: DataFrame, party_id_col: str, party_type_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve activity party (Contact/Account) to Constituent (project-specific pattern).\n",
    "    \n",
    "    Combines two framework helpers:\n",
    "    1. Polymorphic lookup: partyid + entitytype → ContactId/AccountId\n",
    "    2. Constituent resolution: Contact/Account → ConstituentId\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with party columns\n",
    "        party_id_col: Column name for party ID (e.g., \"partyid\", \"PartySrcId\")\n",
    "        party_type_col: Column name for party entity type (e.g., \"partyid_entitytype\", \"PartyType\")\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ConstituentId column added\n",
    "    \"\"\"\n",
    "    # 1. Resolve polymorphic party (partyid → ContactId/AccountId)\n",
    "    df_with_party = data_sync.resolve_polymorphic_lookup(\n",
    "        df,\n",
    "        lookup_id_column=party_id_col,\n",
    "        entity_type_column=party_type_col,\n",
    "        entity_type_mappings={\"contact\": \"Contact\", \"account\": \"Account\"}\n",
    "    )\n",
    "    \n",
    "    # 2. Resolve Constituent from Contact/Account\n",
    "    return data_sync.resolve_constituent_id(\n",
    "        df_with_party,\n",
    "        contact_fk_column=\"ContactId\",\n",
    "        account_fk_column=\"AccountId\",\n",
    "        output_column=\"ConstituentId\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Activity helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a46d4-9623-442e-bafd-72b1235549a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Data Tables\n",
    "\n",
    "Synchronize Dynamics 365 data tables with watermark-based incremental sync."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7c562",
   "metadata": {},
   "source": [
    "### Transform: Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_sql = f\"\"\"\n",
    "MERGE INTO {silver_lakehouse_name}.Source AS target\n",
    "USING (\n",
    "    SELECT \n",
    "        '{source_id}' AS SourceId,\n",
    "        '{source_name}' AS Name,\n",
    "        current_timestamp() AS CreatedDate,\n",
    "        current_timestamp() AS ModifiedDate\n",
    ") AS source\n",
    "ON target.SourceId = source.SourceId\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    SourceId, CreatedDate, ModifiedDate, Name\n",
    ") VALUES (\n",
    "    source.SourceId, source.CreatedDate, source.ModifiedDate, source.Name\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(merge_sql)\n",
    "row = result.collect()[0]\n",
    "\n",
    "logging.info(f\"✅ Rows processed {row['num_affected_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd8f3e-96fe-4287-a7e7-966732b12313",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4620c-01ae-4b79-9b82-01162f502ecb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform_country(df):\n",
    "    \"\"\"Transform customeraddress.country to Country. Deduplicate by name (case-insensitive).\"\"\"\n",
    "    return (\n",
    "        D365TransformHelpers.deduplicate_by_window(\n",
    "            df.filter(F.col(\"country\").isNotNull() & (F.trim(F.col(\"country\")) != \"\")),\n",
    "            partition_by=F.upper(F.trim(F.col(\"country\"))),\n",
    "            order_by=\"customeraddressid\"\n",
    "        )\n",
    "        .select(\n",
    "            F.trim(F.col(\"country\")).alias(\"Name\"),\n",
    "            F.current_timestamp().alias(\"CreatedDate\"),\n",
    "            F.current_timestamp().alias(\"ModifiedDate\"),\n",
    "            F.col(\"customeraddressid\").cast(\"string\").alias(\"SourceSystemId\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Custom MERGE SQL - match by name (case-insensitive)\n",
    "country_merge_sql = \"\"\"\n",
    "MERGE INTO {TARGET_TABLE} AS target\n",
    "USING {SOURCE_VIEW} AS source\n",
    "ON upper(target.Name) = upper(source.Name)\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (\n",
    "        CountryId, CreatedDate, ModifiedDate, Name, SourceId, SourceSystemId\n",
    "    )\n",
    "    VALUES (\n",
    "        source.CountryId, source.CreatedDate, source.ModifiedDate,\n",
    "        source.Name, source.SourceId, source.SourceSystemId\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"customeraddress\",\n",
    "    source_primary_key=\"customeraddressid\",\n",
    "    source_columns=[\"country\"],\n",
    "    target_table=\"Country\",\n",
    "    target_primary_key=\"CountryId\",\n",
    "    transform_func=transform_country,\n",
    "    merge_sql=country_merge_sql\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c3084-ed72-48eb-b953-9aee623112a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: OpportunityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222f274-cd7f-42a8-ab2f-2e0c7f8eac22",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "def transform_opportunitytype(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform opportunitysalesprocess to OpportunityType.\n",
    "    \n",
    "    Why workflow join: Dynamics 365 stores workflow process definition in 'workflow' table.\n",
    "    OpportunityType.Name comes from workflow.name (human-readable label).\n",
    "    \"\"\"\n",
    "    # Get workflow names (WHY: processid is just GUID, we need human-readable name)\n",
    "    workflow = data_sync.reader.read_bronze_table(\n",
    "        table_name=\"workflow\",\n",
    "        columns={\n",
    "            \"workflowid\": \"wf_id\",\n",
    "            \"name\": (\"wf_name\", \"string\")  # auto-trims\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Join to resolve Name via processid → workflowid\n",
    "    return (\n",
    "        df.join(workflow, col(\"processid\") == col(\"wf_id\"), \"left\")\n",
    "          .select(\n",
    "              col(\"processid\"),  # Framework uses this for SourceSystemId\n",
    "              col(\"wf_name\").alias(\"Name\"),\n",
    "              current_timestamp().alias(\"CreatedDate\"),\n",
    "              current_timestamp().alias(\"ModifiedDate\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"opportunitysalesprocess\",\n",
    "    source_primary_key=\"processid\",\n",
    "    source_columns=[\"processid\"],\n",
    "    target_table=\"OpportunityType\",\n",
    "    target_primary_key=\"OpportunityTypeId\",\n",
    "    transform_func=transform_opportunitytype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f14fb-e74e-4936-b460-88465ab2eb25",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5a3df-0f85-47b6-85f0-81bb058f59b3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def transform_campaign(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transform campaign to Silver. FK: CampaignType via typecode.\"\"\"\n",
    "    return df.select(\n",
    "        col(\"Id\"),\n",
    "        col(\"typecode_SilverRecordId\").alias(\"CampaignTypeId\"),\n",
    "        col(\"totalactualcost_base\").alias(\"Cost\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"actualend\").cast(TimestampType()).alias(\"EndDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "        col(\"name\").alias(\"Name\"),\n",
    "        col(\"actualstart\").cast(TimestampType()).alias(\"StartDate\")\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"campaign\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"typecode\", \"totalactualcost_base\",\n",
    "        \"createdon\", \"actualend\", \"modifiedon\", \"name\", \"actualstart\"\n",
    "    ],\n",
    "    target_table=\"Campaign\",\n",
    "    target_primary_key=\"CampaignId\",\n",
    "    transform_func=transform_campaign,\n",
    "    fk_mappings={\n",
    "        \"typecode\": \"CampaignType\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728e901-6e08-4be4-b8f7-d27fb27e2ff3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7ab78-77ab-4529-a79a-03db927a7ab6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Create in-memory view\n",
    "# WHY: Addresses exist in both contact and account - union them without writing to disk\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW address_stage_union AS\n",
    "SELECT \n",
    "    address1_city, address1_country, address1_latitude, address1_longitude,\n",
    "    address1_stateorprovince, address1_postalcode, address1_addressid,\n",
    "    createdon, modifiedon, IsDelete, SinkModifiedOn\n",
    "FROM {bronze_lakehouse_name}.contact\n",
    "WHERE address1_addressid IS NOT NULL\n",
    "UNION ALL\n",
    "SELECT \n",
    "    address1_city, address1_country, address1_latitude, address1_longitude,\n",
    "    address1_stateorprovince, address1_postalcode, address1_addressid,\n",
    "    createdon, modifiedon, IsDelete, SinkModifiedOn\n",
    "FROM {bronze_lakehouse_name}.account\n",
    "WHERE address1_addressid IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Temporary view created: address_stage_union (in-memory union)\")\n",
    "\n",
    "\n",
    "def transform_address(df):\n",
    "    \"\"\"Transform Address with Region (from customeraddress optionset) and CountryId (from Silver Country).\"\"\"\n",
    "    # Region lookup from customeraddress (WHY: msnfp_region is stored separately)\n",
    "    ca = data_sync.reader.read_bronze_table(\n",
    "        table_name=\"customeraddress\",\n",
    "        columns={\"Id\": \"ca_id\", \"msnfp_region\": \"RegionCode\"}\n",
    "    )\n",
    "    \n",
    "    # Region optionset labels (msnfp_region code → label)\n",
    "    # WHY: msnfp_region is a local optionset (integer codes), need human-readable labels\n",
    "    region_labels = (\n",
    "        optionset_sync._read_bronze_options(\n",
    "            entity_name=\"customeraddress\",\n",
    "            optionset_name=\"msnfp_region\",\n",
    "            is_global=False\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"Option\").alias(\"RegionCode\"),\n",
    "            F.col(\"Name\").alias(\"Region\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Country lookup from Silver (source-scoped)\n",
    "    country = (\n",
    "        get_silver_table(\"Country\")\n",
    "        .filter(F.col(\"SourceId\") == F.lit(source_id))\n",
    "        .select(\"CountryId\", F.col(\"Name\").alias(\"CountryName\"))\n",
    "    )\n",
    "    \n",
    "    # Base selection + Region joins\n",
    "    base = (\n",
    "        df.select(\n",
    "            F.col(\"address1_city\").alias(\"City\"),\n",
    "            F.col(\"address1_country\").alias(\"CountryName\"),\n",
    "            F.col(\"address1_latitude\").alias(\"Latitude\"),\n",
    "            F.col(\"address1_longitude\").alias(\"Longitude\"),\n",
    "            F.col(\"address1_stateorprovince\").alias(\"State\"),\n",
    "            F.col(\"address1_postalcode\").alias(\"ZipCode\"),\n",
    "            F.col(\"address1_addressid\").alias(\"SourceSystemId\"),\n",
    "            F.col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "            F.col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\")\n",
    "        )\n",
    "        .join(ca, F.col(\"SourceSystemId\") == F.col(\"ca_id\"), \"left\")\n",
    "        .drop(\"ca_id\")\n",
    "        .join(region_labels, on=\"RegionCode\", how=\"left\")\n",
    "        .drop(\"RegionCode\")\n",
    "    )\n",
    "    \n",
    "    # Case-insensitive Country join using helper\n",
    "    result = D365TransformHelpers.join_case_insensitive(\n",
    "        base, country,\n",
    "        left_column=\"CountryName\",\n",
    "        right_column=\"CountryName\",\n",
    "        how=\"left\"\n",
    "    ).drop(\"CountryName\")\n",
    "    \n",
    "    return result.select(\n",
    "        F.col(\"City\"),\n",
    "        F.col(\"CountryId\"),\n",
    "        F.col(\"Latitude\"),\n",
    "        F.col(\"Longitude\"),\n",
    "        F.col(\"Region\"),\n",
    "        F.col(\"State\"),\n",
    "        F.col(\"ZipCode\"),\n",
    "        F.col(\"CreatedDate\"),\n",
    "        F.col(\"ModifiedDate\"),\n",
    "        F.col(\"SourceSystemId\")\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"address_stage_union\",\n",
    "    source_primary_key=\"address1_addressid\",\n",
    "    source_columns=[\n",
    "        \"address1_city\", \"address1_country\", \"address1_latitude\", \"address1_longitude\",\n",
    "        \"address1_stateorprovince\", \"address1_postalcode\", \"createdon\", \"modifiedon\"\n",
    "    ],\n",
    "    target_table=\"Address\",\n",
    "    target_primary_key=\"AddressId\",\n",
    "    transform_func=transform_address,\n",
    "    source_table_lakehouse=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144f4b8-d72d-4eed-9eb9-fd264dd656d5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: CampaignChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8dc8a9-bf02-49c7-9817-0a0ff59b9c3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, lower, expr\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def _load_channel_metadata() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load optionset metadata mapping channeltypecode to human-readable labels.\n",
    "    \n",
    "    WHY: Dynamics 365 stores campaignactivity.channeltypecode as integer option values.\n",
    "         We need the localized label (e.g., \"Email\", \"Phone Call\", \"Letter\") \n",
    "         to match against Channel.Name dimension table.\n",
    "    \n",
    "    Business context: channeltypecode is NOT the same as Campaign.typecode!\n",
    "         - Campaign.typecode = Type of campaign (e.g., \"Email Campaign\")\n",
    "         - campaignactivity.channeltypecode = Channel used in activity (e.g., \"Email\")\n",
    "    \n",
    "    Framework usage: Uses optionset_sync._read_bronze_options() for consistent\n",
    "         optionset handling (language selection, null filtering, deduplication).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: typecode_opt (int), OptChannelName (string)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        optionset_sync._read_bronze_options(\n",
    "            entity_name=\"campaignactivity\",\n",
    "            optionset_name=\"channeltypecode\",\n",
    "            is_global=False\n",
    "        )\n",
    "        .select(\n",
    "            col(\"Option\").cast(\"bigint\").alias(\"typecode_opt\"),\n",
    "            col(\"Name\").alias(\"OptChannelName\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def _filter_campaign_activities(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter to only activities where regardingobjectid points to a campaign.\n",
    "    \n",
    "    WHY: campaignactivity records can have regardingobjectid pointing to different\n",
    "         entity types (campaign, lead, opportunity, etc.). We only want activities\n",
    "         that are explicitly linked to campaigns (regardingobjectid_entitytype = \"campaign\").\n",
    "    \n",
    "    Business rule: Activity must have:\n",
    "         - regardingobjectid pointing to a campaign (not lead/opportunity/etc.)\n",
    "         - channeltypecode indicating which channel was used\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: ActivityId, CampaignSrcId, ChannelTypeCode\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.filter(lower(col(\"regardingobjectid_entitytype\")) == lit(\"campaign\"))\n",
    "          .select(\n",
    "              col(\"activityid\").cast(StringType()).alias(\"ActivityId\"),\n",
    "              col(\"regardingobjectid\").cast(StringType()).alias(\"CampaignSrcId\"),\n",
    "              col(\"channeltypecode\").cast(\"bigint\").alias(\"ChannelTypeCode\")\n",
    "          )\n",
    "          .filter(col(\"CampaignSrcId\").isNotNull() & col(\"ActivityId\").isNotNull())\n",
    "    )\n",
    "\n",
    "\n",
    "def _enrich_channel_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add human-readable channel names to activities via optionset lookup.\n",
    "    \n",
    "    WHY: channeltypecode is just an integer (e.g., 1, 2, 3).\n",
    "         We need the label (e.g., \"Email\", \"Phone Call\") to match against\n",
    "         Channel.Name in the dimension table.\n",
    "    \n",
    "    Business logic: Maps channeltypecode → LocalizedLabel from OptionsetMetadata\n",
    "    \n",
    "    Returns:\n",
    "        Original DataFrame + OptChannelName column\n",
    "    \"\"\"\n",
    "    optionset = _load_channel_metadata()\n",
    "    \n",
    "    return df.join(\n",
    "        optionset,\n",
    "        col(\"ChannelTypeCode\") == col(\"typecode_opt\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"ActivityId\", \"CampaignSrcId\", \"ChannelTypeCode\", \"OptChannelName\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _resolve_campaign_ids(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Translate Bronze campaign IDs to Silver CampaignId GUIDs.\n",
    "    \n",
    "    WHY: campaignactivity.regardingobjectid contains Bronze campaign IDs (source-specific).\n",
    "         We need Silver CampaignId GUIDs to create foreign key relationship.\n",
    "    \n",
    "    Uses: Silver Campaign table filtered by SourceId (this data source only)\n",
    "    Lookup: CampaignSrcId (Bronze) → CampaignId (Silver GUID)\n",
    "    \n",
    "    Returns:\n",
    "        Original DataFrame + CampaignId column\n",
    "    \"\"\"\n",
    "    campaigns = (\n",
    "        get_silver_table(\"Campaign\")\n",
    "        .filter(col(\"SourceId\") == lit(data_sync.source_id))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").cast(StringType()).alias(\"CampSSId\"),\n",
    "            col(\"CampaignId\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df.join(\n",
    "        campaigns,\n",
    "        col(\"CampaignSrcId\") == col(\"CampSSId\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"ActivityId\", \"CampaignSrcId\", \"ChannelTypeCode\", \"OptChannelName\", \"CampaignId\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _resolve_channel_ids(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Map channel name labels to ChannelId dimension keys.\n",
    "    \n",
    "    WHY: Channel is a shared dimension table (synced from channeltypecode optionset).\n",
    "         We need to match OptChannelName → Channel.Name → ChannelId.\n",
    "    \n",
    "    Business context: Channel dimension is shared across all data sources.\n",
    "         Names like \"Email\", \"Phone Call\", \"Letter\" are standardized.\n",
    "    \n",
    "    Returns:\n",
    "        Original DataFrame + ChannelId column\n",
    "    \"\"\"\n",
    "    channels = get_silver_table(\"Channel\").select(\n",
    "        col(\"Name\").alias(\"ChannelName\"),\n",
    "        col(\"ChannelId\")\n",
    "    )\n",
    "    \n",
    "    return df.join(\n",
    "        channels,\n",
    "        col(\"OptChannelName\") == col(\"ChannelName\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        \"ActivityId\", \"CampaignSrcId\", \"ChannelTypeCode\", \"CampaignId\", \"ChannelId\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _build_campaign_channel_pairs(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Orchestrate link table creation: filter → enrich → resolve → deduplicate.\n",
    "    \n",
    "    WHY: CampaignChannel is a many-to-many link table (Campaign ↔ Channel).\n",
    "         Multiple activities can create the same (CampaignId, ChannelId) pair.\n",
    "         We need exactly ONE link record per unique pair.\n",
    "    \n",
    "    Business logic flow:\n",
    "         1. Filter to campaign-related activities\n",
    "         2. Enrich with channel names (optionset lookup)\n",
    "         3. Resolve Campaign IDs (Bronze → Silver)\n",
    "         4. Resolve Channel IDs (name → dimension)\n",
    "         5. Deduplicate by (CampaignId, ChannelId) pair\n",
    "    \n",
    "    SourceSystemId strategy: Keep ActivityId for traceability\n",
    "         (tracks which activity created the pair, useful for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: CampaignId, ChannelId, ActivityId\n",
    "    \"\"\"\n",
    "    # Step 1: Filter to campaign activities\n",
    "    filtered = _filter_campaign_activities(df)\n",
    "    \n",
    "    # Step 2: Enrich with channel names\n",
    "    with_names = _enrich_channel_names(filtered)\n",
    "    \n",
    "    # Step 3: Resolve Campaign IDs\n",
    "    with_campaign = _resolve_campaign_ids(with_names)\n",
    "    \n",
    "    # Step 4: Resolve Channel IDs\n",
    "    with_channel = _resolve_channel_ids(with_campaign)\n",
    "    \n",
    "    # Step 5: Filter out incomplete resolutions and deduplicate\n",
    "    return (\n",
    "        with_channel\n",
    "        .filter(col(\"CampaignId\").isNotNull() & col(\"ChannelId\").isNotNull())\n",
    "        .dropDuplicates([\"CampaignId\", \"ChannelId\", \"ActivityId\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_campaign_channel(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform campaignactivity → CampaignChannel link table.\n",
    "    \n",
    "    Source: campaignactivity (Bronze table tracking activities performed for campaigns)\n",
    "    Target: CampaignChannel (Silver link table: Campaign ↔ Channel many-to-many)\n",
    "    \n",
    "    Business logic:\n",
    "         Each campaign activity has a channeltypecode indicating which channel was used\n",
    "         (e.g., Email, Phone, Letter). This creates a relationship between the campaign\n",
    "         and that channel. Multiple activities can use the same channel for the same\n",
    "         campaign → deduplicate to one link record per (Campaign, Channel) pair.\n",
    "    \n",
    "    WHY campaignactivity (not Campaign)?\n",
    "         - Campaign.typecode = campaign TYPE (e.g., \"Email Campaign\")\n",
    "         - campaignactivity.channeltypecode = actual CHANNEL used (e.g., \"Email\")\n",
    "         - A \"Multi-Channel Campaign\" might use Email, Phone, and Letter channels\n",
    "         - We derive this from actual activities, not from campaign metadata\n",
    "    \n",
    "    SourceSystemId = ActivityId:\n",
    "         - Tracks which activity created the pair (traceability)\n",
    "         - Framework uses this for SourceSystemIdMapping tracking\n",
    "         - Multiple activities can create same pair → dedupe keeps one ActivityId\n",
    "    \"\"\"\n",
    "    pairs = _build_campaign_channel_pairs(df)\n",
    "    \n",
    "    return (\n",
    "        pairs\n",
    "        .withColumn(\"CampaignChannelId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"SourceSystemId\", col(\"ActivityId\"))\n",
    "        .select(\"CampaignChannelId\", \"ChannelId\", \"CampaignId\", \"SourceSystemId\")\n",
    "    )\n",
    "\n",
    "\n",
    "merge_sql_campaign_channel = f\"\"\"\n",
    "MERGE INTO {data_sync.target_lakehouse}.CampaignChannel AS target\n",
    "USING {{SOURCE_VIEW}} AS source\n",
    "  ON  target.CampaignId = source.CampaignId\n",
    " AND  target.ChannelId  = source.ChannelId\n",
    "WHEN NOT MATCHED THEN INSERT (CampaignChannelId, ChannelId, CampaignId)\n",
    "VALUES (source.CampaignChannelId, source.ChannelId, source.CampaignId)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _cleanup_campaign_channel_links():\n",
    "    \"\"\"\n",
    "    Remove stale (CampaignId, ChannelId) pairs that no longer exist in Bronze.\n",
    "    \n",
    "    WHY cleanup is necessary for derived link tables:\n",
    "         CampaignChannel is DERIVED from campaignactivity (not a primary entity).\n",
    "         Framework MERGE matches on composite key (CampaignId, ChannelId), but tracks\n",
    "         individual ActivityId via SourceSystemIdMapping.\n",
    "    \n",
    "    Problem scenarios requiring cleanup:\n",
    "         1. Activity changes channeltypecode: Email → Phone\n",
    "            - Framework inserts new pair (Campaign, Phone)\n",
    "            - Old pair (Campaign, Email) remains stale if no other activities use Email\n",
    "         \n",
    "         2. Activity deleted (IsDelete=1)\n",
    "            - Framework deletes record with that ActivityId\n",
    "            - But if multiple activities created same pair, pair should remain!\n",
    "            - Cleanup determines if pair is TRULY stale (no activities support it)\n",
    "         \n",
    "         3. Activity changes regardingobjectid: Campaign1 → Campaign2\n",
    "            - Creates pair for Campaign2\n",
    "            - Pair for Campaign1 becomes stale if no other activities support it\n",
    "    \n",
    "    Pattern: left_anti join\n",
    "         expected_pairs = rebuild from current Bronze state\n",
    "         stale_pairs = (existing Silver pairs) - (expected pairs)\n",
    "         DELETE stale_pairs\n",
    "    \n",
    "    NOTE: This pattern is needed because:\n",
    "         - Multiple ActivityIds can produce the same (CampaignId, ChannelId) pair\n",
    "         - Framework SourceSystemIdMapping tracks per-ActivityId, not per-pair\n",
    "         - Composite key MERGE doesn't automatically clean up stale pairs\n",
    "    \"\"\"\n",
    "    # Rebuild expected pairs from current Bronze state\n",
    "    expected = _build_campaign_channel_pairs(\n",
    "        data_sync.reader.read_bronze_table(\n",
    "            \"campaignactivity\",\n",
    "            columns=[\n",
    "                \"activityid\",\n",
    "                \"regardingobjectid\",\n",
    "                \"regardingobjectid_entitytype\",\n",
    "                \"channeltypecode\"\n",
    "            ]\n",
    "        )\n",
    "    ).select(\"CampaignId\", \"ChannelId\").dropDuplicates()\n",
    "    \n",
    "    # Get campaigns owned by this source\n",
    "    my_campaigns = (\n",
    "        get_silver_table(\"Campaign\")\n",
    "        .filter(col(\"SourceId\") == lit(data_sync.source_id))\n",
    "        .select(\"CampaignId\")\n",
    "    )\n",
    "    \n",
    "    # Get existing pairs in Silver for this source's campaigns\n",
    "    target_pairs = (\n",
    "        get_silver_table(\"CampaignChannel\").alias(\"t\")\n",
    "        .join(my_campaigns.alias(\"c\"), col(\"t.CampaignId\") == col(\"c.CampaignId\"), \"inner\")\n",
    "        .select(\"t.CampaignId\", \"t.ChannelId\")\n",
    "    )\n",
    "    \n",
    "    # Find stale pairs: in target but not in expected\n",
    "    stale = (\n",
    "        target_pairs.alias(\"t\")\n",
    "        .join(\n",
    "            expected.alias(\"e\"),\n",
    "            (col(\"t.CampaignId\") == col(\"e.CampaignId\")) &\n",
    "            (col(\"t.ChannelId\") == col(\"e.ChannelId\")),\n",
    "            \"left_anti\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Delete stale pairs\n",
    "    stale.createOrReplaceTempView(\"_stale_campaign_channel\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {data_sync.target_lakehouse}.CampaignChannel AS t\n",
    "        USING _stale_campaign_channel AS s\n",
    "          ON  t.CampaignId = s.CampaignId\n",
    "         AND  t.ChannelId  = s.ChannelId\n",
    "        WHEN MATCHED THEN DELETE\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.catalog.dropTempView(\"_stale_campaign_channel\")\n",
    "\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"campaignactivity\",\n",
    "    source_primary_key=\"activityid\",\n",
    "    source_columns=[\"activityid\", \"regardingobjectid\", \"regardingobjectid_entitytype\", \"channeltypecode\"],\n",
    "    target_table=\"CampaignChannel\",\n",
    "    target_primary_key=\"CampaignChannelId\",\n",
    "    transform_func=transform_campaign_channel,\n",
    "    merge_sql=merge_sql_campaign_channel\n",
    ")\n",
    "\n",
    "_cleanup_campaign_channel_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0617c87-e133-4712-b314-fc93f0dd5ada",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: EmailEngagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e052fe-3278-4284-8360-f2938cbbb44c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "\n",
    "def transform_emailengagement(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform email → EmailEngagement.\n",
    "    \n",
    "    WHY: Track email campaign activities. Resolve Campaign via regardingobjectid \n",
    "         (direct link to campaign OR indirect via campaignactivity → campaign).\n",
    "         Email channel assignment for all email activities.\n",
    "    \"\"\"\n",
    "    # Base email attributes with regardingobjectid for campaign resolution\n",
    "    base = df.select(\n",
    "        col(\"Id\").alias(\"EmailId\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "        col(\"senton\").cast(TimestampType()).alias(\"SendDate\"),\n",
    "        col(\"subject\").alias(\"Subject\"),\n",
    "        col(\"versionnumber\").alias(\"VariantType\"),\n",
    "        col(\"regardingobjectid\"),\n",
    "        col(\"regardingobjectid_entitytype\")\n",
    "    )\n",
    "    \n",
    "    # Resolve Campaign using shared helper (handles direct + indirect resolution)\n",
    "    with_campaign = resolve_campaign_for_activity(base)\n",
    "    \n",
    "    # Attach Email channel using shared helper\n",
    "    email_channel = lookup_channel_by_name(CHANNEL_EMAIL)\n",
    "    with_channel = with_campaign.crossJoin(email_channel)\n",
    "    \n",
    "    return (\n",
    "        with_channel.select(\n",
    "            col(\"EmailId\").alias(\"Id\"),  # Framework expects \"Id\" for SourceSystemId\n",
    "            col(\"CampaignId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"EmailId\"),\n",
    "            col(\"ModifiedDate\"),\n",
    "            col(\"SendDate\"),\n",
    "            col(\"Subject\"),\n",
    "            col(\"VariantType\"),\n",
    "            col(\"ChannelId\")\n",
    "        )\n",
    "        .dropDuplicates([\"Id\"])\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"email\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\",\n",
    "        \"createdon\", \"modifiedon\", \"senton\",\n",
    "        \"subject\", \"versionnumber\",\n",
    "        \"regardingobjectid\", \"regardingobjectid_entitytype\",\n",
    "        \"SinkModifiedOn\"\n",
    "    ],\n",
    "    target_table=\"EmailEngagement\",\n",
    "    target_primary_key=\"EmailEngagementId\",\n",
    "    transform_func=transform_emailengagement\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8cac3-48d7-4b5b-91fb-9ccf672021cf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cecbf-9713-404d-8fc8-25349573e61f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def transform_account(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transform account to Silver. FK: Address via address1_addressid.\"\"\"\n",
    "    return df.select(\n",
    "        col(\"Id\"),\n",
    "        col(\"address1_addressid_SilverRecordId\").alias(\"AddressId\"),\n",
    "        col(\"name\").alias(\"Name\"),\n",
    "        col(\"emailaddress1\").alias(\"Email\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\")\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"account\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"name\", \"emailaddress1\", \"address1_addressid\",\n",
    "        \"createdon\", \"modifiedon\"\n",
    "    ],\n",
    "    target_table=\"Account\",\n",
    "    target_primary_key=\"AccountId\",\n",
    "    transform_func=transform_account,\n",
    "    fk_mappings={\n",
    "        \"address1_addressid\": \"Address\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac06bd4-4841-4a6f-9474-fa606306993e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0389a6-c450-4cc4-9335-353d2173fc3d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def transform_contact(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transform contact to Silver. FKs: Gender, Address.\"\"\"\n",
    "    return df.select(\n",
    "        col(\"Id\"),\n",
    "        col(\"gendercode_SilverRecordId\").alias(\"GenderId\"),\n",
    "        col(\"address1_addressid_SilverRecordId\").alias(\"AddressId\"),\n",
    "        col(\"firstname\").alias(\"FirstName\"),\n",
    "        col(\"lastname\").alias(\"LastName\"),\n",
    "        col(\"emailaddress1\").alias(\"Email\"),\n",
    "        col(\"birthdate\").cast(\"date\").alias(\"BirthDate\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\")\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"contact\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"firstname\", \"lastname\", \"emailaddress1\", \"birthdate\",\n",
    "        \"gendercode\", \"address1_addressid\", \"createdon\", \"modifiedon\"\n",
    "    ],\n",
    "    target_table=\"Contact\",\n",
    "    target_primary_key=\"ContactId\",\n",
    "    transform_func=transform_contact,\n",
    "    fk_mappings={\n",
    "        \"gendercode\": \"Gender\",\n",
    "        \"address1_addressid\": \"Address\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d9451-eb4d-422d-ac3e-7fcd4b142880",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Constituent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11c105-b0b9-4769-b841-b4ac19158861",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "def transform_constituent_from_contact(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transform contact to Constituent (Individual type).\"\"\"\n",
    "    individual_type = (\n",
    "        get_silver_table(\"ConstituentType\")\n",
    "        .filter(col(\"Name\") == CONSTITUENT_TYPE_INDIVIDUAL)\n",
    "        .select(\"ConstituentTypeId\")\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    if not individual_type:\n",
    "        raise ValueError(f\"ConstituentType '{CONSTITUENT_TYPE_INDIVIDUAL}' not found\")\n",
    "\n",
    "    contact_mapping = (\n",
    "        get_silver_table(\"SourceSystemIdMapping\")\n",
    "        .filter((col(\"SourceId\") == lit(source_id)) & \n",
    "                (col(\"SourceTable\") == \"Contact\"))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").alias(\"ContactSourceId\"),\n",
    "            col(\"SilverRecordId\").alias(\"ContactId\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df.join(contact_mapping, col(\"Id\") == col(\"ContactSourceId\"), \"inner\")\n",
    "          .select(\n",
    "              col(\"Id\"),\n",
    "              col(\"ContactId\"),\n",
    "              lit(None).cast(\"string\").alias(\"AccountId\"),\n",
    "              lit(individual_type[\"ConstituentTypeId\"]).alias(\"ConstituentTypeId\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "# Custom MERGE SQL - matches on (ContactId, AccountId) composite key with NULL-safe comparison\n",
    "constituent_merge_sql = \"\"\"\n",
    "MERGE INTO {TARGET_TABLE} AS target\n",
    "USING {SOURCE_VIEW} AS source\n",
    "ON target.ContactId <=> source.ContactId\n",
    "   AND target.AccountId <=> source.AccountId\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET target.ConstituentTypeId = source.ConstituentTypeId\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (ConstituentId, ContactId, AccountId, ConstituentTypeId)\n",
    "    VALUES (source.ConstituentId, source.ContactId, source.AccountId, source.ConstituentTypeId)\n",
    "\"\"\"\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"contact\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\"Id\"],\n",
    "    target_table=\"Constituent\",\n",
    "    target_primary_key=\"ConstituentId\",\n",
    "    transform_func=transform_constituent_from_contact,\n",
    "    merge_sql=constituent_merge_sql\n",
    ")\n",
    "\n",
    "def transform_constituent_from_account(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform account to Constituent (Org/Household type based on msnfp_accounttype).\n",
    "    \"\"\"\n",
    "    constituent_types = (\n",
    "        get_silver_table(\"ConstituentType\")\n",
    "        .select(\"Name\", \"ConstituentTypeId\")\n",
    "    )\n",
    "\n",
    "    type_map = {r[\"Name\"]: r[\"ConstituentTypeId\"] for r in constituent_types.collect()}\n",
    "    org_type_id = type_map.get(CONSTITUENT_TYPE_ORGANIZATION)\n",
    "    household_type_id = type_map.get(CONSTITUENT_TYPE_HOUSEHOLD)\n",
    "\n",
    "    if not org_type_id or not household_type_id:\n",
    "        raise ValueError(f\"ConstituentType '{CONSTITUENT_TYPE_ORGANIZATION}' or '{CONSTITUENT_TYPE_HOUSEHOLD}' not found\")\n",
    "\n",
    "    account_mapping = (\n",
    "        get_silver_table(\"SourceSystemIdMapping\")\n",
    "        .filter((col(\"SourceId\") == lit(source_id)) & \n",
    "                (col(\"SourceTable\") == \"Account\"))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").alias(\"AccountSourceId\"),\n",
    "            col(\"SilverRecordId\").alias(\"AccountId\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df.join(account_mapping, col(\"Id\") == col(\"AccountSourceId\"), \"inner\")\n",
    "          .select(\n",
    "              col(\"Id\"),\n",
    "              lit(None).cast(\"string\").alias(\"ContactId\"),\n",
    "              col(\"AccountId\"),\n",
    "              when(col(\"msnfp_accounttype\") == ACCOUNT_TYPE_ORGANIZATION, lit(org_type_id))\n",
    "              .when(col(\"msnfp_accounttype\") == ACCOUNT_TYPE_HOUSEHOLD, lit(household_type_id))\n",
    "              .alias(\"ConstituentTypeId\")\n",
    "          )\n",
    "          .filter(col(\"ConstituentTypeId\").isNotNull())\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"account\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\"Id\", \"msnfp_accounttype\"],\n",
    "    target_table=\"Constituent\",\n",
    "    target_primary_key=\"ConstituentId\",\n",
    "    transform_func=transform_constituent_from_account,\n",
    "    merge_sql=constituent_merge_sql\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7adbd-8e72-408d-8a10-4650eea6380c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: ConstituentEmailEngagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53a94d-3669-4f9b-8028-2419533b7e98",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, expr, lit, lower\n",
    "from pyspark.sql.types import StringType, TimestampType, BooleanType\n",
    "\n",
    "def _resolve_email_engagement_link(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Link ConstituentEmailEngagement to parent EmailEngagement record.\n",
    "    \n",
    "    WHY: EmailEngagement (email activity) is parent; ConstituentEmailEngagement tracks per-recipient.\n",
    "         Need EmailEngagementId (Silver GUID) and EmailId (from parent EmailEngagement).\n",
    "    \"\"\"\n",
    "    mapping = (\n",
    "        get_silver_table(\"SourceSystemIdMapping\")\n",
    "        .filter(col(\"SourceId\") == lit(data_sync.source_id))\n",
    "        .filter(col(\"SourceTable\") == lit(\"EmailEngagement\"))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").cast(StringType()).alias(\"_EmailActivityId\"),\n",
    "            col(\"SilverRecordId\").alias(\"EmailEngagementId\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    email_engagement = (\n",
    "        get_silver_table(\"EmailEngagement\")\n",
    "        .select(col(\"EmailEngagementId\"), col(\"EmailId\"))\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        df.join(mapping, df[\"EmailActivityId\"] == col(\"_EmailActivityId\"), \"left\")\n",
    "        .drop(\"_EmailActivityId\")\n",
    "        .join(email_engagement, on=\"EmailEngagementId\", how=\"left\")\n",
    "    )\n",
    "\n",
    "def transform_constituent_email_engagement(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform email activity → ConstituentEmailEngagement (per-recipient engagement tracking).\n",
    "    \n",
    "    WHY: One email can have multiple recipients (TO/CC/BCC). Track engagement per recipient.\n",
    "         SourceSystemId = ActivityPartyId (unique per email-person-role combination).\n",
    "         addressused field captures actual email address used (may differ from contact's primary).\n",
    "    \"\"\"\n",
    "    # Extract email engagement metrics\n",
    "    email_base = df.select(\n",
    "        col(\"Id\").alias(\"EmailActivityId\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "        col(\"senton\").cast(TimestampType()).alias(\"SendDate\"),\n",
    "        col(\"subject\").alias(\"EmailSubject\"),\n",
    "        (col(\"lastopenedtime\").isNotNull()).cast(BooleanType()).alias(\"WasOpened\"),\n",
    "        col(\"lastopenedtime\").cast(TimestampType()).alias(\"OpenedDate\"),\n",
    "        (col(\"linksclickedcount\") > 0).cast(BooleanType()).alias(\"ClickThrough\"),\n",
    "        expr(\"CASE WHEN linksclickedcount > 0 THEN cast(modifiedon as timestamp) ELSE NULL END\").alias(\"ClickThroughDate\")\n",
    "    )\n",
    "    \n",
    "    # Load activityparty with addressused (framework helper doesn't include addressused)\n",
    "    activityparty = data_sync.reader.read_bronze_table(\n",
    "        table_name=\"activityparty\",\n",
    "        columns={\n",
    "            \"activityid\": \"EmailActivityId\",\n",
    "            \"partyid\": \"partyid\",\n",
    "            \"partyid_entitytype\": \"partyid_entitytype\",\n",
    "            \"Id\": \"SourceSystemId\",\n",
    "            \"addressused\": \"ConstituentEmail\"\n",
    "        },\n",
    "        filters=[\n",
    "            col(\"participationtypemask\").isin(*PARTICIPATION_TYPES_ALL_RECIPIENTS),\n",
    "            col(\"partyid\").isNotNull(),\n",
    "            lower(col(\"partyid_entitytype\")).isin(\"contact\", \"account\")\n",
    "        ]\n",
    "    ).withColumn(\"partyid_entitytype\", lower(col(\"partyid_entitytype\"))) \\\n",
    "     .dropDuplicates([\"EmailActivityId\", \"partyid\", \"partyid_entitytype\", \"SourceSystemId\"])\n",
    "    \n",
    "    exploded = email_base.join(activityparty, on=\"EmailActivityId\", how=\"inner\")\n",
    "    \n",
    "    # Resolve partyid (contact/account) → ConstituentId\n",
    "    resolved = data_sync.resolve_polymorphic_lookup(\n",
    "        exploded,\n",
    "        lookup_id_column=\"partyid\",\n",
    "        entity_type_column=\"partyid_entitytype\",\n",
    "        entity_type_mappings={\"contact\": \"Contact\", \"account\": \"Account\"}\n",
    "    )\n",
    "    \n",
    "    with_constituent = data_sync.resolve_constituent_id(\n",
    "        resolved,\n",
    "        contact_fk_column=\"ContactId\",\n",
    "        account_fk_column=\"AccountId\",\n",
    "        output_column=\"ConstituentId\"\n",
    "    )\n",
    "    \n",
    "    linked = _resolve_email_engagement_link(with_constituent)\n",
    "    \n",
    "    return (\n",
    "        linked\n",
    "        .filter(col(\"ConstituentId\").isNotNull())\n",
    "        .select(\n",
    "            col(\"EmailEngagementId\"),\n",
    "            lit(None).cast(StringType()).alias(\"Timezone\"),\n",
    "            col(\"ConstituentEmail\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"EmailId\"),\n",
    "            col(\"EmailSubject\"),\n",
    "            col(\"ModifiedDate\"),\n",
    "            col(\"OpenedDate\"),\n",
    "            col(\"SendDate\"),\n",
    "            col(\"SourceSystemId\"),\n",
    "            col(\"WasOpened\"),\n",
    "            col(\"ClickThrough\"),\n",
    "            col(\"ClickThroughDate\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"email\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\",\n",
    "        \"createdon\", \"modifiedon\", \"senton\",\n",
    "        \"subject\",\n",
    "        \"lastopenedtime\", \"linksclickedcount\",\n",
    "        \"SinkModifiedOn\"\n",
    "    ],\n",
    "    target_table=\"ConstituentEmailEngagement\",\n",
    "    target_primary_key=\"ConstituentEmailEngagementId\",\n",
    "    transform_func=transform_constituent_email_engagement\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3416c-0093-49d0-a348-268e1f5286e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: ConstituentOpportunityStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fc637-c97f-4492-a60f-531bc4927472",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, lower, sha2, concat_ws\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def _load_opportunity_stage_mapping() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load OpportunityStage mapping (statecode → OpportunityStageId).\n",
    "    \n",
    "    WHY: Dynamics 365 stores opportunity state as numeric code (0, 1, 2).\n",
    "         OpportunityStage dimension table was synced first with these codes.\n",
    "         Sticky mapping ensures consistent OpportunityStageId across runs.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: StateCodeStr, OpportunityStageId\n",
    "    \"\"\"\n",
    "    return (\n",
    "        get_silver_table(\"SourceSystemIdMapping\")\n",
    "        .filter((col(\"SourceId\") == lit(data_sync.source_id)) &\n",
    "                (col(\"SourceTable\") == \"OpportunityStage\"))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").alias(\"StateCodeStr\"),\n",
    "            col(\"SilverRecordId\").alias(\"OpportunityStageId\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _prepare_opportunity_base_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract and normalize opportunity columns needed for constituent-stage pairing.\n",
    "    \n",
    "    WHY: We need:\n",
    "        - statecode: To resolve which OpportunityStage\n",
    "        - customerid + entitytype: To resolve which Constituent (polymorphic lookup)\n",
    "    \"\"\"\n",
    "    return df.select(\n",
    "        col(\"Id\").alias(\"OpportunitySourceId\"),\n",
    "        col(\"customerid\").alias(\"customerid\"),\n",
    "        lower(col(\"customerid_entitytype\")).alias(\"customerid_entitytype\"),\n",
    "        col(\"statecode\").alias(\"StateCodeStr\")\n",
    "    )\n",
    "\n",
    "def _resolve_opportunity_stage(base_df: DataFrame, stage_mapping: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join opportunity data with OpportunityStage mapping.\n",
    "    \n",
    "    WHY: Converts numeric statecode to stable OpportunityStageId GUID.\n",
    "         Uses sticky mapping to preserve IDs across sync runs.\n",
    "    \"\"\"\n",
    "    return base_df.join(stage_mapping, on=\"StateCodeStr\", how=\"left\")\n",
    "\n",
    "def _resolve_opportunity_customer_constituent(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve opportunity customer (polymorphic Contact/Account) to Constituent.\n",
    "    \n",
    "    Business Logic:\n",
    "    - opportunity.customerid can point to Contact OR Account (indicated by customerid_entitytype)\n",
    "    - Contact/Account both link to Constituent\n",
    "    - Result: ConstituentId who owns this opportunity\n",
    "    \n",
    "    WHY Two-step resolution:\n",
    "    1. Polymorphic lookup: customerid → ContactId/AccountId (based on entitytype)\n",
    "    2. Constituent resolution: Contact/Account → ConstituentId\n",
    "    \"\"\"\n",
    "    # Step 1: Resolve polymorphic customerid → ContactId/AccountId\n",
    "    with_party = data_sync.resolve_polymorphic_lookup(\n",
    "        df,\n",
    "        lookup_id_column=\"customerid\",\n",
    "        entity_type_column=\"customerid_entitytype\",\n",
    "        entity_type_mappings={\"contact\": \"Contact\", \"account\": \"Account\"}\n",
    "    )\n",
    "    \n",
    "    # Step 2: Resolve Contact/Account → Constituent\n",
    "    return data_sync.resolve_constituent_id(\n",
    "        with_party,\n",
    "        contact_fk_column=\"ContactId\",\n",
    "        account_fk_column=\"AccountId\",\n",
    "        output_column=\"ConstituentId\"\n",
    "    )\n",
    "\n",
    "def _build_constituent_opportunity_stage_pairs(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Build final ConstituentOpportunityStage link table with deterministic composite key.\n",
    "    \n",
    "    WHY Deterministic Hash:\n",
    "    - Link table tracks many-to-many: Constituent ↔ OpportunityStage\n",
    "    - Same (ConstituentId, OpportunityStageId) pair must get same SourceSystemId\n",
    "    - SHA2 hash ensures idempotence: re-running sync won't create duplicate pairs\n",
    "    - Framework uses SourceSystemId for MERGE matching via SourceSystemIdMapping\n",
    "    \n",
    "    WHY Filter NULLs:\n",
    "    - Foreign key constraints require valid ConstituentId and OpportunityStageId\n",
    "    - NULL values indicate resolution failure (missing Contact/Account or invalid statecode)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.where(col(\"ConstituentId\").isNotNull() & col(\"OpportunityStageId\").isNotNull())\n",
    "          .select(\n",
    "              # Deterministic composite key: hash of (ConstituentId, OpportunityStageId)\n",
    "              sha2(concat_ws(\"|\", col(\"ConstituentId\"), col(\"OpportunityStageId\")), 256)\n",
    "                  .alias(\"SourceSystemId\"),\n",
    "              col(\"ConstituentId\"),\n",
    "              col(\"OpportunityStageId\")\n",
    "          )\n",
    "          .dropDuplicates([\"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "def transform_constituent_opportunity_stage(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform opportunity to ConstituentOpportunityStage link table.\n",
    "    \n",
    "    Business Logic:\n",
    "    ConstituentOpportunityStage tracks which constituents have opportunities in which stages.\n",
    "    This is a many-to-many link table (one constituent can have multiple opportunities,\n",
    "    and opportunities can be in different stages over time).\n",
    "    \n",
    "    Data Flow:\n",
    "    opportunity.statecode → OpportunityStageId (via sticky mapping)\n",
    "    opportunity.customerid → ConstituentId (via polymorphic Contact/Account lookup)\n",
    "    → Link table pair: (ConstituentId, OpportunityStageId)\n",
    "    \n",
    "    WHY This Pattern:\n",
    "    - Enables analytics: \"Which constituents have opportunities in Won stage?\"\n",
    "    - Deterministic hash prevents duplicate pairs across sync runs\n",
    "    - Framework handles MERGE automatically using hash-based SourceSystemId\n",
    "    \"\"\"\n",
    "    # Step 1: Load OpportunityStage mapping (statecode → OpportunityStageId)\n",
    "    stage_mapping = _load_opportunity_stage_mapping()\n",
    "    \n",
    "    # Step 2: Prepare opportunity base data\n",
    "    base_data = _prepare_opportunity_base_data(df)\n",
    "    \n",
    "    # Step 3: Resolve OpportunityStage from statecode\n",
    "    with_stage = _resolve_opportunity_stage(base_data, stage_mapping)\n",
    "    \n",
    "    # Step 4: Resolve Constituent from polymorphic customerid\n",
    "    with_constituent = _resolve_opportunity_customer_constituent(with_stage)\n",
    "    \n",
    "    # Step 5: Build final link table pairs with deterministic hash\n",
    "    return _build_constituent_opportunity_stage_pairs(with_constituent)\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"opportunity\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\"Id\", \"customerid\", \"customerid_entitytype\", \"statecode\", \"SinkModifiedOn\"],\n",
    "    target_table=\"ConstituentOpportunityStage\",\n",
    "    target_primary_key=\"ConstituentOpportunityStageId\",\n",
    "    transform_func=transform_constituent_opportunity_stage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994299c-09d8-48e8-bc85-fb80ea881614",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a272ab0-b89a-4551-89fd-cedf52d0d3af",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "\n",
    "def transform_letter(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform letter to Silver via activityparty explosion (1 letter → N recipients).\n",
    "    Resolves CampaignId and ConstituentId using Activity Helper Functions.\n",
    "    \"\"\"\n",
    "    # Read activity parties (1:N explosion point - one letter to multiple recipients)\n",
    "    activityparty = data_sync.reader.read_activity_parties(\n",
    "        activity_id_column=\"activityid\",\n",
    "        participation_types=PARTICIPATION_TYPES_SENDER_RECIPIENT\n",
    "    )\n",
    "    \n",
    "    # Join letter with parties\n",
    "    with_parties = (\n",
    "        df.select(\n",
    "            col(\"Id\").alias(\"activityid\"),\n",
    "            col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "            col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "            col(\"actualend\").cast(TimestampType()).alias(\"SentDate\"),\n",
    "            col(\"subject\").alias(\"Subject\"),\n",
    "            col(\"regardingobjectid\"),\n",
    "            col(\"regardingobjectid_entitytype\")\n",
    "        )\n",
    "        .join(activityparty, on=\"activityid\", how=\"inner\")\n",
    "    )\n",
    "    \n",
    "    # Resolve Campaign and Constituent using shared Activity helpers\n",
    "    with_campaign = resolve_campaign_for_activity(with_parties)\n",
    "    with_constituent = resolve_activity_party_constituent(\n",
    "        with_campaign,\n",
    "        party_id_col=\"partyid\",\n",
    "        party_type_col=\"partyid_entitytype\"\n",
    "    )\n",
    "    \n",
    "    # Add Letter channel\n",
    "    channel_letter = lookup_channel_by_name(CHANNEL_LETTER)\n",
    "    \n",
    "    return (\n",
    "        with_constituent.crossJoin(channel_letter)\n",
    "        .select(\n",
    "            col(\"ActivityPartyId\").alias(\"SourceSystemId\"),\n",
    "            \"CampaignId\",\n",
    "            \"ConstituentId\",\n",
    "            \"ChannelId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"SentDate\",\n",
    "            \"Subject\"\n",
    "        )\n",
    "        .filter(col(\"ConstituentId\").isNotNull())\n",
    "    )\n",
    "\n",
    "# WHY composite key: Letter can have same ActivityPartyId from different sources,\n",
    "# and same constituent can receive multiple letters\n",
    "letter_merge_sql = \"\"\"\n",
    "MERGE INTO {TARGET_TABLE} AS target\n",
    "USING {SOURCE_VIEW} AS source\n",
    "  ON  target.SourceId = source.SourceId\n",
    "  AND target.SourceSystemId = source.SourceSystemId\n",
    "  AND target.ConstituentId = source.ConstituentId\n",
    "\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.ModifiedDate = source.ModifiedDate,\n",
    "    target.SentDate = source.SentDate,\n",
    "    target.Subject = source.Subject,\n",
    "    target.CampaignId = source.CampaignId,\n",
    "    target.ChannelId = source.ChannelId\n",
    "\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    LetterId, CampaignId, ChannelId, ConstituentId, CreatedDate,\n",
    "    ModifiedDate, SentDate, Subject, SourceId, SourceSystemId\n",
    ") VALUES (\n",
    "    source.LetterId, source.CampaignId, source.ChannelId, source.ConstituentId,\n",
    "    source.CreatedDate, source.ModifiedDate, source.SentDate, source.Subject,\n",
    "    source.SourceId, source.SourceSystemId\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "data_sync.sync_activity_table(\n",
    "    source_table=\"letter\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"createdon\", \"modifiedon\", \"actualend\", \"subject\",\n",
    "        \"regardingobjectid\", \"regardingobjectid_entitytype\"\n",
    "    ],\n",
    "    target_table=\"Letter\",\n",
    "    target_primary_key=\"LetterId\",\n",
    "    activity_party_join_func=transform_letter,\n",
    "    merge_sql=letter_merge_sql\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a56a37-a643-43df-bdb8-7eaf1fb785a6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b795a9a-ac38-4f23-abf5-e8e6ea38731c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, trim, substring, when, row_number\n",
    "from pyspark.sql.types import TimestampType, DecimalType, StringType\n",
    "\n",
    "def resolve_opportunity_type(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve OpportunityType via opportunitysalesprocess.\n",
    "    \n",
    "    Business Logic:\n",
    "    - One opportunity can have multiple sales processes (workflows)\n",
    "    - WHY Window deduplication: Prefer active process (statecode=0), then most recent\n",
    "    - OpportunityType comes from the chosen process's processid\n",
    "    \"\"\"\n",
    "    # Read opportunitysalesprocess using framework (auto-trims strings)\n",
    "    sales_process = data_sync.reader.read_bronze_table(\n",
    "        \"opportunitysalesprocess\",\n",
    "        columns={\n",
    "            \"opportunityid\": (\"OpportunitySalesProcessOpportunityId\", \"string\"),\n",
    "            \"processid\": (\"OpportunitySalesProcessProcessId\", \"string\"),\n",
    "            \"statecode\": \"OpportunitySalesProcessStateCode\",\n",
    "            \"SinkModifiedOn\": (\"OpportunitySalesProcessModified\", \"timestamp\")\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Deduplicate: prefer active processes, then latest modified\n",
    "    window = Window.partitionBy(\"OpportunitySalesProcessOpportunityId\").orderBy(\n",
    "        when(col(\"OpportunitySalesProcessStateCode\") == SALES_PROCESS_STATE_ACTIVE, lit(0)).otherwise(lit(1)),\n",
    "        col(\"OpportunitySalesProcessModified\").desc_nulls_last()\n",
    "    )\n",
    "    \n",
    "    active_process = (\n",
    "        sales_process\n",
    "        .withColumn(\"rank\", row_number().over(window))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .select(\"OpportunitySalesProcessOpportunityId\", \"OpportunitySalesProcessProcessId\")\n",
    "    )\n",
    "    \n",
    "    # Join opportunity → sales process\n",
    "    with_process = df.join(\n",
    "        active_process,\n",
    "        col(\"Id\").cast(StringType()) == col(\"OpportunitySalesProcessOpportunityId\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Resolve processid → OpportunityTypeId via Silver OpportunityType (source-scoped)\n",
    "    opportunity_type = (\n",
    "        get_silver_table(\"OpportunityType\")\n",
    "        .filter(col(\"SourceId\") == lit(data_sync.source_id))\n",
    "        .select(\n",
    "            col(\"SourceSystemId\").alias(\"ProcessIdString\"),\n",
    "            col(\"OpportunityTypeId\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return with_process.join(\n",
    "        opportunity_type,\n",
    "        col(\"OpportunitySalesProcessProcessId\") == col(\"ProcessIdString\"),\n",
    "        \"left\"\n",
    "    ).drop(\"OpportunitySalesProcessOpportunityId\", \"OpportunitySalesProcessProcessId\", \"ProcessIdString\")\n",
    "\n",
    "\n",
    "def resolve_opportunity_stage(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Resolve OpportunityStage via statecode mapping.\n",
    "    \n",
    "    Maps Dynamics 365 statecode values to OpportunityStage names:\n",
    "    - 0 → Open, 1 → Won, 2 → Lost\n",
    "    \"\"\"\n",
    "    opportunity_stage = (\n",
    "        get_silver_table(\"OpportunityStage\")\n",
    "        .select(\n",
    "            col(\"Name\").alias(\"OpportunityStageName\"),\n",
    "            col(\"OpportunityStageId\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Map statecode to stage name using constants\n",
    "    stage_name = (\n",
    "        when(col(\"statecode\").cast(\"long\") == OPPORTUNITY_STATE_OPEN, lit(OPPORTUNITY_STAGE_NAME_OPEN))\n",
    "        .when(col(\"statecode\").cast(\"long\") == OPPORTUNITY_STATE_WON, lit(OPPORTUNITY_STAGE_NAME_WON))\n",
    "        .when(col(\"statecode\").cast(\"long\") == OPPORTUNITY_STATE_LOST, lit(OPPORTUNITY_STAGE_NAME_LOST))\n",
    "        .otherwise(lit(None).cast(StringType()))\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        df.withColumn(\"OpportunityStageName\", stage_name)\n",
    "          .join(opportunity_stage, on=\"OpportunityStageName\", how=\"left\")\n",
    "          .drop(\"OpportunityStageName\")\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_opportunity(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform Opportunity from Bronze to Silver.\n",
    "    \n",
    "    Business Logic:\n",
    "    - OpportunityType: Resolved via opportunitysalesprocess (BPF workflow)\n",
    "    - OpportunityStage: Mapped from statecode (0=Open, 1=Won, 2=Lost)\n",
    "    - ConstituentId: Resolved via polymorphic customerid (Contact or Account)\n",
    "    - CampaignId: Direct FK mapping\n",
    "    \"\"\"\n",
    "    # Resolve OpportunityStage and OpportunityType\n",
    "    df = resolve_opportunity_stage(df)\n",
    "    df = resolve_opportunity_type(df)\n",
    "    \n",
    "    # Resolve customerid polymorphically (Contact or Account based on entitytype)\n",
    "    df = data_sync.resolve_polymorphic_lookup(\n",
    "        df,\n",
    "        lookup_id_column=\"customerid\",\n",
    "        entity_type_column=\"customerid_entitytype\",\n",
    "        entity_type_mappings={\n",
    "            \"contact\": \"Contact\",\n",
    "            \"account\": \"Account\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Resolve ConstituentId from Contact/Account\n",
    "    df = data_sync.resolve_constituent_id(\n",
    "        df,\n",
    "        contact_fk_column=\"ContactId\",\n",
    "        account_fk_column=\"AccountId\",\n",
    "        output_column=\"ConstituentId\"\n",
    "    )\n",
    "    \n",
    "    return df.select(\n",
    "        col(\"Id\"),\n",
    "        col(\"campaignid_SilverRecordId\").alias(\"CampaignId\"),\n",
    "        col(\"actualclosedate\").cast(TimestampType()).alias(\"CloseDate\"),\n",
    "        col(\"ConstituentId\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"estimatedvalue_base\").cast(DecimalType(16, 8)).alias(\"ExpectedRevenue\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "        col(\"OpportunityStageId\"),\n",
    "        col(\"OpportunityTypeId\"),\n",
    "        substring(trim(col(\"name\")).cast(StringType()), 1, MAX_NAME_LENGTH).alias(\"OpportunityName\")\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"opportunity\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"campaignid\", \"customerid\", \"customerid_entitytype\",\n",
    "        \"actualclosedate\", \"createdon\", \"modifiedon\",\n",
    "        \"estimatedvalue_base\", \"name\", \"statecode\"\n",
    "    ],\n",
    "    target_table=\"Opportunity\",\n",
    "    target_primary_key=\"OpportunityId\",\n",
    "    transform_func=transform_opportunity,\n",
    "    fk_mappings={\n",
    "        \"campaignid\": \"Campaign\"\n",
    "        # customerid resolved dynamically in transform_func based on customerid_entitytype\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14eef9-be6a-489d-8916-7b7d3b1ad755",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Phonecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc931931-906f-42ff-a0c1-febe6b53831f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def transform_phonecall(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform phonecall to Silver via activityparty explosion (1 phonecall → N recipients).\n",
    "    Resolves CampaignId and ConstituentId using Activity Helper Functions.\n",
    "    \"\"\"\n",
    "    activityparty = data_sync.reader.read_activity_parties(\n",
    "        activity_id_column=\"activityid\",\n",
    "        participation_types=PARTICIPATION_TYPES_SENDER_RECIPIENT\n",
    "    )\n",
    "    \n",
    "    with_parties = (\n",
    "        df.select(\n",
    "            col(\"Id\").alias(\"activityid\"),\n",
    "            col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "            col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "            col(\"actualstart\").cast(TimestampType()).alias(\"CallDate\"),\n",
    "            col(\"description\").alias(\"Description\"),\n",
    "            col(\"regardingobjectid\"),\n",
    "            col(\"regardingobjectid_entitytype\")\n",
    "        )\n",
    "        .join(activityparty, on=\"activityid\", how=\"inner\")\n",
    "    )\n",
    "    \n",
    "    with_campaign = resolve_campaign_for_activity(with_parties)\n",
    "    with_constituent = resolve_activity_party_constituent(\n",
    "        with_campaign,\n",
    "        party_id_col=\"partyid\",\n",
    "        party_type_col=\"partyid_entitytype\"\n",
    "    )\n",
    "    \n",
    "    channel_phonecall = lookup_channel_by_name(CHANNEL_PHONE_CALL)\n",
    "    \n",
    "    return (\n",
    "        with_constituent.crossJoin(channel_phonecall)\n",
    "        .select(\n",
    "            col(\"ActivityPartyId\").alias(\"SourceSystemId\"),\n",
    "            \"CampaignId\",\n",
    "            \"ConstituentId\",\n",
    "            \"ChannelId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"CallDate\",\n",
    "            \"Description\"\n",
    "        )\n",
    "        .filter(col(\"ConstituentId\").isNotNull())\n",
    "    )\n",
    "\n",
    "# WHY composite key: Phonecall can have same ActivityPartyId from different sources,\n",
    "# and same constituent can receive multiple phonecalls\n",
    "phonecall_merge_sql = \"\"\"\n",
    "MERGE INTO {TARGET_TABLE} AS target\n",
    "USING {SOURCE_VIEW} AS source\n",
    "  ON  target.SourceId = source.SourceId\n",
    "  AND target.SourceSystemId = source.SourceSystemId\n",
    "  AND target.ConstituentId = source.ConstituentId\n",
    "\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.ModifiedDate = source.ModifiedDate,\n",
    "    target.CallDate = source.CallDate,\n",
    "    target.Description = source.Description,\n",
    "    target.CampaignId = source.CampaignId,\n",
    "    target.ChannelId = source.ChannelId\n",
    "\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    PhonecallId, CampaignId, ChannelId, ConstituentId, CreatedDate,\n",
    "    ModifiedDate, CallDate, Description, SourceId, SourceSystemId\n",
    ") VALUES (\n",
    "    source.PhonecallId, source.CampaignId, source.ChannelId, source.ConstituentId,\n",
    "    source.CreatedDate, source.ModifiedDate, source.CallDate, source.Description,\n",
    "    source.SourceId, source.SourceSystemId\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "data_sync.sync_activity_table(\n",
    "    source_table=\"phonecall\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"createdon\", \"modifiedon\", \"actualstart\", \"description\",\n",
    "        \"regardingobjectid\", \"regardingobjectid_entitytype\"\n",
    "    ],\n",
    "    target_table=\"Phonecall\",\n",
    "    target_primary_key=\"PhonecallId\",\n",
    "    activity_party_join_func=transform_phonecall,\n",
    "    merge_sql=phonecall_merge_sql\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38b45c-5de8-4ff7-9f04-ee2f09fadae3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894be446-0ddb-4d81-93bc-d66c252bc0a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def transform_transaction(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transform transaction to Silver. Resolves ConstituentId via Contact/Account FKs.\"\"\"\n",
    "    return (\n",
    "        data_sync.resolve_constituent_id(\n",
    "            df,\n",
    "            contact_fk_column=\"msnfp_receiptoncontactid_SilverRecordId\",\n",
    "            account_fk_column=\"msnfp_transaction_receiptonaccountid_SilverRecordId\",\n",
    "            output_column=\"ConstituentId\"\n",
    "        )\n",
    "        .select(\n",
    "            col(\"Id\"),\n",
    "            col(\"msnfp_amount\").alias(\"Amount\"),\n",
    "            col(\"msnfp_name\").alias(\"Name\"),\n",
    "            col(\"msnfp_bookdate\").cast(TimestampType()).alias(\"TransactionDate\"),\n",
    "            col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "            col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "            col(\"ConstituentId\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"msnfp_transaction\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\",\n",
    "        \"msnfp_transactionid\",\n",
    "        \"msnfp_amount\",\n",
    "        \"msnfp_name\",\n",
    "        \"msnfp_bookdate\",\n",
    "        \"msnfp_receiptoncontactid\",\n",
    "        \"msnfp_transaction_receiptonaccountid\",\n",
    "        \"createdon\",\n",
    "        \"modifiedon\"\n",
    "    ],\n",
    "    target_table=\"Transaction\",\n",
    "    target_primary_key=\"TransactionId\",\n",
    "    transform_func=transform_transaction,\n",
    "    fk_mappings={\n",
    "        \"msnfp_receiptoncontactid\": \"Contact\",\n",
    "        \"msnfp_transaction_receiptonaccountid\": \"Account\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420eb961-0c83-4231-860d-e1b94576e837",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c9cfe-4ce5-42ab-851c-786f97c8e4cd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def transform_program(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Transform msnfp_designation to Silver Program.\"\"\"\n",
    "    return df.select(\n",
    "        col(\"Id\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\"),\n",
    "        col(\"msnfp_name\").alias(\"Name\")\n",
    "    )\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"msnfp_designation\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\", \"createdon\", \"modifiedon\", \"msnfp_name\"\n",
    "    ],\n",
    "    target_table=\"Program\",\n",
    "    target_primary_key=\"ProgramId\",\n",
    "    transform_func=transform_program\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0cfe99-b44b-4346-a97e-bc86dbedb30d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: ConstituentProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634454a-a26d-45e5-a65f-8c1fb2c515ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, expr, lit, lower, coalesce\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "\n",
    "def _load_transaction_lookup_data() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load transaction data needed to resolve constituent references.\n",
    "    \n",
    "    WHY: Designated credits don't directly reference Contact/Account.\n",
    "         They link to Transactions, which hold the constituent reference\n",
    "         (msnfp_receiptoncontactid or msnfp_transaction_receiptonaccountid).\n",
    "    \"\"\"\n",
    "    return data_sync.reader.read_bronze_table(\n",
    "        \"msnfp_transaction\",\n",
    "        columns={\n",
    "            \"Id\": \"TransactionId\",\n",
    "            \"msnfp_receiptoncontactid\": \"TransactionContactSourceId\",\n",
    "            \"msnfp_transaction_receiptonaccountid\": \"TransactionAccountSourceId\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "def _prepare_designated_credit_base(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract and normalize designated credit columns.\n",
    "    \n",
    "    WHY: Dynamics 365 uses polymorphic lookups (ID + entitytype).\n",
    "         We need to validate that msnfp_transactionid points to msnfp_transaction\n",
    "         and msnfp_designatiedcredit_designationid points to msnfp_designation.\n",
    "    \"\"\"\n",
    "    return df.select(\n",
    "        col(\"Id\").alias(\"DesignatedCreditSourceId\"),\n",
    "        col(\"msnfp_transactionid\").alias(\"DesignatedCreditTransactionId\"),\n",
    "        lower(coalesce(col(\"msnfp_transactionid_entitytype\"), lit(\"\"))).alias(\"DesignatedCreditTransactionEntity\"),\n",
    "        col(\"msnfp_designatiedcredit_designationid\").alias(\"DesignatedCreditDesignationId\"),\n",
    "        lower(coalesce(col(\"msnfp_designatiedcredit_designationid_entitytype\"), lit(\"\"))).alias(\"DesignatedCreditDesignationEntity\"),\n",
    "        col(\"createdon\").cast(TimestampType()).alias(\"CreatedDate\"),\n",
    "        col(\"modifiedon\").cast(TimestampType()).alias(\"ModifiedDate\")\n",
    "    )\n",
    "\n",
    "def _filter_valid_designated_credits(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only designated credits with valid transaction and designation references.\n",
    "    \n",
    "    WHY: Polymorphic lookups can point to any entity type.\n",
    "         We only want records where:\n",
    "         - msnfp_transactionid points to msnfp_transaction (not other entities)\n",
    "         - msnfp_designatiedcredit_designationid points to msnfp_designation (not other entities)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.filter((col(\"DesignatedCreditTransactionId\").isNotNull()) & \n",
    "                  (col(\"DesignatedCreditTransactionEntity\") == \"msnfp_transaction\"))\n",
    "          .filter((col(\"DesignatedCreditDesignationId\").isNotNull()) & \n",
    "                  (col(\"DesignatedCreditDesignationEntity\") == \"msnfp_designation\"))\n",
    "    )\n",
    "\n",
    "def _join_transaction_for_constituent_reference(df: DataFrame, tx: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join transaction data to get Contact/Account references needed for constituent resolution.\n",
    "    \n",
    "    WHY: Designated credits track \"which program received credit from which transaction\".\n",
    "         To know which constituent gave the donation, we need to follow:\n",
    "         DesignatedCredit → Transaction → Contact/Account → Constituent\n",
    "    \"\"\"\n",
    "    return df.join(tx, col(\"DesignatedCreditTransactionId\") == col(\"TransactionId\"), \"left\")\n",
    "\n",
    "def _prepare_for_constituent_resolution(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reshape data to match framework's resolve_constituent_id expectations.\n",
    "    \n",
    "    WHY: Framework expects specific column names for FK resolution:\n",
    "         - msnfp_receiptoncontactid (Contact FK from transaction)\n",
    "         - msnfp_transaction_receiptonaccountid (Account FK from transaction)\n",
    "         - msnfp_designatiedcredit_designationid (Program FK, needs FK mapping)\n",
    "    \"\"\"\n",
    "    return df.select(\n",
    "        col(\"DesignatedCreditSourceId\"),\n",
    "        col(\"DesignatedCreditDesignationId\").alias(\"msnfp_designatiedcredit_designationid\"),\n",
    "        col(\"TransactionContactSourceId\").alias(\"msnfp_receiptoncontactid\"),\n",
    "        col(\"TransactionAccountSourceId\").alias(\"msnfp_transaction_receiptonaccountid\"),\n",
    "        col(\"CreatedDate\"),\n",
    "        col(\"ModifiedDate\")\n",
    "    )\n",
    "\n",
    "def _build_constituent_program_output(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Build final ConstituentProgram schema with synthetic PK and deduplication.\n",
    "    \n",
    "    WHY: ConstituentProgram is a link table (Constituent ↔ Program).\n",
    "         - ConstituentProgramId: New synthetic UUID for Silver table\n",
    "         - SourceSystemId: Original designated credit ID for tracking\n",
    "         - Filter out records where constituent resolution failed (NULL ConstituentId)\n",
    "         - Deduplicate by SourceSystemId (one ConstituentProgram per designated credit)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.withColumn(\"ConstituentProgramId\", expr(\"uuid()\"))\n",
    "          .select(\n",
    "              \"ConstituentProgramId\",\n",
    "              \"ConstituentId\",\n",
    "              col(\"msnfp_designatiedcredit_designationid\"),\n",
    "              \"CreatedDate\",\n",
    "              \"ModifiedDate\",\n",
    "              col(\"DesignatedCreditSourceId\").alias(\"SourceSystemId\")\n",
    "          )\n",
    "          .filter(col(\"ConstituentId\").isNotNull())\n",
    "          .dropDuplicates([\"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "def transform_constituent_program(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform designated credit to ConstituentProgram.\n",
    "    \n",
    "    Business Logic:\n",
    "    ConstituentProgram links constituents to programs they support.\n",
    "    In Dynamics 365: Designation (Program) → DesignatedCredit → Transaction → Contact/Account → Constituent\n",
    "    \"\"\"\n",
    "    # Step 1: Load transaction lookup data (holds constituent references)\n",
    "    transactions = _load_transaction_lookup_data()\n",
    "    \n",
    "    # Step 2: Prepare base data from designated credits\n",
    "    base = _prepare_designated_credit_base(df)\n",
    "    \n",
    "    # Step 3: Filter to valid transaction and designation references\n",
    "    valid = _filter_valid_designated_credits(base)\n",
    "    \n",
    "    # Step 4: Join transaction data to get Contact/Account FKs\n",
    "    with_transactions = _join_transaction_for_constituent_reference(valid, transactions)\n",
    "    \n",
    "    # Step 5: Prepare data for constituent resolution\n",
    "    prepared = _prepare_for_constituent_resolution(with_transactions)\n",
    "    \n",
    "    # Step 6: Resolve ConstituentId via Contact/Account using framework\n",
    "    with_constituent = data_sync.resolve_constituent_id(\n",
    "        prepared,\n",
    "        contact_fk_column=\"msnfp_receiptoncontactid\",\n",
    "        account_fk_column=\"msnfp_transaction_receiptonaccountid\",\n",
    "        output_column=\"ConstituentId\"\n",
    "    )\n",
    "    \n",
    "    # Step 7: Build final output schema\n",
    "    return _build_constituent_program_output(with_constituent)\n",
    "\n",
    "data_sync.sync_table(\n",
    "    source_table=\"msnfp_designatedcredit\",\n",
    "    source_primary_key=\"Id\",\n",
    "    source_columns=[\n",
    "        \"Id\",\n",
    "        \"msnfp_transactionid\", \"msnfp_transactionid_entitytype\",\n",
    "        \"msnfp_designatiedcredit_designationid\", \"msnfp_designatiedcredit_designationid_entitytype\",\n",
    "        \"createdon\", \"modifiedon\"\n",
    "    ],\n",
    "    target_table=\"ConstituentProgram\",\n",
    "    target_primary_key=\"ConstituentProgramId\",\n",
    "    transform_func=transform_constituent_program,\n",
    "    fk_mappings={\n",
    "        \"msnfp_designatiedcredit_designationid\": \"Program\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "{DYNAMICS_LAKEHOUSE_ID}",
    "default_lakehouse_name": "{DYNAMICS_LAKEHOUSE_NAME}",
    "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
    "known_lakehouses": [
     {
      "id": "{DYNAMICS_LAKEHOUSE_ID}"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
