{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf0c9e09-c3f1-4089-81e2-87d7745f7e6a",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5afc22-6ae2-4363-b5d8-049bbf04f997",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "%run <Fundraising_Config>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70886833-d962-435e-8381-fd84e759f875",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Constituent segment types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a97089-5d28-440e-8f76-3491b60b4aee",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "segment_type_df = get_gold_table(\"DimConstituentSegmentType\")\n",
        "display(segment_type_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44548064-f8e9-4125-a550-6b9364cd6769",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Constituent segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2132c089-ec30-4c11-8fd2-b6e82015eb77",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "segment_df = get_gold_table(\"DimConstituentSegment\")\n",
        "ordered_df = segment_df.orderBy(\"TypeKey\", \"ConstituentSegmentKey\")\n",
        "display(ordered_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bbd01d0-8c86-45d6-a184-ecda94d5e5ee",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Constituents - Datamarkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d783afd-38f2-42b4-9a25-88061dc19c04",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "const_df = get_gold_table(\"dm_Constituent\")\n",
        "display(const_df.limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9637b5c7-c469-464a-a10a-53be0c723da6",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Age ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab98a2dd-d75f-473c-b9d0-bd5f6f276575",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when, lit, xxhash64\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Dynamically find ConstituentSegmentTypeKey for \"Age Range\"\n",
        "age_range_type_row = get_gold_table(\"DimConstituentSegmentType\") \\\n",
        "    .filter(col(\"ConstituentSegmentType\") == \"Age Range\") \\\n",
        "    .select(\"ConstituentSegmentTypeKey\") \\\n",
        "    .first()\n",
        "\n",
        "if age_range_type_row is None:\n",
        "    raise ValueError(\"‚ùå Segment type 'Age Range' not found.\")\n",
        "\n",
        "age_range_type_key = age_range_type_row[\"ConstituentSegmentTypeKey\"]\n",
        "\n",
        "# Load required tables\n",
        "constituent_df = get_gold_table(\"dm_Constituent\").select(\"ConstituentKey\", \"ConstituentName\", \"Age\")\n",
        "\n",
        "segment_df = get_gold_table(\"DimConstituentSegment\") \\\n",
        "    .filter(col(\"TypeKey\") == age_range_type_key) \\\n",
        "    .select(\"ConstituentSegmentKey\", \"ConstituentSegmentName\", \"TypeKey\")\n",
        "\n",
        "segment_type_df = get_gold_table(\"DimConstituentSegmentType\")\n",
        "\n",
        "# Add age boundaries based on segment name\n",
        "segment_df = segment_df.withColumn(\"AgeMin\", when(col(\"ConstituentSegmentName\") == \"<18\", lit(0))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"18-24\", lit(18))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"25-34\", lit(25))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"35-44\", lit(35))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"45-54\", lit(45))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"55-64\", lit(55))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"65-74\", lit(65))\n",
        "    .when(col(\"ConstituentSegmentName\") == \">=75\", lit(75))\n",
        ")\n",
        "\n",
        "segment_df = segment_df.withColumn(\"AgeMax\", when(col(\"ConstituentSegmentName\") == \"<18\", lit(17))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"18-24\", lit(24))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"25-34\", lit(34))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"35-44\", lit(44))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"45-54\", lit(54))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"55-64\", lit(64))\n",
        "    .when(col(\"ConstituentSegmentName\") == \"65-74\", lit(74))\n",
        "    .when(col(\"ConstituentSegmentName\") == \">75\", lit(200))\n",
        ")\n",
        "\n",
        "# Join constituents with the corresponding age segment\n",
        "classified_df = constituent_df.join(segment_df,\n",
        "    (col(\"Age\").isNotNull()) &\n",
        "    (col(\"Age\") >= col(\"AgeMin\")) & (col(\"Age\") <= col(\"AgeMax\")),\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# Get \"Unclassified\" segment key dynamically\n",
        "unclassified_row = get_gold_table(\"DimConstituentSegment\") \\\n",
        "    .filter((col(\"TypeKey\") == age_range_type_key) & (col(\"ConstituentSegmentName\") == \"Unclassified\")) \\\n",
        "    .select(\"ConstituentSegmentKey\").first()\n",
        "\n",
        "if unclassified_row is None:\n",
        "    raise ValueError(\"‚ùå Segment 'Unclassified' not found in Age Range segments.\")\n",
        "\n",
        "unclassified_key = unclassified_row[\"ConstituentSegmentKey\"]\n",
        "\n",
        "# Use matched segment or fallback to \"Unclassified\"\n",
        "classified_df = classified_df.withColumn(\"FinalSegmentKey\",\n",
        "    when(col(\"ConstituentSegmentKey\").isNotNull(), col(\"ConstituentSegmentKey\"))\n",
        "    .otherwise(lit(unclassified_key))\n",
        ")\n",
        "\n",
        "# Prepare bridge table for inserting new records\n",
        "new_bridge_df = classified_df.select(\n",
        "    col(\"ConstituentKey\"),\n",
        "    col(\"FinalSegmentKey\").alias(\"ConstituentSegmentKey\")\n",
        ").withColumn(\n",
        "    \"ConstituentSegmentBridgeKey\",\n",
        "    xxhash64(col(\"ConstituentKey\"), col(\"ConstituentSegmentKey\")).cast(\"bigint\")\n",
        ")\n",
        "\n",
        "\n",
        "# Get all keys for \"Age Range\" segments to be removed before insert\n",
        "segment_keys_to_remove = get_gold_table(\"DimConstituentSegment\") \\\n",
        "    .filter(col(\"TypeKey\") == age_range_type_key) \\\n",
        "    .select(\"ConstituentSegmentKey\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "print(\"üìå Removing existing Age Range segments with keys:\", segment_keys_to_remove)\n",
        "\n",
        "# Remove old values\n",
        "bridge_table = DeltaTable.forName(spark, f\"{gold_lakehouse_name}.DimConstituentSegmentBridge\")\n",
        "bridge_table.delete(f\"ConstituentSegmentKey IN ({','.join(map(str, segment_keys_to_remove))})  AND ConstituentSegmentMappingId IS NULL\")\n",
        "new_bridge_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{gold_lakehouse_name}.DimConstituentSegmentBridge\")\n",
        "\n",
        "print(\"‚úÖ Segment update complete.\")\n",
        "\n",
        "# Debug output ‚Äì join to get human-readable segment name and type\n",
        "output_df = classified_df \\\n",
        "    .join(\n",
        "        segment_df.select(\n",
        "            col(\"ConstituentSegmentKey\").alias(\"segKey\"),\n",
        "            col(\"TypeKey\").alias(\"SegmentTypeKey\"),\n",
        "            col(\"ConstituentSegmentName\")\n",
        "        ).alias(\"seg\"),\n",
        "        classified_df[\"FinalSegmentKey\"] == col(\"seg.segKey\"),\n",
        "        \"left\"\n",
        "    ) \\\n",
        "    .join(\n",
        "        segment_type_df.select(\n",
        "            col(\"ConstituentSegmentTypeKey\"),\n",
        "            col(\"ConstituentSegmentType\")\n",
        "        ).alias(\"stype\"),\n",
        "        col(\"seg.SegmentTypeKey\") == col(\"stype.ConstituentSegmentTypeKey\"),\n",
        "        \"left\"\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"ConstituentKey\"),\n",
        "        col(\"ConstituentName\"),\n",
        "        col(\"Age\"),\n",
        "        col(\"FinalSegmentKey\").alias(\"SegmentKey\"),\n",
        "        col(\"stype.ConstituentSegmentType\").alias(\"SegmentType\"),\n",
        "        col(\"seg.ConstituentSegmentName\").alias(\"SegmentName\")\n",
        "    ) \\\n",
        "    .orderBy(\"ConstituentKey\")\n",
        "\n",
        "# Show final output\n",
        "display(output_df.limit(200))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b81151c-a4cc-4081-8d8f-82074fb499ec",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Giving ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b733c99-79ab-4528-b065-8722f9706a21",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when, lit, xxhash64\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Load input tables\n",
        "constituent_df = get_gold_table(\"dm_Constituent\").select(\"ConstituentKey\", \"ConstituentName\", \"LifetimeDonationAmount\")\n",
        "\n",
        "# Replace nulls with 0\n",
        "constituent_df = constituent_df.withColumn(\n",
        "    \"LifetimeDonationAmount\",\n",
        "    coalesce(col(\"LifetimeDonationAmount\"), lit(0))\n",
        ")\n",
        "\n",
        "segment_df_raw = get_gold_table(\"DimConstituentSegment\").alias(\"segment\")\n",
        "segment_type_df = get_gold_table(\"DimConstituentSegmentType\").alias(\"stype\")\n",
        "\n",
        "# Select only Lifetime Giving Range segments\n",
        "segment_df = segment_df_raw.join(\n",
        "    segment_type_df,\n",
        "    segment_df_raw[\"TypeKey\"] == segment_type_df[\"ConstituentSegmentTypeKey\"],\n",
        "    \"inner\"\n",
        ").filter(col(\"ConstituentSegmentType\") == \"Lifetime Giving Range\") \\\n",
        " .select(\n",
        "     col(\"segment.ConstituentSegmentKey\"),\n",
        "     col(\"segment.ConstituentSegmentName\"),\n",
        "     col(\"segment.TypeKey\")\n",
        ")\n",
        "\n",
        "# Assign numeric ranges\n",
        "segment_df = segment_df.withColumn(\"AmountMin\", when(col(\"ConstituentSegmentName\") == \"<$250\", 0)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$250‚Äì$999\", 250)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$1,000‚Äì$4,999\", 1000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$5,000‚Äì$9,999\", 5000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$10,000‚Äì$24,999\", 10000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$25,000‚Äì$49,999\", 25000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$50,000‚Äì$99,999\", 50000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$100,000‚Äì$499,999\", 100000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$500,000‚Äì$999,999\", 500000)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$1,000,000+\", 1000000))\n",
        "\n",
        "segment_df = segment_df.withColumn(\"AmountMax\", when(col(\"ConstituentSegmentName\") == \"<$250\", 249)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$250‚Äì$999\", 999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$1,000‚Äì$4,999\", 4999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$5,000‚Äì$9,999\", 9999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$10,000‚Äì$24,999\", 24999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$25,000‚Äì$49,999\", 49999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$50,000‚Äì$99,999\", 99999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$100,000‚Äì$499,999\", 499999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$500,000‚Äì$999,999\", 999999)\n",
        "    .when(col(\"ConstituentSegmentName\") == \"$1,000,000+\", 999999999))\n",
        "\n",
        "# Join with constituents\n",
        "classified_df = constituent_df.join(\n",
        "    segment_df,\n",
        "    (col(\"LifetimeDonationAmount\").isNotNull()) &\n",
        "    (col(\"LifetimeDonationAmount\") >= col(\"AmountMin\")) &\n",
        "    (col(\"LifetimeDonationAmount\") <= col(\"AmountMax\")),\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# Prepare final output\n",
        "new_bridge_df = classified_df.select(\n",
        "    col(\"ConstituentKey\"),\n",
        "    col(\"ConstituentSegmentKey\")\n",
        ").withColumn(\n",
        "    \"ConstituentSegmentBridgeKey\",\n",
        "    xxhash64(col(\"ConstituentKey\"), col(\"ConstituentSegmentKey\")).cast(\"bigint\")\n",
        ")\n",
        "\n",
        "# Remove old values\n",
        "segment_keys_to_remove = segment_df.select(\"ConstituentSegmentKey\").rdd.flatMap(lambda x: x).collect()\n",
        "bridge_table = DeltaTable.forName(spark, f\"{gold_lakehouse_name}.DimConstituentSegmentBridge\")\n",
        "bridge_table.delete(f\"ConstituentSegmentKey IN ({','.join(map(str, segment_keys_to_remove))}) AND ConstituentSegmentMappingId IS NULL\")\n",
        "\n",
        "# Insert new mappings\n",
        "new_bridge_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{gold_lakehouse_name}.DimConstituentSegmentBridge\")\n",
        "\n",
        "# Display debug info\n",
        "display(classified_df.select(\n",
        "    \"ConstituentKey\", \"ConstituentName\", \"LifetimeDonationAmount\",\n",
        "    \"ConstituentSegmentKey\", \"ConstituentSegmentName\"\n",
        ").orderBy(\"ConstituentKey\").limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4bef71-7ef4-4a51-a012-0422b9c7a438",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "\n",
        "# Gift Recurrance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73aaa2fa-14a2-4be5-ab68-c54efeffcf73",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, lit, when, xxhash64, max as spark_max, min as spark_min, coalesce\n",
        "from pyspark.sql.types import LongType\n",
        "from delta.tables import DeltaTable\n",
        "from datetime import datetime, timedelta\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Load base tables\n",
        "config_df = get_gold_table(\"Configuration\")\n",
        "constituent_df = get_gold_table(\"dm_Constituent\").select(\"ConstituentKey\", \"IsNewDonor\", \"LastDonationDateKey\")\n",
        "donation_df = get_gold_table(\"FactDonation\").select(\"ConstituentKey\", \"DonationDateKey\", \"IsReccuring\")\n",
        "date_df = get_gold_table(\"DimDate\").select(\"DateKey\", \"Date\", \"Year\", \"FiscalYear\")\n",
        "segment_df = get_gold_table(\"DimConstituentSegment\").alias(\"seg\")\n",
        "segment_type_df = get_gold_table(\"DimConstituentSegmentType\").alias(\"stype\")\n",
        "\n",
        "# Get Gift Recurrance segment type key\n",
        "gift_type_key = segment_type_df.filter(col(\"ConstituentSegmentType\") == \"Gift Recurrance\") \\\n",
        "    .select(\"ConstituentSegmentTypeKey\").first()[\"ConstituentSegmentTypeKey\"]\n",
        "\n",
        "# Filter only Gift Recurrance segments\n",
        "segment_df = segment_df.filter(col(\"seg.TypeKey\") == lit(gift_type_key)) \\\n",
        "    .select(\"ConstituentSegmentKey\", \"ConstituentSegmentName\")\n",
        "\n",
        "segment_key_map = {\n",
        "    row[\"ConstituentSegmentName\"].strip().lower(): row[\"ConstituentSegmentKey\"]\n",
        "    for row in segment_df.collect()\n",
        "}\n",
        "\n",
        "# Date setup\n",
        "fiscal_start_month = int(config_df.filter(col(\"Name\") == \"FiscalYearStartMonth\").select(\"Value\").first()[\"Value\"])\n",
        "today = datetime.today()\n",
        "one_year_ago = today - timedelta(days=365)\n",
        "two_years_ago = today - timedelta(days=730)\n",
        "current_year = today.year\n",
        "previous_year = current_year - 1\n",
        "\n",
        "# Determine fiscal year\n",
        "fiscal_today = date_df.filter(col(\"Date\") == lit(today.date())).select(\"FiscalYear\").first()\n",
        "fiscal_year = fiscal_today[\"FiscalYear\"] if fiscal_today else current_year\n",
        "fiscal_prev_year = fiscal_year - 1\n",
        "\n",
        "# Join donations with DimDate\n",
        "joined_donations = donation_df.join(date_df, donation_df.DonationDateKey == date_df.DateKey, \"left\") \\\n",
        "    .select(\"ConstituentKey\", \"DonationDateKey\", \"IsReccuring\", \"Year\", \"FiscalYear\", \"Date\")\n",
        "\n",
        "# Aggregate metrics\n",
        "agg_df = joined_donations.groupBy(\"ConstituentKey\").agg(\n",
        "    spark_max(when(col(\"Date\") >= lit(one_year_ago), lit(1))).alias(\"HasRecentDonation\"),\n",
        "    spark_max(when(col(\"Date\") >= lit(two_years_ago), lit(1))).alias(\"HasDonation24m\"),\n",
        "    spark_min(when(col(\"Date\") < lit(two_years_ago), lit(1))).alias(\"HasOldDonation\"),\n",
        "    spark_max(when(col(\"IsReccuring\") == True, lit(1))).alias(\"HasRecurring\"),\n",
        "    spark_max(when(col(\"Year\") == previous_year, lit(1))).alias(\"HasPrevYearCY\"),\n",
        "    spark_max(when(col(\"Year\") == current_year, lit(1))).alias(\"HasCurrYearCY\"),\n",
        "    spark_max(when(col(\"FiscalYear\") == fiscal_prev_year, lit(1))).alias(\"HasPrevYearFY\"),\n",
        "    spark_max(when(col(\"FiscalYear\") == fiscal_year, lit(1))).alias(\"HasCurrYearFY\")\n",
        ")\n",
        "\n",
        "# Classify into segments\n",
        "classified_df = constituent_df.join(agg_df, \"ConstituentKey\", \"left\")\n",
        "\n",
        "multi_segment_df = classified_df.select(\n",
        "    \"ConstituentKey\", \"IsNewDonor\", \"LastDonationDateKey\",\n",
        "    \"HasRecentDonation\", \"HasRecurring\", \"HasPrevYearCY\", \"HasCurrYearCY\",\n",
        "    \"HasPrevYearFY\", \"HasCurrYearFY\", \"HasDonation24m\", \"HasOldDonation\"\n",
        ")\n",
        "\n",
        "segment_rows = []\n",
        "\n",
        "if \"new donor\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter(col(\"IsNewDonor\") == True)\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"new donor\"])))\n",
        "\n",
        "if \"recurring donor\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter(col(\"HasRecurring\") == 1)\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"recurring donor\"])))\n",
        "\n",
        "if \"active\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter(col(\"HasRecentDonation\") == 1)\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"active\"])))\n",
        "\n",
        "if \"lybnt t12m\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter((col(\"HasPrevYearCY\") == 1) & (coalesce(col(\"HasRecentDonation\"), lit(0)) != 1))\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"lybnt t12m\"])))\n",
        "\n",
        "if \"lybnt cy\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter((col(\"HasPrevYearCY\") == 1) & (coalesce(col(\"HasCurrYearCY\"), lit(0)) != 1))\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"lybnt cy\"])))\n",
        "\n",
        "if \"lybnt fy\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter((col(\"HasPrevYearFY\") == 1) & (coalesce(col(\"HasCurrYearFY\"), lit(0)) != 1))\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"lybnt fy\"])))\n",
        "\n",
        "if \"lapsed donor\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter((coalesce(col(\"HasDonation24m\"), lit(0)) != 1) & (col(\"HasOldDonation\") == 1))\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"lapsed donor\"])))\n",
        "\n",
        "if \"prospect\" in segment_key_map:\n",
        "    segment_rows.append(multi_segment_df.filter(col(\"LastDonationDateKey\").isNull())\n",
        "                        .withColumn(\"ConstituentSegmentKey\", lit(segment_key_map[\"prospect\"])))\n",
        "\n",
        "# Combine all rows\n",
        "union_df = reduce(DataFrame.unionAll, segment_rows)\n",
        "\n",
        "# Final bridge DF\n",
        "new_bridge_df = union_df.select(\"ConstituentKey\", \"ConstituentSegmentKey\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .withColumn(\"ConstituentSegmentBridgeKey\", xxhash64(col(\"ConstituentKey\"), col(\"ConstituentSegmentKey\")).cast(\"bigint\")) \\\n",
        "    .withColumn(\"ConstituentKey\", col(\"ConstituentKey\").cast(LongType())) \\\n",
        "    .withColumn(\"ConstituentSegmentKey\", col(\"ConstituentSegmentKey\").cast(LongType())) \\\n",
        "    .withColumn(\"ConstituentSegmentBridgeKey\", col(\"ConstituentSegmentBridgeKey\").cast(LongType()))\n",
        "\n",
        "# Remove old values from bridge\n",
        "bridge_table = DeltaTable.forName(spark, f\"{gold_lakehouse_name}.DimConstituentSegmentBridge\")\n",
        "bridge_table.delete(f\"ConstituentSegmentKey IN ({','.join(map(str, segment_key_map.values()))}) AND ConstituentSegmentMappingId IS NULL\")\n",
        "\n",
        "# Insert updated bridge records\n",
        "new_bridge_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(f\"{gold_lakehouse_name}.DimConstituentSegmentBridge\")\n",
        "\n",
        "# Optional debug preview\n",
        "constituent_named_df = get_gold_table(\"dm_Constituent\").select(\"ConstituentKey\", \"ConstituentName\")\n",
        "segment_type_df = get_gold_table(\"DimConstituentSegmentType\")\n",
        "\n",
        "output_df = new_bridge_df \\\n",
        "    .join(segment_df, \"ConstituentSegmentKey\", \"left\") \\\n",
        "    .join(segment_type_df, segment_type_df[\"ConstituentSegmentTypeKey\"] == gift_type_key, \"left\") \\\n",
        "    .join(constituent_named_df, \"ConstituentKey\", \"left\") \\\n",
        "    .select(\"ConstituentKey\", \"ConstituentName\", \"ConstituentSegmentKey\", \"ConstituentSegmentType\", \"ConstituentSegmentName\")\n",
        "\n",
        "display(output_df.orderBy(\"ConstituentKey\").limit(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a3eca6-2c19-4c34-a2ec-7343322ebb49",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Test Gift Recurrance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adde3dd2-6242-4ded-acb4-fd22f20c48d2",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "def show_donations_by_segment(segment_name: str, donation_columns=None):\n",
        "    \"\"\"\n",
        "    Display donations for the first constituent that belongs to the given segment name.\n",
        "    \n",
        "    :param segment_name: Name of the segment in DimConstituentSegment (e.g., \"Recurring Donor\")\n",
        "    :param donation_columns: Optional list of columns to select from FactDonation\n",
        "    \"\"\"\n",
        "    if donation_columns is None:\n",
        "        donation_columns = [\n",
        "            \"Amount\",\n",
        "            \"ConstituentKey\",\n",
        "            \"DonationDateKey\",\n",
        "            \"DonationId\",\n",
        "            \"DonationKey\",\n",
        "            \"DonationName\",\n",
        "            \"IsReccuring\",\n",
        "            \"SourceKey\"\n",
        "        ]\n",
        "\n",
        "    # Find segment key\n",
        "    segment_key_row = get_gold_table(\"DimConstituentSegment\") \\\n",
        "        .filter(col(\"ConstituentSegmentName\") == segment_name) \\\n",
        "        .select(\"ConstituentSegmentKey\") \\\n",
        "        .first()\n",
        "\n",
        "    if segment_key_row is None:\n",
        "        print(f\"‚ö†Ô∏è Segment '{segment_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    segment_key = segment_key_row[\"ConstituentSegmentKey\"]\n",
        "\n",
        "    # Find first constituent in this segment\n",
        "    constituent_row = get_gold_table(\"DimConstituentSegmentBridge\") \\\n",
        "        .filter(col(\"ConstituentSegmentKey\") == segment_key) \\\n",
        "        .select(\"ConstituentKey\") \\\n",
        "        .first()\n",
        "\n",
        "    if constituent_row is None:\n",
        "        print(f\"‚ö†Ô∏è No constituent found in segment '{segment_name}'.\")\n",
        "        return\n",
        "\n",
        "    constituent_key = constituent_row[\"ConstituentKey\"]\n",
        "\n",
        "    # Display donations for this constituent\n",
        "    donation_debug_df = get_gold_table(\"FactDonation\") \\\n",
        "        .filter(col(\"ConstituentKey\") == constituent_key) \\\n",
        "        .select(*donation_columns)\n",
        "\n",
        "    print(f\"‚úÖ Showing donations for ConstituentKey = {constituent_key} in segment '{segment_name}':\")\n",
        "    display(donation_debug_df)\n",
        "\n",
        "    const_df = get_gold_table(\"dm_Constituent\").filter(col(\"ConstituentKey\") == constituent_key)\n",
        "    display(const_df.limit(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa6eb72-2caf-4621-89d3-44e78012fd8d",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: Recurring Donor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad94ad30-b912-47e6-a184-adf769f29eaa",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "has at least one recurring transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2a8072-692c-4d98-b618-1a2a47eb83f2",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"Recurring Donor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78108d8-8443-49bd-b6ea-43fd594007fb",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: New Donor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27cd9f74-5b27-4e64-acc3-caae28e14d6a",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "IsNewDonor == TRUE (first gift within last 12 months)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7241c8b-a784-4738-8559-f7aed03fcca2",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"New Donor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35ae11a1-53a7-457a-ae2c-2d7f4d4bdeee",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: Lapsed Donor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee5877a8-af0f-4031-b421-067e7d1b399c",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "no transaction in last 24 months but there are older ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022c2c1a-748b-4114-a144-70c0a6b14491",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"Lapsed Donor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b321cabb-5a7e-41d6-9ff5-0b1163980fd2",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: Active"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd0029d-6379-4961-8cd1-ca89317b4a1d",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "has transaction in last 12 months "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652a5bf5-2688-4b6e-ae4a-352781be3f48",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"Active\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926fe40d-9a65-4697-95e0-50fb5775575e",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: LYBNT T12M "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0258a91-677f-4619-bed2-ef7f7dcf9a62",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"LYBNT T12M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ce8cb9-ea11-4d7b-a4d0-bc472d13c1d9",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: LYBNT CY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8777b912-b3cb-49fd-b0f1-3c409a2ddc4c",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"LYBNT CY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7460792-a0c4-48cf-9f4b-ec8a895b10a4",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: LYBNT FY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0bb6e52-9296-454a-bd48-f0676dea346d",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"LYBNT FY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fbd7730-2a13-4978-b7e2-0afc31e3234b",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "### Test: Prospect"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cc0623-4ee0-4187-81cb-c856401a0303",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "no donations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acde381-f354-4121-ab17-dc79caf6b33e",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "show_donations_by_segment(\"Prospect\")"
      ]
    }
  ],
  "metadata": {
    "a365ComputeOptions": null,
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "{GOLD_LAKEHOUSE_ID}",
        "default_lakehouse_name": "{GOLD_LAKEHOUSE_NAME}",
        "known_lakehouses": [
          {
            "id": "{GOLD_LAKEHOUSE_ID}"
          }
        ]
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "synapse_pyspark",
      "language": null,
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "sessionKeepAliveTimeout": 0,
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}