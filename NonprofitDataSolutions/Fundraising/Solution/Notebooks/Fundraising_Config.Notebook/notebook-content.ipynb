{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4966af6-4f65-4324-ac9a-73fcde772a79",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Default configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd6e4f1-fc25-4e02-9964-b6854a613cc4",
      "metadata": {
        "editable": false,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "silver_lakehouse_name = \"{SILVER_LAKEHOUSE_NAME}\"\n",
        "gold_lakehouse_name = \"{GOLD_LAKEHOUSE_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ed99fd-e629-417a-90f3-d2d3bd481131",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime, timezone\n",
        "from itertools import product\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from pyspark.sql.functions import lit, max\n",
        "import logging\n",
        "from typing import Optional\n",
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Set Spark configuration\n",
        "spark.conf.set(\"spark.sql.caseSensitive\", True)\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81084644-1cc4-4a4e-9f0f-be6cad91c3ae",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac1d63e-a76d-4849-8e44-2bccddbe4f07",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "def get_full_table_name(lakehouse: str, table_name: str):\n",
        "    return f\"{lakehouse}.{table_name}\"\n",
        "\n",
        "def get_table_from_lakehouse(lakehouse: str, table_name: str):\n",
        "    return spark.read.table(get_full_table_name(lakehouse, table_name))\n",
        "\n",
        "def get_silver_table(table_name: str):\n",
        "    return get_table_from_lakehouse(silver_lakehouse_name, table_name)\n",
        "\n",
        "def get_gold_table(table_name: str):\n",
        "    return get_table_from_lakehouse(gold_lakehouse_name, table_name)\n",
        "\n",
        "def get_lakehouse_table_counts(lakehouse_name: str, tables: List[str]):\n",
        "    \"\"\"\n",
        "    Returns a Spark DataFrame with table names, record counts, and any errors\n",
        "    from a Microsoft Fabric Lakehouse, based on a provided list of table names.\n",
        "\n",
        "    :param lakehouse_name: Name of the Lakehouse (used as the Spark catalog)\n",
        "    :param tables: List of table names to check\n",
        "    :return: Spark DataFrame with columns: Table, RecordCount, Error\n",
        "    \"\"\"\n",
        "\n",
        "    results: List[Dict[str, Optional[str]]] = []\n",
        "\n",
        "    for table in tables:\n",
        "        try:\n",
        "            full_table_name = get_full_table_name(lakehouse_name, table)\n",
        "            count = spark.sql(f\"SELECT COUNT(*) as count FROM {full_table_name}\").collect()[0][\"count\"]\n",
        "            \n",
        "            results.append({\n",
        "                \"Table\": table,\n",
        "                \"RecordCount\": count,\n",
        "                \"Error\": None\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                \"Table\": table,\n",
        "                \"RecordCount\": None,\n",
        "                \"Error\": str(e)\n",
        "            })\n",
        "\n",
        "    # Convert to pandas then to Spark DataFrame with explicit schema\n",
        "    df_pd = pd.DataFrame(results)\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"Table\", StringType(), True),\n",
        "        StructField(\"RecordCount\", LongType(), True),\n",
        "        StructField(\"Error\", StringType(), True),\n",
        "    ])\n",
        "\n",
        "    return spark.createDataFrame(df_pd, schema=schema)\n",
        "\n",
        "def table_exists(full_table_name: str) -> bool:\n",
        "    \"\"\"\n",
        "    Checks whether a table exists in Spark catalog.\n",
        "\n",
        "    Args:\n",
        "        full_table_name (str): Fully qualified table name, e.g. \"lakehouse.table_name\"\n",
        "\n",
        "    Returns:\n",
        "        bool: True if table exists, False otherwise.\n",
        "    \"\"\"\n",
        "    return spark.catalog.tableExists(full_table_name)\n",
        "\n",
        "def have_same_columns(df1: DataFrame, df2: DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if two DataFrames have the same set of columns (names only).\n",
        "    Order does not matter.\n",
        "    \"\"\"\n",
        "    cols1 = set(df1.columns)\n",
        "    cols2 = set(df2.columns)\n",
        "    return cols1 == cols2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051e8f39-7017-43bc-bc6a-aa4ac3f1d6bc",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Change Data Feed functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e46c7b0-09d3-4fe2-89c6-2a7c0b30a4fe",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "The transformation notebook must be attached to the source lakehouse for correct working of `DESRIBE HISTORY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336f95aa-ae77-4f09-87b8-75bd10973e3a",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Optional\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
        "\n",
        "CDF_STATE_TABLE_NAME = \"ChangeDataFeedState\"\n",
        "\n",
        "@dataclass\n",
        "class CdfTable:\n",
        "    source_lakehouse: str\n",
        "    target_lakehouse: str\n",
        "    \n",
        "    columns: List[str]\n",
        "    merge_sql_template: str\n",
        "    enrich_func: Optional[Callable[[DataFrame], DataFrame]] = None\n",
        "\n",
        "    source_table_name: Optional[str] = None\n",
        "    source_primary_key: Optional[str] = None\n",
        "    target_table_name: Optional[str] = None\n",
        "\n",
        "    name: Optional[str] = None   # DELETE\n",
        "    primary_key: Optional[str] = None # DELETE\n",
        "\n",
        "    hard_delete: bool = False\n",
        "    delete_on: Optional[str] = None\n",
        "\n",
        "    # optional hook to refine/validate delete keys (Id, SourceId)\n",
        "    delete_keys_func: Optional[Callable[[DataFrame, \"CdfTable\"], DataFrame]] = None\n",
        "\n",
        "    # technical mapping hook (required if source PK != target PK)\n",
        "    delete_key_mapper: Optional[Callable[[DataFrame, \"CdfTable\"], DataFrame]] = None\n",
        "\n",
        "def EnsureChangeDataFeedStateTable(lakehouse_name: str):\n",
        "    full_table_name = f\"{lakehouse_name}.{CDF_STATE_TABLE_NAME}\"\n",
        "\n",
        "    if not table_exists(full_table_name):\n",
        "        logging.info(f\"âœ… Creating table: {full_table_name}\")\n",
        "        \n",
        "        schema = StructType([\n",
        "            StructField(\"source_table_name\", StringType(), True),\n",
        "            StructField(\"target_table_name\", StringType(), True),\n",
        "            StructField(\"last_processed_commit_version\", LongType(), True),\n",
        "            StructField(\"last_processed_timestamp\", TimestampType(), True)\n",
        "        ])\n",
        "\n",
        "        empty_df = spark.createDataFrame([], schema)\n",
        "\n",
        "        empty_df.write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .format(\"delta\") \\\n",
        "            .saveAsTable(full_table_name)\n",
        "    else:\n",
        "        logging.info(f\"â„¹ï¸ Table already exists: {full_table_name}\")\n",
        "\n",
        "def GetLastProcessedDataVersion(target_lakehouse_name: str, source_table_name: str, target_table_name: str):\n",
        "    fullHistoryDf = spark.sql(f\"DESCRIBE HISTORY {source_table_name}\")\n",
        "\n",
        "    cdf_state_df = spark.read.format(\"delta\") \\\n",
        "        .table(f\"{target_lakehouse_name}.{CDF_STATE_TABLE_NAME}\") \\\n",
        "        .where((col(\"source_table_name\") == source_table_name) & \n",
        "               (col(\"target_table_name\") == target_table_name))\n",
        "\n",
        "    lastProcessedEntityVersion = (\n",
        "        cdf_state_df\n",
        "        .select(max(\"last_processed_commit_version\"))\n",
        "        .collect()[0][0]\n",
        "    )\n",
        "\n",
        "    nextVersionToProcess = lastProcessedEntityVersion + 1 if lastProcessedEntityVersion is not None else 0\n",
        "    latestMaxCommitVersion = fullHistoryDf.select(max(\"version\")).collect()[0][0]\n",
        "\n",
        "    return (nextVersionToProcess, latestMaxCommitVersion)\n",
        "\n",
        "def UpdateChangeDataFeedTable(target_lakehouse_name: str, source_table_name: str, target_table_name: str, latest_max_commit_version: int):\n",
        "    logging.info(f\"âœ… Updating {CDF_STATE_TABLE_NAME} for: {source_table_name} â†’ {target_table_name} in {target_lakehouse_name}\")\n",
        "    current_timestamp_val = datetime.now(timezone.utc)\n",
        "\n",
        "    spark.createDataFrame(\n",
        "        [(source_table_name, target_table_name, latest_max_commit_version, current_timestamp_val)],\n",
        "        [\"source_table_name\", \"target_table_name\", \"last_processed_commit_version\", \"last_processed_timestamp\"]\n",
        "    ).write.mode(\"append\").format(\"delta\").saveAsTable(f\"{target_lakehouse_name}.{CDF_STATE_TABLE_NAME}\")\n",
        "\n",
        "def get_source_id_by_name(source_name: str, lakehouse: str = silver_lakehouse_name) -> str:\n",
        "    \"\"\"\n",
        "    Look up the SourceId (GUID) from Silver.Source by source name.\n",
        "    \"\"\"\n",
        "    df = spark.read.table(f\"{lakehouse}.Source\").filter(col(\"Name\") == source_name)\n",
        "\n",
        "    if df.isEmpty():\n",
        "        raise ValueError(f\"âŒ Source with name '{source_name}' not found in {lakehouse}.Source\")\n",
        "\n",
        "    return df.select(\"SourceId\").first()[\"SourceId\"]\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "import logging \n",
        "\n",
        "\n",
        "def log_merge_metrics(table_fullname: str, label: str):\n",
        "    \"\"\"\n",
        "    Logs inserted/updated/deleted counts from the most recent MERGE on `table_fullname`.\n",
        "    Works with runtimes that report either:\n",
        "      - numTargetRowsNotMatchedBySourceDeleted, or\n",
        "      - numTargetRowsDeleted\n",
        "    \"\"\"\n",
        "    row = (spark.sql(f\"DESCRIBE HISTORY {table_fullname}\")\n",
        "             .orderBy(F.col(\"version\").desc())\n",
        "             .limit(1)\n",
        "             .select(\"operation\", \"operationMetrics\")\n",
        "             .collect()[0])\n",
        "\n",
        "    op = row[\"operation\"]\n",
        "    metrics = row[\"operationMetrics\"] or {}\n",
        "\n",
        "    inserted = int(metrics.get(\"numTargetRowsInserted\", \"0\"))\n",
        "    updated  = int(metrics.get(\"numTargetRowsUpdated\",\n",
        "                               metrics.get(\"numTargetRowsMatchedUpdated\", \"0\")))\n",
        "    deleted  = int(metrics.get(\"numTargetRowsNotMatchedBySourceDeleted\",\n",
        "                               metrics.get(\"numTargetRowsDeleted\", \"0\")))\n",
        "\n",
        "    logging.info(f\"ðŸ“Š {label} MERGE: inserted={inserted}, updated={updated}, deleted={deleted}.\")\n",
        "\n",
        "def _is_silver(lakehouse_name: str) -> bool:\n",
        "    try:\n",
        "        return lakehouse_name.lower() == silver_lakehouse_name.lower()\n",
        "    except NameError:\n",
        "        return \"silver\" in lakehouse_name.lower()\n",
        "\n",
        "def _is_gold(lakehouse_name: str) -> bool:\n",
        "    try:\n",
        "        return lakehouse_name.lower() == gold_lakehouse_name.lower()\n",
        "    except NameError:\n",
        "        return \"gold\" in lakehouse_name.lower()\n",
        "\n",
        "def _default_delete_predicate_for_target(target_lakehouse: str, source_primary_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Silver â‡’ identity by (SourceId, SourceSystemId) with *matching names* on k.\n",
        "    Gold   â‡’ identity by table PK name (e.g., t.CampaignId = k.CampaignId).\n",
        "    \"\"\"\n",
        "    if _is_silver(target_lakehouse):\n",
        "        return \"t.SourceId = k.SourceId AND t.SourceSystemId = k.SourceSystemId\"\n",
        "    if not source_primary_key:\n",
        "        raise ValueError(\"Gold default delete needs source_primary_key (e.g., CampaignId).\")\n",
        "    return f\"t.{source_primary_key} = k.{source_primary_key}\"\n",
        "\n",
        "def _predicate_needs(colname: str, predicate_sql: str) -> bool:\n",
        "    return f\"k.{colname}\" in (predicate_sql or \"\")\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "from pyspark.sql import DataFrame, Window\n",
        "from pyspark.sql.functions import col, lit, max\n",
        "\n",
        "def ProcessCdfTable(table: CdfTable, source_name: Optional[str] = None):\n",
        "    # 0) Resolve names/paths\n",
        "    source_table_name = table.source_table_name or getattr(table, \"name\", None)\n",
        "    source_primary_key = table.source_primary_key or getattr(table, \"primary_key\", None)\n",
        "    target_table_name  = table.target_table_name  or getattr(table, \"name\", None)\n",
        "\n",
        "    if not source_table_name or not target_table_name or not source_primary_key:\n",
        "        raise ValueError(\"CdfTable must define source_table_name, target_table_name, and source_primary_key.\")\n",
        "\n",
        "    entity_path = f\"{table.source_lakehouse}.{source_table_name}\"\n",
        "    temp_view   = f\"latestSnapshot_{source_table_name}\"\n",
        "\n",
        "    # 1) CDF window\n",
        "    next_version, latest_commit = GetLastProcessedDataVersion(\n",
        "        table.target_lakehouse, source_table_name, target_table_name\n",
        "    )\n",
        "    if next_version > latest_commit:\n",
        "        logging.info(f\"âœ… {target_table_name} is up to date.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"ðŸ”„ Processing {target_table_name} from {table.source_lakehouse} â†’ {table.target_lakehouse} ({next_version} to {latest_commit})\")\n",
        "\n",
        "    # 2) Read CDF for the window (+ PK)\n",
        "    df = (\n",
        "        spark.read.format(\"delta\")\n",
        "            .option(\"readChangeFeed\", \"true\")\n",
        "            .option(\"startingVersion\", next_version)\n",
        "            .option(\"endingVersion\",   latest_commit)\n",
        "            .table(entity_path)\n",
        "            .filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\", \"delete\"))\n",
        "            .select(*table.columns, source_primary_key, \"_commit_version\", \"_change_type\")\n",
        "    )\n",
        "\n",
        "    # 3) Split changes\n",
        "    active_df = df.filter(col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
        "    delete_df = (\n",
        "        df.filter(col(\"_change_type\") == \"delete\")\n",
        "          .select(\n",
        "              col(source_primary_key).alias(source_primary_key),\n",
        "              col(\"_commit_version\").alias(\"del_ver\")\n",
        "          )\n",
        "          .dropDuplicates([source_primary_key, \"del_ver\"])\n",
        "    )\n",
        "\n",
        "    # 4) ----------------- DELETE FIRST -----------------\n",
        "    if getattr(table, \"hard_delete\", False) and not delete_df.rdd.isEmpty():\n",
        "        # 4a) Resolve predicate (explicit > default)\n",
        "        source_pk = source_primary_key\n",
        "        predicate = table.delete_on or _default_delete_predicate_for_target(\n",
        "            table.target_lakehouse, source_pk\n",
        "        )\n",
        "\n",
        "        # 4b) Build keys matching the predicate (Silver vs Gold vs fallback)\n",
        "        if _predicate_needs(\"SourceSystemId\", predicate) and _predicate_needs(\"SourceId\", predicate):\n",
        "            # Silver default: need k.SourceSystemId + k.SourceId\n",
        "            if not source_name:\n",
        "                raise ValueError(\n",
        "                    \"source_name is required because the delete predicate uses k.SourceId \"\n",
        "                    f\"(predicate: {predicate})\"\n",
        "                )\n",
        "            src_id = get_source_id_by_name(source_name)\n",
        "            keys = (\n",
        "                delete_df.select(col(source_pk).alias(\"SourceSystemId\"))\n",
        "                         .withColumn(\"SourceId\", lit(src_id))\n",
        "                         .dropDuplicates()\n",
        "            )\n",
        "\n",
        "        elif _predicate_needs(source_pk, predicate):\n",
        "            # Gold default: need k.<PK> (e.g., k.CampaignId)\n",
        "            keys = delete_df.select(col(source_pk).alias(source_pk)).dropDuplicates()\n",
        "\n",
        "        else:\n",
        "            # Fallback:\n",
        "            # - if predicate needs k.Id, alias PK â†’ \"Id\"\n",
        "            # - else, keep the original PK name (so a mapper can remap it later)\n",
        "            base_col_name = \"Id\" if _predicate_needs(\"Id\", predicate) else source_pk\n",
        "            keys = delete_df.select(col(source_pk).alias(base_col_name)).dropDuplicates()\n",
        "\n",
        "            # attach k.SourceId if the predicate asks for it\n",
        "            if _predicate_needs(\"SourceId\", predicate):\n",
        "                if not source_name:\n",
        "                    raise ValueError(\n",
        "                    \"source_name is required because the delete predicate uses k.SourceId \"\n",
        "                    f\"(predicate: {predicate})\"\n",
        "                    )\n",
        "                src_id = get_source_id_by_name(source_name)\n",
        "                keys = keys.withColumn(\"SourceId\", lit(src_id))\n",
        "\n",
        "\n",
        "        base_cnt = keys.count()\n",
        "\n",
        "        # 4c) Technical key mapping (e.g., ContactId â†’ ConstituentId)\n",
        "        if table.delete_key_mapper is not None and _is_gold(table.target_lakehouse):\n",
        "            keys = table.delete_key_mapper(keys, table)\n",
        "             \n",
        "\n",
        "        # 4d) Customer policy filter (only in Silver, optional)\n",
        "        if table.delete_keys_func is not None and _is_silver(table.target_lakehouse):\n",
        "            keys_before = base_cnt\n",
        "            filtered_keys = table.delete_keys_func(keys, table)\n",
        "\n",
        "            if filtered_keys is None:\n",
        "                logging.info(f\"â„¹ï¸ Delete policy returned None for {table.target_lakehouse}.{target_table_name} â†’ skipping policy\")\n",
        "            else:\n",
        "                keys = filtered_keys\n",
        "                keys_after = keys.count()\n",
        "                excluded = keys_before - keys_after\n",
        "                if excluded > 0:\n",
        "                    logging.info(f\"ðŸ›¡ï¸ Delete policy excluded {excluded} key(s) for {table.target_lakehouse}.{target_table_name}\")\n",
        "\n",
        "             \n",
        "\n",
        "        # 4e) Execute MERGE-DELETE\n",
        "        if keys.rdd.isEmpty():\n",
        "            logging.info(f\"â„¹ï¸ No keys to delete for {table.target_lakehouse}.{target_table_name} after mapping/policy\")\n",
        "        else:\n",
        "            keys.createOrReplaceTempView(\"deleted_keys\")\n",
        "            target_fullname = f\"{table.target_lakehouse}.{target_table_name}\"\n",
        "\n",
        "            delete_count = spark.sql(f\"\"\"\n",
        "                SELECT COUNT(*) AS cnt\n",
        "                FROM {target_fullname} AS t\n",
        "                JOIN deleted_keys AS k\n",
        "                  ON {predicate}\n",
        "            \"\"\").collect()[0][\"cnt\"]\n",
        "\n",
        "            if delete_count > 0:\n",
        "                spark.sql(f\"\"\"\n",
        "                    MERGE INTO {target_fullname} AS t\n",
        "                    USING deleted_keys AS k\n",
        "                    ON {predicate}\n",
        "                    WHEN MATCHED THEN DELETE\n",
        "                \"\"\")\n",
        "                logging.info(f\"ðŸ—‘ï¸ {target_table_name}: Deleted {delete_count} row(s).\")\n",
        "            else:\n",
        "                logging.info(f\"â„¹ï¸ {target_table_name}: no matching rows to delete.\")\n",
        "\n",
        "            spark.catalog.dropTempView(\"deleted_keys\")\n",
        "    # --------------- end delete -----------------------\n",
        "\n",
        "    # 5) Build latest snapshot ONLY from active rows (last image per PK)\n",
        "    # Suppress any active image that is not newer than the delete for that PK\n",
        "    w = Window.partitionBy(source_primary_key)\n",
        "\n",
        "    # Latest active image per PK with its commit version\n",
        "    active_latest = (\n",
        "        active_df\n",
        "        .withColumn(\"act_ver\", F.max(\"_commit_version\").over(w))\n",
        "        .filter(col(\"_commit_version\") == col(\"act_ver\"))\n",
        "        # keep act_ver for the comparison with delete version\n",
        "    )\n",
        "\n",
        "    # Latest delete version per PK (if any)\n",
        "    del_latest = (\n",
        "        delete_df\n",
        "        .groupBy(source_primary_key)\n",
        "        .agg(F.max(\"del_ver\").alias(\"del_ver\"))\n",
        "    )\n",
        "\n",
        "    # Keep active if there is no delete for the PK, or the active is strictly newer than the delete\n",
        "    df_snapshot = (\n",
        "        active_latest.alias(\"a\")\n",
        "        .join(del_latest.alias(\"d\"), on=source_primary_key, how=\"left\")\n",
        "        .filter( (col(\"d.del_ver\").isNull()) | (col(\"a.act_ver\") > col(\"d.del_ver\")) )\n",
        "        .drop(\"act_ver\", \"del_ver\", \"_change_type\", \"_commit_version\")\n",
        "    )\n",
        "\n",
        "\n",
        "    #df_snapshot = (\n",
        "    #    active_df.withColumn(\"maxCommit\", max(\"_commit_version\").over(w))\n",
        "    #             .filter(col(\"_commit_version\") == col(\"maxCommit\"))\n",
        "    #             .drop(\"maxCommit\", \"_change_type\", \"_commit_version\")\n",
        "    #)\n",
        "\n",
        "    # 6) Enrich \n",
        "    src_cnt = df_snapshot.count()\n",
        "    if table.enrich_func:\n",
        "        logging.info(f\"âž¡ï¸ Enriching {source_table_name} â†’ {target_table_name} ({src_cnt} source rows).\")\n",
        "        df_snapshot = table.enrich_func(df_snapshot)\n",
        "        tgt_cnt = df_snapshot.count()\n",
        "        msg = \"same\" if src_cnt == tgt_cnt else f\"changed ({src_cnt} â†’ {tgt_cnt})\"\n",
        "        logging.info(f\"â„¹ï¸ Enrichment row count {msg}.\")\n",
        "\n",
        "    # 7) Merge snapshot into target\n",
        "    df_snapshot.createOrReplaceTempView(temp_view)\n",
        "    if table.merge_sql_template and table.merge_sql_template.strip():\n",
        "        spark.sql(table.merge_sql_template)\n",
        "        logging.info(f\"âœ… Updated {target_table_name}.\")\n",
        "    else:\n",
        "        logging.info(f\"âš ï¸ Skipping merge for {target_table_name} â€“ merge_sql_template is empty or None.\")\n",
        "\n",
        "    # 8) Advance watermark\n",
        "    UpdateChangeDataFeedTable(table.target_lakehouse, source_table_name, target_table_name, latest_commit)\n",
        "    spark.catalog.dropTempView(temp_view)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a179a824-4151-4a25-88fa-690597308ecc",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Data model functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9011ea0-8c3d-467f-8d39-10fd4a98e149",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import concurrent.futures\n",
        "import logging\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.types import StructType, LongType\n",
        "from pyspark.sql.functions import expr, monotonically_increasing_id\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class BaseDataModel(ABC):\n",
        "    def __init__(self, spark: SparkSession, lakehouse: str):\n",
        "        self.spark = spark\n",
        "        self.lakehouse = lakehouse\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_entities(self) -> list[Tuple[StructType, str, Optional[bool]]]:\n",
        "        \"\"\"\n",
        "        Must return a list of (schema, table_name) tuples\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def create_entities(self):\n",
        "        \"\"\"\n",
        "        Create all tables concurrently.\n",
        "        \"\"\"\n",
        "        entities = self.get_entities()\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            futures = [\n",
        "                executor.submit(self.create_table, schema, table_name, enable_change_data_feed)\n",
        "                for schema, table_name, enable_change_data_feed in entities\n",
        "            ]\n",
        "\n",
        "            concurrent.futures.wait(futures)\n",
        "\n",
        "            errors = []\n",
        "            for future in futures:\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as e:\n",
        "                    errors.append(e)\n",
        "\n",
        "            if errors:\n",
        "                error_msgs = \"\\n\".join(str(e) for e in errors)\n",
        "                raise Exception(f\"One or more table creations failed:\\n{error_msgs}\")\n",
        "\n",
        "    def add_primary_key(self, df: DataFrame, schema: StructType) -> (DataFrame, str | None):\n",
        "        pk_field = next(\n",
        "            (f for f in schema.fields if f.metadata.get(\"primaryKey\", False)),\n",
        "            None\n",
        "        )\n",
        "        if pk_field is None:\n",
        "            return df, None\n",
        "\n",
        "        name, dtype, kind = pk_field.name, pk_field.dataType, pk_field.metadata.get(\"pkType\")\n",
        "\n",
        "        # Generate or validate values\n",
        "        if kind == \"guid\":\n",
        "            df = df.withColumn(name, expr(\"uuid()\"))\n",
        "        elif isinstance(dtype, LongType):\n",
        "            df = df.withColumn(name, monotonically_increasing_id())\n",
        "        # Other types: leave unchanged\n",
        "\n",
        "        return df, name\n",
        "\n",
        "    def create_table(self, schema: StructType, table_name: str, enable_change_data_feed: bool = True):\n",
        "        full_table_name = f\"{self.lakehouse}.{table_name}\"\n",
        "\n",
        "        try:\n",
        "            df: DataFrame = self.spark.createDataFrame(data=[], schema=schema)\n",
        "\n",
        "            if table_exists(full_table_name):\n",
        "                df.write.format(\"delta\")\\\n",
        "                        .mode(\"append\")\\\n",
        "                        .option(\"mergeSchema\", \"true\")\\\n",
        "                        .saveAsTable(full_table_name)\n",
        "                logging.info(f\"âœ… Table {full_table_name} updated successfully.\")\n",
        "            else:\n",
        "                df, pk_name = self.add_primary_key(df, schema)\n",
        "                df.write.option(\"delta.enableChangeDataFeed\", (\"false\" if enable_change_data_feed is False else \"true\"))\\\n",
        "                        .format(\"delta\")\\\n",
        "                        .mode(\"overwrite\")\\\n",
        "                        .saveAsTable(full_table_name)\n",
        "                logging.info(f\"âœ… Table {full_table_name} created successfully (EnableChangeDataFeed={enable_change_data_feed}).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"â›” Failed to create or append data to table {full_table_name}: {e}\")\n",
        "            raise\n"
      ]
    }
  ],
  "metadata": {
    "dependencies": {},
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "synapse_pyspark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
