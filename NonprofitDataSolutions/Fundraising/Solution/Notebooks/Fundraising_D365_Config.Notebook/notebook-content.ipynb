{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "6579fec2-a5f5-438f-9639-0d55d3273957",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"# Config"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a5fa0385-c221-46a5-bf77-50af64a74559",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"%run <Fundraising_Config>"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "7dd6e4f1-fc25-4e02-9964-b6854a613cc4",
			"metadata": {
				"editable": true,
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				},
				"tags": [
					"parameters"
				]
			},
			"outputs": [],
			"source": [
				"# Resources\n",
				"bronze_lakehouse_name = \"{DYNAMICS_LAKEHOUSE_NAME}\"\n",
				"\n",
				"# Configuration\n",
				"source_name = \"Dynamics365\"\n",
				"source_id = \"8d0a7c33-2a55-468b-9f12-1a14a43d8a19\""
			]
		},
		{
			"cell_type": "markdown",
			"id": "dbedb082-cbca-47a6-92d2-b8619f120e1a",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Base classes\n",
				"\n",
				"Foundation classes for Dynamics 365 synchronization operations."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "77305452-2708-4647-96e7-ba49f54a43d4",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import DataFrame, functions as F\n",
				"from pyspark.sql.functions import col, expr, lit, lower, trim, substring, current_timestamp\n",
				"from pyspark.sql.types import StringType\n",
				"from typing import Optional, Callable, Tuple\n",
				"from abc import ABC\n",
				"import logging\n",
				"\n",
				"\n",
				"class Dynamics365BaseSync(ABC):\n",
				"    \"\"\"\n",
				"    Abstract base class for Dynamics 365 synchronization operations.\n",
				"    \n",
				"    Provides common initialization for all Dynamics 365 sync classes.\n",
				"    \"\"\"\n",
				"    \n",
				"    def __init__(self, source_id: str, source_lakehouse: str, target_lakehouse: str):\n",
				"        \"\"\"\n",
				"        Initialize the Dynamics 365 synchronizer.\n",
				"        \n",
				"        Args:\n",
				"            source_id: SourceId GUID for this data source\n",
				"            source_lakehouse: Bronze lakehouse name\n",
				"            target_lakehouse: Silver lakehouse name\n",
				"        \"\"\"\n",
				"        self.source_id = source_id\n",
				"        self.source_lakehouse = source_lakehouse\n",
				"        self.target_lakehouse = target_lakehouse\n",
				"        self.mapping_table = f\"{target_lakehouse}.SourceSystemIdMapping\""
			]
		},
		{
			"cell_type": "markdown",
			"id": "16ab5247-0b7f-4c6a-b5f7-ca2e5841e47e",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Dynamics 365 option set synchronization\n",
				"\n",
				"Synchronizes Dynamics 365 option sets to Silver dimension tables.\n",
				"Performs full compare-and-sync: inserts new, updates changed, deletes removed options."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "696df5e3-19d9-45f6-ae7d-9f0531977c6f",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from typing import Optional, Callable, Tuple\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql import functions as F\n",
				"from pyspark.sql.functions import col, lit, lower, trim, substring, current_timestamp, expr\n",
				"from pyspark.sql.types import StringType\n",
				"import logging\n",
				"\n",
				"class Dynamics365OptionsetSync:\n",
				"    def __init__(\n",
				"        self,\n",
				"        *,\n",
				"        source_id: str,\n",
				"        source_lakehouse: str,\n",
				"        target_lakehouse: str,\n",
				"        mapping_table: Optional[str] = None,\n",
				"    ) -> None:\n",
				"        self.source_id = source_id\n",
				"        self.source_lakehouse = source_lakehouse\n",
				"        self.target_lakehouse = target_lakehouse\n",
				"        # use provided mapping table or default\n",
				"        self.mapping_table = mapping_table or f\"{target_lakehouse}.SourceSystemIdMapping\"\n",
				"\n",
				"    def sync_optionset(\n",
				"        self,\n",
				"        entity_name: str,\n",
				"        optionset_name: str,\n",
				"        target_table: str,\n",
				"        target_primary_key: str,\n",
				"        is_global: bool = False,\n",
				"        transform_func: Optional[Callable[[DataFrame], DataFrame]] = None,\n",
				"    ) -> None:\n",
				"        logging.info(f\"üîÑ Syncing optionset: {entity_name}.{optionset_name} ‚Üí {target_table}\")\n",
				"\n",
				"        # 1) Bronze snapshot ‚Üí canonical (Name, SourceSystemId, SourceId, Option for transforms)\n",
				"        bronze_opts = self._read_bronze_options(entity_name, optionset_name, is_global)\n",
				"        if transform_func is not None:\n",
				"            logging.info(\"üîß Applying custom transformation to optionset labels\")\n",
				"            bronze_opts = transform_func(bronze_opts)\n",
				"\n",
				"        bronze_opts = (\n",
				"            bronze_opts.select(\"Name\", \"SourceSystemId\")\n",
				"                       .dropna(subset=[\"Name\", \"SourceSystemId\"])\n",
				"                       .dropDuplicates([\"SourceSystemId\"])\n",
				"                       .withColumn(\"SourceId\", F.lit(self.source_id))\n",
				"        )\n",
				"\n",
				"        count_b = bronze_opts.count()\n",
				"        logging.info(f\"üìä Bronze options count: {count_b}\")\n",
				"        if count_b == 0:\n",
				"            logging.warning(\"‚ö†Ô∏è No options found ‚Äî skipping\")\n",
				"            return\n",
				"\n",
				"        # 2) UPDATE (creator-owned only)\n",
				"        upd_cnt = self._update_changed_options(bronze_opts, target_table, target_primary_key)\n",
				"\n",
				"        # 3) INSERT (name-share discovery ‚Üí mapping; then insert truly-new)\n",
				"        ins_cnt = self._insert_new_options(bronze_opts, target_table, target_primary_key)\n",
				"\n",
				"        logging.info(\n",
				"            f\"‚úÖ Optionset sync complete: {target_table} (insert={ins_cnt}, update={upd_cnt})\"\n",
				"        )\n",
				"\n",
				"\n",
				"    def _resolve_source_table(self, optionset_name: str, is_global: bool) -> Tuple[str, str]:\n",
				"        opt = (optionset_name or \"\").lower()\n",
				"        if opt == \"statecode\":\n",
				"            return \"StateMetadata\", \"State\"\n",
				"        if opt == \"statuscode\":\n",
				"            return \"StatusMetadata\", \"Status\"\n",
				"        if is_global:\n",
				"            return \"GlobalOptionsetMetadata\", \"Option\"\n",
				"        return \"OptionsetMetadata\", \"Option\"\n",
				"\n",
				"    def _language_filter(self, df):\n",
				"        if \"LocalizedLabelLanguageId\" not in df.columns:\n",
				"            return df  # no language column -> no-op\n",
				"        lang_col = \"LocalizedLabelLanguageId\"\n",
				"        langs = [r[0] for r in df.select(lang_col).where(col(lang_col).isNotNull()).distinct().collect()]\n",
				"        if not langs:\n",
				"            return df\n",
				"        chosen = 1033 if 1033 in langs else langs[0]\n",
				"        return df.where(col(lang_col) == lit(chosen))\n",
				"\n",
				"\n",
				"    def _read_bronze_options(self, entity_name: str, optionset_name: str, is_global: bool) -> DataFrame:\n",
				"        source_table, value_col = self._resolve_source_table(optionset_name, is_global)\n",
				"        src_full = f\"{self.source_lakehouse}.{source_table}\"\n",
				"        df = spark.read.table(src_full)\n",
				"\n",
				"        # Language smart-pick\n",
				"        df = self._language_filter(df)\n",
				"\n",
				"        # Filter by entity/optionset when present (entity ignored for globals)\n",
				"        if entity_name and \"EntityName\" in df.columns:\n",
				"            df = df.where(lower(col(\"EntityName\")) == F.lit(entity_name.lower()))\n",
				"        if optionset_name and \"OptionSetName\" in df.columns:\n",
				"            df = df.where(lower(col(\"OptionSetName\")) == F.lit(optionset_name.lower()))\n",
				"\n",
				"        # Project: Name, SourceSystemId, and always expose Option for transforms\n",
				"        return (\n",
				"            df.select(\n",
				"                substring(trim(col(\"LocalizedLabel\")), 1, 500).alias(\"Name\"),\n",
				"                col(value_col).cast(StringType()).alias(\"SourceSystemId\"),\n",
				"                col(value_col).alias(\"Option\"),  # unified for transforms\n",
				"            )\n",
				"            .dropna(subset=[\"Name\", \"SourceSystemId\"])\n",
				"            .dropDuplicates([\"SourceSystemId\"])\n",
				"        )\n",
				"\n",
				"    def _read_silver_state(self, target_table: str, target_pk: str) -> DataFrame:\n",
				"        target_full  = f\"{self.target_lakehouse}.{target_table}\"\n",
				"        mapping_full = self.mapping_table\n",
				"\n",
				"        t = spark.read.table(target_full).alias(\"t\")\n",
				"        m = (\n",
				"            spark.read.table(mapping_full).alias(\"m\")\n",
				"                .where((col(\"m.SourceId\") == lit(self.source_id)) & (col(\"m.SourceTable\") == lit(target_table)))\n",
				"                .select(col(\"m.SilverRecordId\"), col(\"m.SourceSystemId\"))\n",
				"        )\n",
				"        return (\n",
				"            t.join(m, col(f\"t.{target_pk}\") == col(\"m.SilverRecordId\"), \"inner\")\n",
				"             .select(\n",
				"                 col(f\"t.{target_pk}\").alias(\"SilverRecordId\"),\n",
				"                 col(\"m.SourceSystemId\").alias(\"SourceSystemId\"),\n",
				"                 col(\"t.Name\").alias(\"Name\")\n",
				"             )\n",
				"        )\n",
				"\n",
				"    def _update_changed_options(self, bronze_opts: DataFrame, target_table: str, target_pk: str) -> int:\n",
				"        target_full = f\"{self.target_lakehouse}.{target_table}\"\n",
				"        silver_cur = self._read_silver_state(target_table, target_pk)\n",
				"        creators = spark.read.table(target_full).select(\n",
				"            col(target_pk).alias(\"SilverRecordId\"),\n",
				"            col(\"SourceId\").alias(\"CreatorSourceId\")\n",
				"        )\n",
				"\n",
				"        to_upd = (\n",
				"            silver_cur.alias(\"s\")\n",
				"            .join(creators.alias(\"t\"), col(\"s.SilverRecordId\") == col(\"t.SilverRecordId\"), \"inner\")\n",
				"            .join(bronze_opts.alias(\"b\"), col(\"s.SourceSystemId\") == col(\"b.SourceSystemId\"), \"inner\")\n",
				"            .where(col(\"t.CreatorSourceId\") == lit(self.source_id))\n",
				"            .where(col(\"s.Name\") != col(\"b.Name\"))\n",
				"            .select(\n",
				"                col(\"s.SilverRecordId\").alias(target_pk),\n",
				"                col(\"b.Name\").alias(\"Name\"),\n",
				"                current_timestamp().alias(\"ModifiedDate\")\n",
				"            )\n",
				"        )\n",
				"\n",
				"        cnt = to_upd.count()\n",
				"        if cnt > 0:\n",
				"            to_upd.createOrReplaceTempView(\"_opt_upd\")\n",
				"            spark.sql(f\"\"\"\n",
				"                MERGE INTO {target_full} AS t\n",
				"                USING _opt_upd AS s\n",
				"                ON t.{target_pk} = s.{target_pk}\n",
				"                WHEN MATCHED THEN UPDATE SET t.Name = s.Name, t.ModifiedDate = s.ModifiedDate\n",
				"            \"\"\")\n",
				"            spark.catalog.dropTempView(\"_opt_upd\")\n",
				"        return cnt\n",
				"\n",
				"    def _insert_new_options(self, bronze_opts: DataFrame, target_table: str, target_pk: str) -> int:\n",
				"        target_full = f\"{self.target_lakehouse}.{target_table}\"\n",
				"        mapping_full = self.mapping_table\n",
				"        tgt = spark.read.table(target_full)\n",
				"\n",
				"        # Normalize names\n",
				"        b_norm = bronze_opts.withColumn(\"__nm__\", lower(trim(col(\"Name\"))))\n",
				"\n",
				"        # Deduplicate target by normalized name (choose earliest CreatedDate, else lowest PK)\n",
				"        if \"CreatedDate\" in tgt.columns:\n",
				"            t_norm_all = tgt.select(\n",
				"                lower(trim(col(\"Name\"))).alias(\"__nm__\"),\n",
				"                col(target_pk).alias(\"SilverRecordId\"),\n",
				"                col(\"CreatedDate\")\n",
				"            )\n",
				"            w = Window.partitionBy(\"__nm__\").orderBy(col(\"CreatedDate\").asc_nulls_last())\n",
				"        else:\n",
				"            t_norm_all = tgt.select(\n",
				"                lower(trim(col(\"Name\"))).alias(\"__nm__\"),\n",
				"                col(target_pk).alias(\"SilverRecordId\")\n",
				"            ).withColumn(\"CreatedDate\", lit(None).cast(\"timestamp\"))\n",
				"            w = Window.partitionBy(\"__nm__\").orderBy(col(\"SilverRecordId\").asc())\n",
				"\n",
				"        t_norm = (\n",
				"            t_norm_all\n",
				"            .withColumn(\"_rn\", row_number().over(w))\n",
				"            .filter(col(\"_rn\") == 1)\n",
				"            .select(\"__nm__\", \"SilverRecordId\")\n",
				"        )\n",
				"\n",
				"        # 3A) Name-share discovery ‚Üí MERGE mappings \n",
				"        nm_pairs = (\n",
				"            b_norm.alias(\"b\").join(t_norm.alias(\"t\"), on=\"__nm__\", how=\"inner\")\n",
				"                 .select(\n",
				"                     col(\"t.SilverRecordId\").alias(\"SilverRecordId\"),\n",
				"                     lit(self.source_id).alias(\"SourceId\"),\n",
				"                     col(\"b.SourceSystemId\").alias(\"SourceSystemId\"),\n",
				"                     lit(target_table).alias(\"SourceTable\")\n",
				"                 )\n",
				"                 .dropDuplicates([\"SilverRecordId\",\"SourceId\",\"SourceSystemId\",\"SourceTable\"])\n",
				"        )\n",
				"        nm_pairs.createOrReplaceTempView(\"_map_nm_pairs\")\n",
				"\n",
				"        spark.sql(f\"\"\"\n",
				"            MERGE INTO {mapping_full} AS m\n",
				"            USING _map_nm_pairs AS s\n",
				"            ON  m.SilverRecordId = s.SilverRecordId\n",
				"            AND m.SourceId       = s.SourceId\n",
				"            AND m.SourceSystemId = s.SourceSystemId\n",
				"            AND m.SourceTable    = s.SourceTable\n",
				"            WHEN NOT MATCHED THEN\n",
				"              INSERT (SilverRecordId, SourceId, SourceSystemId, SourceTable)\n",
				"              VALUES (s.SilverRecordId, s.SourceId, s.SourceSystemId, s.SourceTable)\n",
				"        \"\"\")\n",
				"        spark.catalog.dropTempView(\"_map_nm_pairs\")\n",
				"\n",
				"        # 3B) Truly new ‚Üí insert target + MERGE mappings\n",
				"        new_rows = (\n",
				"            b_norm.alias(\"b\").join(t_norm.alias(\"t\"), on=\"__nm__\", how=\"left_anti\")\n",
				"                 .select(col(\"b.Name\"), col(\"b.SourceSystemId\"))\n",
				"                 .dropDuplicates([\"SourceSystemId\"])   # safety: one per source code\n",
				"        )\n",
				"\n",
				"        to_ins = new_rows.select(\n",
				"            expr(\"uuid()\").alias(target_pk),\n",
				"            col(\"Name\"),\n",
				"            current_timestamp().alias(\"CreatedDate\"),\n",
				"            current_timestamp().alias(\"ModifiedDate\"),\n",
				"            lit(self.source_id).alias(\"SourceId\"),\n",
				"            col(\"SourceSystemId\")\n",
				"        )\n",
				"\n",
				"        cnt = to_ins.count()\n",
				"        if cnt > 0:\n",
				"            tgt_cols = set(tgt.schema.fieldNames())\n",
				"            wanted = [target_pk, \"Name\", \"CreatedDate\", \"ModifiedDate\", \"SourceId\", \"SourceSystemId\"]\n",
				"            available = [c for c in wanted if c in tgt_cols]\n",
				"            to_ins.select(*available).write.mode(\"append\").format(\"delta\").saveAsTable(target_full)\n",
				"\n",
				"            # Build mapping for newly inserted rows\n",
				"            to_ins.createOrReplaceTempView(\"_map_new_rows\")\n",
				"            spark.sql(f\"\"\"\n",
				"                MERGE INTO {mapping_full} AS m\n",
				"                USING (\n",
				"                  SELECT\n",
				"                    {target_pk}     AS SilverRecordId,\n",
				"                    '{self.source_id}' AS SourceId,\n",
				"                    SourceSystemId   AS SourceSystemId,\n",
				"                    '{target_table}' AS SourceTable\n",
				"                  FROM _map_new_rows\n",
				"                ) AS s\n",
				"                ON  m.SilverRecordId = s.SilverRecordId\n",
				"                AND m.SourceId       = s.SourceId\n",
				"                AND m.SourceSystemId = s.SourceSystemId\n",
				"                AND m.SourceTable    = s.SourceTable\n",
				"                WHEN NOT MATCHED THEN\n",
				"                  INSERT (SilverRecordId, SourceId, SourceSystemId, SourceTable)\n",
				"                  VALUES (s.SilverRecordId, s.SourceId, s.SourceSystemId, s.SourceTable)\n",
				"            \"\"\")\n",
				"            spark.catalog.dropTempView(\"_map_new_rows\")\n",
				"\n",
				"        # 3C) BACKFILL: any target rows owned by this source without mapping \n",
				"        tgt_owned = tgt.select(col(target_pk).alias(\"SilverRecordId\"),\n",
				"                               col(\"SourceId\"),\n",
				"                               col(\"SourceSystemId\")) \\\n",
				"                       .where(col(\"SourceId\") == lit(self.source_id))\n",
				"\n",
				"        existing_map_keys = spark.read.table(mapping_full) \\\n",
				"          .where((col(\"SourceId\")==lit(self.source_id)) & (col(\"SourceTable\")==lit(target_table))) \\\n",
				"          .select(\"SourceId\",\"SourceSystemId\",\"SourceTable\") \\\n",
				"          .dropDuplicates()\n",
				"        \n",
				"        backfill = (\n",
				"          tgt_owned\n",
				"            .withColumn(\"SourceTable\", lit(target_table))\n",
				"            .select(\"SilverRecordId\",\"SourceId\",\"SourceSystemId\",\"SourceTable\")\n",
				"            .join(existing_map_keys, on=[\"SourceId\",\"SourceSystemId\",\"SourceTable\"], how=\"left_anti\")\n",
				"        )\n",
				"\n",
				"\n",
				"        if not backfill.rdd.isEmpty():\n",
				"            backfill.createOrReplaceTempView(\"_map_backfill\")\n",
				"            spark.sql(f\"\"\"\n",
				"                MERGE INTO {mapping_full} AS m\n",
				"                USING _map_backfill AS s\n",
				"                ON  m.SilverRecordId = s.SilverRecordId\n",
				"                AND m.SourceId       = s.SourceId\n",
				"                AND m.SourceSystemId = s.SourceSystemId\n",
				"                AND m.SourceTable    = s.SourceTable\n",
				"                WHEN NOT MATCHED THEN\n",
				"                  INSERT (SilverRecordId, SourceId, SourceSystemId, SourceTable)\n",
				"                  VALUES (s.SilverRecordId, s.SourceId, s.SourceSystemId, s.SourceTable)\n",
				"            \"\"\")\n",
				"            spark.catalog.dropTempView(\"_map_backfill\")\n",
				"\n",
				"        return cnt\n",
				"        \n",
				" #   def _delete_removed_options(self, bronze_opts: DataFrame, target_table: str, target_pk: str) -> int:\n",
				" #       \"\"\"Optional hard-delete for optionset rows owned by this SourceId and unmapped in Bronze.\n",
				" #       Disabled by default (sticky mode). Enable via enable_delete=True\n",
				" #       \"\"\"\n",
				" #       target_full = f\"{self.target_lakehouse}.{target_table}\"\n",
				" #       mapping_full = self.mapping_table\n",
				" #       silver_cur = self._read_silver_state(target_table, target_pk)\n",
				" #\n",
				" #       to_delete = (\n",
				" #           silver_cur.alias(\"s\")\n",
				" #           .join(bronze_opts.alias(\"b\"), col(\"s.SourceSystemId\") == col(\"b.SourceSystemId\"), \"left_anti\")\n",
				" #           .select(\"SilverRecordId\")\n",
				" #       )\n",
				" #       cnt = to_delete.count()\n",
				" #       if cnt == 0:\n",
				" #           return 0\n",
				" #\n",
				" #       logging.info(f\"üóëÔ∏è Deleting {cnt} record(s) from {target_table} and mapping (scoped to this source)\")\n",
				" #       to_delete.createOrReplaceTempView(\"_opt_del\")\n",
				" #       # Delete target rows\n",
				" #       spark.sql(f\"\"\"\n",
				" #           DELETE FROM {target_full}\n",
				" #           WHERE {target_pk} IN (SELECT SilverRecordId FROM _opt_del)\n",
				" #       \"\"\")\n",
				" #       # Delete mappings\n",
				" #       spark.sql(f\"\"\"\n",
				" #           DELETE FROM {mapping_full}\n",
				" #           WHERE SilverRecordId IN (SELECT SilverRecordId FROM _opt_del)\n",
				" #       \"\"\")\n",
				" #       spark.catalog.dropTempView(\"_opt_del\")\n",
				" #       return cnt\n",
				" #"
			]
		},
		{
			"cell_type": "markdown",
			"id": "67ae47fd-fbc7-4e74-8f31-cc010ee1363c",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Dynamics 365 watermark\n",
				"\n",
				"Service class for managing watermark state tracking for incremental synchronization."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "340b78ff-e759-4cc9-88f1-46ae9eb2437f",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from typing import Optional\n",
				"from datetime import datetime, timezone\n",
				"from pyspark.sql import functions as F\n",
				"from pyspark.sql.functions import col\n",
				"from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
				"import logging\n",
				"\n",
				"class Dynamics365Watermark:\n",
				"    \"\"\"\n",
				"    Service for managing watermark state tracking against a table with columns:\n",
				"      - source_table_name STRING\n",
				"      - target_table_name STRING\n",
				"      - ts_column          STRING\n",
				"      - last_processed_ts  TIMESTAMP\n",
				"      - updated_at         TIMESTAMP\n",
				"    \"\"\"\n",
				"    WATERMARK_TABLE = \"WatermarkState\"\n",
				"\n",
				"    def __init__(self, source_lakehouse: str, target_lakehouse: str):\n",
				"        self.source_lakehouse = source_lakehouse\n",
				"        self.target_lakehouse = target_lakehouse\n",
				"        self.watermark_table_full = f\"{target_lakehouse}.{self.WATERMARK_TABLE}\"\n",
				"        self._ts_column: Optional[str] = None  # set via set_ts_column()\n",
				"\n",
				"    # --- optional helper so you can persist the name of the watermark column used (e.g., \"SinkModifiedOn\")\n",
				"    def set_ts_column(self, ts_col_name: str):\n",
				"        self._ts_column = ts_col_name\n",
				"\n",
				"    def ensure_watermark_table(self):\n",
				"        \"\"\"Create WatermarkState table if it doesn't exist (with your schema).\"\"\"\n",
				"        if self._table_exists(self.watermark_table_full):\n",
				"            logging.debug(f\"‚ÑπÔ∏è Watermark table already exists: {self.watermark_table_full}\")\n",
				"            return\n",
				"\n",
				"        schema = StructType([\n",
				"            StructField(\"source_table_name\", StringType(), True),\n",
				"            StructField(\"target_table_name\", StringType(), True),\n",
				"            StructField(\"ts_column\",          StringType(), True),\n",
				"            StructField(\"last_processed_ts\",  TimestampType(), True),\n",
				"            StructField(\"updated_at\",         TimestampType(), True)\n",
				"        ])\n",
				"        empty_df = spark.createDataFrame([], schema)\n",
				"        empty_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(self.watermark_table_full)\n",
				"        logging.info(f\"‚úÖ Created watermark table: {self.watermark_table_full}\")\n",
				"\n",
				"    def get_last_watermark(self, source_table: str, target_table: str):\n",
				"        \"\"\"Return the last processed watermark (Python datetime) or None.\"\"\"\n",
				"        if not self._table_exists(self.watermark_table_full):\n",
				"            self.ensure_watermark_table()\n",
				"            return None\n",
				"\n",
				"        df = (spark.read.table(self.watermark_table_full)\n",
				"                    .filter(col(\"source_table_name\") == source_table)\n",
				"                    .filter(col(\"target_table_name\") == target_table))\n",
				"\n",
				"        # Robust emptiness check\n",
				"        if df.head(1) == []:\n",
				"            return None\n",
				"\n",
				"        return df.select(F.max(\"last_processed_ts\").alias(\"ts\")).first()[\"ts\"]\n",
				"\n",
				"    def get_current_watermark(self, source_table: str, watermark_column: str, source_table_lakehouse: Optional[str] = None):\n",
				"        \"\"\"\n",
				"        Return max(watermark_column) from Bronze as a Python datetime (fallback to now UTC).\n",
				"        \n",
				"        Args:\n",
				"            source_table: Table name\n",
				"            watermark_column: Column to read max value from\n",
				"            source_table_lakehouse: Lakehouse prefix override. If None, uses configured Bronze lakehouse.\n",
				"                                   If empty string \"\", reads without prefix (for TEMPORARY VIEW).\n",
				"        \"\"\"\n",
				"        # Determine lakehouse prefix\n",
				"        if source_table_lakehouse is None:\n",
				"            lakehouse_prefix = self.source_lakehouse\n",
				"        else:\n",
				"            lakehouse_prefix = source_table_lakehouse\n",
				"        \n",
				"        # Build full table name\n",
				"        # NOTE: Cannot use f\"{lakehouse_prefix}.{source_table}\" directly because when lakehouse_prefix=\"\"\n",
				"        # it would produce \".source_table\" (starts with dot) which is invalid table name.\n",
				"        # Must conditionally add dot only when prefix exists to support TEMPORARY VIEWs.\n",
				"        # When reading from temporary view (created via createOrReplaceTempView), we need to read without\n",
				"        # lakehouse prefix, just the view name itself (e.g., \"address_stage_union\" not \"lakehouse.address_stage_union\").\n",
				"        # In Python, empty string \"\" is falsy, so `if lakehouse_prefix:` evaluates to False when lakehouse_prefix=\"\",\n",
				"        # causing the code to go to else branch and use just source_table without prefix.\n",
				"        if lakehouse_prefix:\n",
				"            source_full = f\"{lakehouse_prefix}.{source_table}\"\n",
				"        else:\n",
				"            source_full = source_table\n",
				"        \n",
				"        row = (spark.read.table(source_full)\n",
				"                    .select(F.max(col(watermark_column)).alias(\"max_wm\"))\n",
				"                    .first())\n",
				"        if row is not None and row[\"max_wm\"] is not None:\n",
				"            return row[\"max_wm\"]\n",
				"        return datetime.now(timezone.utc)\n",
				"\n",
				"    def update_watermark(self, source_table: str, target_table: str, watermark):\n",
				"        \"\"\"Upsert the watermark row using only Python literals (no Column objects).\"\"\"\n",
				"        self.ensure_watermark_table()\n",
				"\n",
				"        # Normalize to Python datetimes\n",
				"        if not isinstance(watermark, datetime):\n",
				"            watermark = spark.sql(\"SELECT current_timestamp() AS ts\").first()[\"ts\"]\n",
				"        now_py = spark.sql(\"SELECT current_timestamp() AS ts\").first()[\"ts\"]\n",
				"\n",
				"        ts_col_val = self._ts_column if self._ts_column is not None else \"\"\n",
				"\n",
				"        src = spark.createDataFrame(\n",
				"            [(source_table, target_table, ts_col_val, watermark, now_py)],\n",
				"            [\"source_table_name\", \"target_table_name\", \"ts_column\", \"last_processed_ts\", \"updated_at\"]\n",
				"        )\n",
				"        src.createOrReplaceTempView(\"_watermark_update\")\n",
				"\n",
				"        spark.sql(f\"\"\"\n",
				"            MERGE INTO {self.watermark_table_full} AS t\n",
				"            USING _watermark_update AS s\n",
				"              ON t.source_table_name = s.source_table_name\n",
				"             AND t.target_table_name = s.target_table_name\n",
				"            WHEN MATCHED THEN UPDATE SET\n",
				"                t.ts_column         = CASE WHEN s.ts_column = '' THEN t.ts_column ELSE s.ts_column END,\n",
				"                t.last_processed_ts = s.last_processed_ts,\n",
				"                t.updated_at        = s.updated_at\n",
				"            WHEN NOT MATCHED THEN INSERT (\n",
				"                source_table_name, target_table_name, ts_column, last_processed_ts, updated_at\n",
				"            ) VALUES (\n",
				"                s.source_table_name, s.target_table_name, s.ts_column, s.last_processed_ts, s.updated_at\n",
				"            )\n",
				"        \"\"\")\n",
				"        spark.catalog.dropTempView(\"_watermark_update\")\n",
				"\n",
				"    def _table_exists(self, table_name: str) -> bool:\n",
				"        \"\"\"Check if table exists.\"\"\"\n",
				"        try:\n",
				"            spark.read.table(table_name).limit(1)\n",
				"            return True\n",
				"        except Exception:\n",
				"            return False\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "c69a2488-7dcc-4328-b6bf-e32522b41fce",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Dynamics 365 data synchronization\n",
				"\n",
				"Main orchestration class for Dynamics 365 Bronze-to-Silver synchronization.\n",
				"\n",
				"**Architecture**: Uses composition pattern with three specialized components:\n",
				"- `Dynamics365DataReader` - Reads data from Bronze lakehouse\n",
				"- `Dynamics365ForeignKeyResolver` - Resolves Bronze FKs to Silver GUIDs\n",
				"- `Dynamics365DataWriter` - Writes data to Silver lakehouse\n",
				"\n",
				"**Features**:\n",
				"- Watermark-based incremental synchronization\n",
				"- Automatic GUID generation for Silver IDs\n",
				"- SourceSystemIdMapping maintenance\n",
				"- Hard delete support with optional policy\n",
				"- Custom transformation functions\n",
				"- Automatic foreign key resolution via `fk_mappings`\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "c2c019ce-1664-4706-a149-0051be3ad7e3",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"### Dynamics 365 data reader\n",
				"\n",
				"Component for reading data from Bronze lakehouse with incremental sync support."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8595421b-1212-40a4-9ab0-fc2406be0fbc",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from typing import List, Optional\n",
				"from pyspark.sql import DataFrame, Window\n",
				"from pyspark.sql.functions import col, lit, row_number, trim, lower\n",
				"from pyspark.sql.types import StringType, TimestampType\n",
				"import logging\n",
				"\n",
				"\n",
				"class Dynamics365DataReader:\n",
				"    \"\"\"\n",
				"    Component responsible for reading data from Bronze lakehouse.\n",
				"    \n",
				"    Capabilities:\n",
				"    - Incremental reading based on watermark column\n",
				"    - Bronze lookup table reads with column mapping and casting\n",
				"    - Activity party reads (Dynamics 365 standard pattern)\n",
				"    - Extract deletes vs active records\n",
				"    - Deduplication of multiple modifications\n",
				"    \"\"\"\n",
				"    \n",
				"    def __init__(self, source_lakehouse: str, watermark_column: str = \"SinkModifiedOn\", delete_flag_column: str = \"IsDelete\"):\n",
				"        \"\"\"\n",
				"        Initialize data reader component.\n",
				"        \n",
				"        Args:\n",
				"            source_lakehouse: Bronze lakehouse name\n",
				"            watermark_column: Column used for incremental sync (default: \"SinkModifiedOn\")\n",
				"            delete_flag_column: Column indicating deleted records (default: \"IsDelete\")\n",
				"        \"\"\"\n",
				"        self.source_lakehouse = source_lakehouse\n",
				"        self.watermark_column = watermark_column\n",
				"        self.delete_flag_column = delete_flag_column\n",
				"    \n",
				"    def read_incremental_changes(\n",
				"        self,\n",
				"        source_table: str,\n",
				"        source_primary_key: str,\n",
				"        source_columns: List[str],\n",
				"        last_watermark: Optional[str],\n",
				"        current_watermark: str,\n",
				"        source_table_lakehouse: Optional[str] = None\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Load only records that changed since last sync run (incremental pattern).\n",
				"        \n",
				"        Instead of reading millions of records every time, we use timestamps\n",
				"        to find just the new/modified ones. Much faster and cheaper!\n",
				"        \n",
				"        Args:\n",
				"            source_table: Bronze table name\n",
				"            source_primary_key: Primary key column name\n",
				"            source_columns: Columns to read\n",
				"            last_watermark: Last sync watermark (None for initial load)\n",
				"            current_watermark: Current watermark (upper bound)\n",
				"            source_table_lakehouse: Lakehouse prefix for source_table. If None, uses self.source_lakehouse.\n",
				"                                   Set to empty string \"\" to read without any prefix (for TEMPORARY VIEW).\n",
				"        \n",
				"        Returns:\n",
				"            DataFrame with records in watermark window\n",
				"        \"\"\"\n",
				"        # Determine lakehouse prefix\n",
				"        if source_table_lakehouse is None:\n",
				"            # Default: use configured Bronze lakehouse\n",
				"            source_full = f\"{self.source_lakehouse}.{source_table}\"\n",
				"        elif source_table_lakehouse == \"\":\n",
				"            # Empty string: no prefix (for TEMPORARY VIEW)\n",
				"            source_full = source_table\n",
				"        else:\n",
				"            # Custom lakehouse specified\n",
				"            source_full = f\"{source_table_lakehouse}.{source_table}\"\n",
				"        \n",
				"        df = spark.read.table(source_full)\n",
				"        \n",
				"        # Only read records modified between last sync and now (time window filtering)\n",
				"        if last_watermark:\n",
				"            df = df.filter(col(self.watermark_column) > lit(last_watermark))\n",
				"        df = df.filter(col(self.watermark_column) <= lit(current_watermark))\n",
				"        \n",
				"        # Pick only the columns we need (saves memory and processing time)\n",
				"        required_cols = list(set(\n",
				"            source_columns +\n",
				"            [source_primary_key, self.watermark_column] +\n",
				"            ([self.delete_flag_column] if self.delete_flag_column else [])\n",
				"        ))\n",
				"        \n",
				"        return df.select(*required_cols)\n",
				"    \n",
				"    def extract_deletes(self, df: DataFrame, source_primary_key: str) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Find records marked as deleted in the source system.\n",
				"        \n",
				"        Source systems usually don't physically delete records - they flag them\n",
				"        with IsDelete=1. We identify these so we can remove them from Silver.\n",
				"        \n",
				"        Args:\n",
				"            df: DataFrame from incremental read\n",
				"            source_primary_key: Primary key column name\n",
				"        \n",
				"        Returns:\n",
				"            DataFrame with only deleted record IDs\n",
				"        \"\"\"\n",
				"        if not self.delete_flag_column:\n",
				"            return spark.createDataFrame([], df.schema)\n",
				"        \n",
				"        return df.filter(col(self.delete_flag_column) == 1) \\\n",
				"            .select(source_primary_key) \\\n",
				"            .dropDuplicates()\n",
				"    \n",
				"    def extract_active(self, df: DataFrame, source_primary_key: str) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Get records that should exist in Silver (not deleted).\n",
				"        \n",
				"        We also keep only the LATEST version if a record was modified multiple times\n",
				"        since last sync. No point processing the same record 5 times.\n",
				"        \n",
				"        Args:\n",
				"            df: DataFrame from incremental read\n",
				"            source_primary_key: Primary key column name\n",
				"        \n",
				"        Returns:\n",
				"            Deduplicated DataFrame with only active (non-deleted) records\n",
				"        \"\"\"\n",
				"        if self.delete_flag_column:\n",
				"            df = df.filter((col(self.delete_flag_column).isNull()) | \n",
				"                          (col(self.delete_flag_column) == 0))\n",
				"        \n",
				"        # If a record was modified 3 times, keep only the newest version (deduplication)\n",
				"        # Secondary sort by PK ensures deterministic behavior when watermarks collide\n",
				"        w = Window.partitionBy(source_primary_key) \\\n",
				"            .orderBy(col(self.watermark_column).desc(), col(source_primary_key).asc())\n",
				"        \n",
				"        result = df.withColumn(\"rn\", row_number().over(w)) \\\n",
				"            .filter(col(\"rn\") == 1) \\\n",
				"            .drop(\"rn\", self.watermark_column)\n",
				"        \n",
				"        if self.delete_flag_column:\n",
				"            result = result.drop(self.delete_flag_column)\n",
				"            \n",
				"        return result\n",
				"    \n",
				"    def read_bronze_table(\n",
				"        self,\n",
				"        table_name: str,\n",
				"        columns,  # Union[List[str], Dict]\n",
				"        filters: Optional[List] = None\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Read Bronze table with column mapping, casting, and filtering.\n",
				"        \n",
				"        Universal Bronze reader for all Bronze table reads (lookups, full entities, staging).\n",
				"        Supports column renaming, type casting, and filtering. Auto-trims string columns.\n",
				"        \n",
				"        Args:\n",
				"            table_name: Bronze table name (without lakehouse prefix)\n",
				"            columns: Column specification - can be:\n",
				"                    1) List of column names (keeps original names): [\"col1\", \"col2\", \"col3\"]\n",
				"                    2) Dict with rename/cast: {\"source_col\": \"alias\"} or {\"source_col\": (\"alias\", \"cast_type\")}\n",
				"                    3) Mix in dict: {\"col1\": \"col1\", \"col2\": (\"col2_renamed\", \"string\")}\n",
				"                    \n",
				"                    Special handling for dict values:\n",
				"                    - String: Simple rename/passthrough\n",
				"                    - Tuple (\"alias\", \"string\"): Applies trim() before casting\n",
				"                    - Tuple (\"alias\", \"timestamp\"): Casts to TimestampType\n",
				"                    \n",
				"            filters: Optional list of filter conditions (Column expressions)\n",
				"        \n",
				"        Returns:\n",
				"            DataFrame with selected/renamed/cast columns\n",
				"        \n",
				"        Examples:\n",
				"            # List format - keeps original column names (no repetition!)\n",
				"            contact = reader.read_bronze_table(\"contact\", [\n",
				"                \"address1_city\", \"address1_country\", \"createdon\", \"modifiedon\"\n",
				"            ])\n",
				"            \n",
				"            # Dict format - rename/cast\n",
				"            wf = reader.read_bronze_table(\"workflow\", {\n",
				"                \"workflowid\": \"wf_id\",\n",
				"                \"name\": (\"wf_name\", \"string\")  # auto-trims\n",
				"            })\n",
				"            \n",
				"            # Mixed format - some passthrough, some with cast\n",
				"            tx = reader.read_bronze_table(\"msnfp_transaction\", {\n",
				"                \"Id\": \"Id\",  # passthrough\n",
				"                \"createdon\": (\"createdon\", \"timestamp\"),  # cast\n",
				"                \"SinkModifiedOn\": (\"SinkModifiedOn\", \"timestamp\")\n",
				"            })\n",
				"        \"\"\"\n",
				"        # Read table from Bronze lakehouse\n",
				"        df = spark.read.table(f\"{self.source_lakehouse}.{table_name}\")\n",
				"        \n",
				"        # Apply filters first (reduces data before projections)\n",
				"        if filters:\n",
				"            for condition in filters:\n",
				"                df = df.filter(condition)\n",
				"        \n",
				"        # Handle list format (simple column names)\n",
				"        if isinstance(columns, list):\n",
				"            return df.select(*[col(c) for c in columns])\n",
				"        \n",
				"        # Handle dict format (rename/cast)\n",
				"        select_exprs = []\n",
				"        for src_col, spec in columns.items():\n",
				"            if isinstance(spec, tuple):\n",
				"                # Tuple format: (\"alias\", \"cast_type\")\n",
				"                alias, cast_type = spec\n",
				"                expr_col = col(src_col)\n",
				"                \n",
				"                # Auto-trim strings before casting\n",
				"                if cast_type == \"string\":\n",
				"                    expr_col = trim(expr_col).cast(StringType())\n",
				"                elif cast_type == \"timestamp\":\n",
				"                    expr_col = expr_col.cast(TimestampType())\n",
				"                else:\n",
				"                    # Generic cast (int, bigint, decimal, etc.)\n",
				"                    expr_col = expr_col.cast(cast_type)\n",
				"                \n",
				"                expr_col = expr_col.alias(alias)\n",
				"            else:\n",
				"                # Simple string format: just alias\n",
				"                expr_col = col(src_col).alias(spec)\n",
				"            \n",
				"            select_exprs.append(expr_col)\n",
				"        \n",
				"        return df.select(*select_exprs)\n",
				"    \n",
				"    def read_activity_parties(\n",
				"        self,\n",
				"        activity_id_column: str = \"activityid\",\n",
				"        participation_types: list = [1, 2]\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Read activityparty records from Bronze lakehouse (Dynamics 365 standard pattern).\n",
				"        \n",
				"        This is a Dynamics 365 universal pattern - every activity (letter, phonecall, email, appointment, fax)\n",
				"        has activityparty records that define participants (from/to/CC/BCC).\n",
				"        \n",
				"        Args:\n",
				"            activity_id_column: Column name for activity ID (default: \"activityid\")\n",
				"            participation_types: List of participationtypemask values to include\n",
				"                               Default [1, 2] = from/to\n",
				"                               Dynamics 365 values: 1=From, 2=To, 3=CC, 4=BCC, 5=Required, 6=Optional, 7=Organizer, etc.\n",
				"        \n",
				"        Returns:\n",
				"            DataFrame with columns:\n",
				"            - activityid: The activity ID (cast to string)\n",
				"            - partyid: The party ID (contact/account - cast to string)\n",
				"            - partyid_entitytype: The party entity type (lowercased)\n",
				"            - ActivityPartyId: The activityparty unique ID (cast to string)\n",
				"        \n",
				"        Example usage:\n",
				"            parties = reader.read_activity_parties(participation_types=[1, 2, 3])\n",
				"            df = df.join(parties, col(\"Id\") == col(\"activityid\"), \"inner\")\n",
				"        \"\"\"\n",
				"        activityparty = (\n",
				"            spark.read.table(f\"{self.source_lakehouse}.activityparty\")\n",
				"            .filter(col(\"participationtypemask\").isin(participation_types))\n",
				"            .filter(col(\"partyid\").isNotNull())\n",
				"            .filter(lower(col(\"partyid_entitytype\")).isin(\"contact\", \"account\"))\n",
				"            .select(\n",
				"                col(activity_id_column).alias(\"activityid\"),\n",
				"                col(\"partyid\").cast(StringType()).alias(\"partyid\"),\n",
				"                lower(col(\"partyid_entitytype\")).alias(\"partyid_entitytype\"),\n",
				"                col(\"Id\").cast(StringType()).alias(\"ActivityPartyId\")\n",
				"            )\n",
				"            .dropDuplicates([\"activityid\", \"partyid\", \"partyid_entitytype\", \"ActivityPartyId\"])\n",
				"        )\n",
				"        \n",
				"        logging.info(f\"üìã Read activity parties (participation types: {participation_types})\")\n",
				"        return activityparty"
			]
		},
		{
			"cell_type": "markdown",
			"id": "58875ae2-1e27-4ce4-9d51-91897f94f918",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"### Dynamics 365 foreign key resolver\n",
				"\n",
				"Component for resolving foreign keys from Bronze IDs to Silver GUIDs."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8335ece7-b67a-4b48-b97b-a74e7c7c6534",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from typing import Dict, Optional\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql.functions import col, lit, lower, coalesce as F_coalesce\n",
				"from pyspark.sql.types import StringType\n",
				"import pyspark.sql.functions as F\n",
				"import logging\n",
				"\n",
				"\n",
				"class Dynamics365ForeignKeyResolver:\n",
				"    \"\"\"\n",
				"    Component responsible for resolving foreign keys from Bronze to Silver.\n",
				"    \n",
				"    Capabilities:\n",
				"    - Standard FK resolution (Bronze ID ‚Üí Silver GUID via SourceSystemIdMapping)\n",
				"    - Constituent resolution (Contact/Account ‚Üí Constituent)\n",
				"    - Polymorphic lookup resolution (regardingobjectid, partyid, etc.)\n",
				"    \"\"\"\n",
				"    \n",
				"    def __init__(self, source_id: str, source_lakehouse: str, target_lakehouse: str, mapping_table: str):\n",
				"        \"\"\"\n",
				"        Initialize FK resolver component.\n",
				"        \n",
				"        Args:\n",
				"            source_id: SourceId GUID for this data source\n",
				"            source_lakehouse: Bronze lakehouse name\n",
				"            target_lakehouse: Silver lakehouse name\n",
				"            mapping_table: Full path to SourceSystemIdMapping table\n",
				"        \"\"\"\n",
				"        self.source_id = source_id\n",
				"        self.source_lakehouse = source_lakehouse\n",
				"        self.target_lakehouse = target_lakehouse\n",
				"        self.mapping_table = mapping_table\n",
				"    \n",
				"    def resolve_foreign_keys(self, df: DataFrame, fk_mappings: Dict[str, str]) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Convert source system codes to Silver GUIDs for related records.\n",
				"        \n",
				"        WHY: Source data uses numeric codes (e.g., typecode=1 means \"Email Campaign\").\n",
				"        Silver layer uses unique GUIDs for each record. This method translates between them.\n",
				"        \n",
				"        EXAMPLE: \n",
				"        - Input: Contact with address1_addressid=123 (source system code)\n",
				"        - Output: Same contact, but also with address1_addressid_SilverRecordId=abc-def-guid\n",
				"        - Now we can link Contact ‚Üí Address in Silver using the GUID\n",
				"        \n",
				"        This is a PUBLIC method you can call from custom transform functions\n",
				"        if you need to resolve additional relationships.\n",
				"        \n",
				"        Args:\n",
				"            df: DataFrame with source data\n",
				"            fk_mappings: Which columns link to which tables\n",
				"                        Example: {\"typecode\": \"CampaignType\", \"address1_addressid\": \"Address\"}\n",
				"        \n",
				"        Returns:\n",
				"            Original DataFrame plus new {column}_SilverRecordId columns for each mapping\n",
				"        \"\"\"\n",
				"        result_df = df\n",
				"        \n",
				"        for bronze_col, silver_table in fk_mappings.items():\n",
				"            if bronze_col not in df.columns:\n",
				"                logging.warning(f\"‚ö†Ô∏è FK column '{bronze_col}' not found in DataFrame, skipping resolution\")\n",
				"                continue\n",
				"            \n",
				"            logging.info(f\"üîó Resolving FK: {bronze_col} ‚Üí {silver_table}\")\n",
				"            \n",
				"            # Read mapping for target Silver table\n",
				"            # DEFENSIVE: Deduplicate by SourceSystemId to prevent JOIN creating duplicates.\n",
				"            # Ideally (SourceId, SourceSystemId, SourceTable) should have UNIQUE constraint,\n",
				"            # but without it we must deduplicate to avoid MERGE error when corrupted mappings exist.\n",
				"            # If duplicates exist, we arbitrarily pick first (non-deterministic but safe).\n",
				"            mapping = spark.read.table(self.mapping_table) \\\n",
				"                .filter(col(\"SourceId\") == lit(self.source_id)) \\\n",
				"                .filter(col(\"SourceTable\") == lit(silver_table)) \\\n",
				"                .select(\n",
				"                    col(\"SilverRecordId\"),\n",
				"                    col(\"SourceSystemId\").alias(f\"_fk_{bronze_col}_ssid\")\n",
				"                ) \\\n",
				"                .dropDuplicates([f\"_fk_{bronze_col}_ssid\"])\n",
				"            \n",
				"            # Join to resolve Bronze value ‚Üí Silver GUID\n",
				"            result_df = result_df.join(\n",
				"                mapping,\n",
				"                col(bronze_col).cast(StringType()) == col(f\"_fk_{bronze_col}_ssid\"),\n",
				"                \"left\"\n",
				"            ) \\\n",
				"            .withColumnRenamed(\"SilverRecordId\", f\"{bronze_col}_SilverRecordId\") \\\n",
				"            .drop(f\"_fk_{bronze_col}_ssid\")\n",
				"            \n",
				"            # Count resolution statistics\n",
				"            non_null_source = result_df.filter(col(bronze_col).isNotNull()).count()\n",
				"            resolved_count = result_df.filter(col(f\"{bronze_col}_SilverRecordId\").isNotNull()).count()\n",
				"            \n",
				"            # Determine log message based on resolution success\n",
				"            if non_null_source == 0:\n",
				"                logging.info(f\"   ‚ÑπÔ∏è All records have NULL {bronze_col} (optional FK)\")\n",
				"            elif resolved_count == non_null_source:\n",
				"                logging.info(f\"   ‚úì Resolved all {resolved_count} {bronze_col} references\")\n",
				"            else:\n",
				"                unresolved = non_null_source - resolved_count\n",
				"                logging.warning(f\"   ‚ö†Ô∏è Resolved only {resolved_count} of {non_null_source} {bronze_col} references ({unresolved} not found in {silver_table})\")\n",
				"        \n",
				"        return result_df\n",
				"    \n",
				"    def resolve_constituent_id(\n",
				"        self,\n",
				"        df: DataFrame,\n",
				"        contact_fk_column: Optional[str] = None,\n",
				"        account_fk_column: Optional[str] = None,\n",
				"        output_column: str = \"ConstituentId\"\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Resolve ConstituentId from Contact or Account foreign keys.\n",
				"        \n",
				"        Constituent is an abstraction over Contact and Account entities.\n",
				"        This method joins the Constituent table via ContactId or AccountId\n",
				"        to find the corresponding ConstituentId.\n",
				"        \n",
				"        USAGE PATTERN:\n",
				"        1. Framework resolves Contact/Account FKs first (via fk_mappings)\n",
				"        2. Call this method to resolve ConstituentId from those resolved FKs\n",
				"        3. Prefer Contact over Account when both are present (coalesce)\n",
				"        \n",
				"        EXAMPLE:\n",
				"            # In transform_transaction:\n",
				"            def transform_transaction(df: DataFrame) -> DataFrame:\n",
				"                # Framework already resolved:\n",
				"                # - msnfp_receiptoncontactid ‚Üí msnfp_receiptoncontactid_SilverRecordId\n",
				"                # - msnfp_transaction_receiptonaccountid ‚Üí msnfp_transaction_receiptonaccountid_SilverRecordId\n",
				"                \n",
				"                # Now resolve Constituent:\n",
				"                df = resolver.resolve_constituent_id(\n",
				"                    df,\n",
				"                    contact_fk_column=\"msnfp_receiptoncontactid_SilverRecordId\",\n",
				"                    account_fk_column=\"msnfp_transaction_receiptonaccountid_SilverRecordId\"\n",
				"                )\n",
				"                \n",
				"                return df.select(..., col(\"ConstituentId\"), ...)\n",
				"        \n",
				"        Args:\n",
				"            df: DataFrame with resolved Contact/Account FK columns\n",
				"            contact_fk_column: Name of column containing resolved ContactId (FK to Contact)\n",
				"            account_fk_column: Name of column containing resolved AccountId (FK to Account)\n",
				"            output_column: Name of output column (default: \"ConstituentId\")\n",
				"        \n",
				"        Returns:\n",
				"            Original DataFrame plus new {output_column} with resolved ConstituentId.\n",
				"            Prefers Contact path over Account path when both are present.\n",
				"        \"\"\"\n",
				"        if not contact_fk_column and not account_fk_column:\n",
				"            raise ValueError(\"Must provide at least one of contact_fk_column or account_fk_column\")\n",
				"        \n",
				"        logging.info(f\"üîó Resolving {output_column} from Constituent table\")\n",
				"        \n",
				"        # Read only needed columns from Constituent table (optimized)\n",
				"        constituent = spark.read.table(f\"{self.target_lakehouse}.Constituent\") \\\n",
				"            .select(\"ConstituentId\", \"ContactId\", \"AccountId\")\n",
				"        \n",
				"        result_df = df\n",
				"        \n",
				"        # Resolve via Contact path\n",
				"        if contact_fk_column and contact_fk_column in df.columns:\n",
				"            cons_contact = constituent.select(\n",
				"                col(\"ConstituentId\").alias(\"_cons_from_contact\"),\n",
				"                col(\"ContactId\").alias(\"_cons_contact_key\")\n",
				"            )\n",
				"            \n",
				"            result_df = result_df.join(\n",
				"                cons_contact,\n",
				"                col(contact_fk_column) == col(\"_cons_contact_key\"),\n",
				"                \"left\"\n",
				"            ).drop(\"_cons_contact_key\")\n",
				"        \n",
				"        # Resolve via Account path\n",
				"        if account_fk_column and account_fk_column in df.columns:\n",
				"            cons_account = constituent.select(\n",
				"                col(\"ConstituentId\").alias(\"_cons_from_account\"),\n",
				"                col(\"AccountId\").alias(\"_cons_account_key\")\n",
				"            )\n",
				"            \n",
				"            result_df = result_df.join(\n",
				"                cons_account,\n",
				"                col(account_fk_column) == col(\"_cons_account_key\"),\n",
				"                \"left\"\n",
				"            ).drop(\"_cons_account_key\")\n",
				"        \n",
				"        # Coalesce: prefer Contact over Account\n",
				"        if contact_fk_column and account_fk_column:\n",
				"            result_df = result_df.withColumn(\n",
				"                output_column,\n",
				"                F_coalesce(col(\"_cons_from_contact\"), col(\"_cons_from_account\"))\n",
				"            )\n",
				"        elif contact_fk_column:\n",
				"            result_df = result_df.withColumnRenamed(\"_cons_from_contact\", output_column)\n",
				"        else:  # account_fk_column only\n",
				"            result_df = result_df.withColumnRenamed(\"_cons_from_account\", output_column)\n",
				"        \n",
				"        # Cleanup temporary columns\n",
				"        temp_cols = [\"_cons_from_contact\", \"_cons_from_account\"]\n",
				"        for temp_col in temp_cols:\n",
				"            if temp_col in result_df.columns:\n",
				"                result_df = result_df.drop(temp_col)\n",
				"        \n",
				"        # Single count operation for logging (avoid multiple Spark evaluations)\n",
				"        total_resolved = result_df.filter(col(output_column).isNotNull()).count()\n",
				"        total_count = result_df.count()\n",
				"        \n",
				"        # Better messaging: distinguish between successful resolution and missing data\n",
				"        if total_resolved == total_count:\n",
				"            logging.info(f\"   ‚úì Resolved all {total_resolved} Constituents\")\n",
				"        else:\n",
				"            null_count = total_count - total_resolved\n",
				"            logging.info(f\"   ‚úì Resolved {total_resolved} Constituents ({null_count} records without Contact/Account link)\")\n",
				"        \n",
				"        return result_df\n",
				"    \n",
				"    def resolve_polymorphic_lookup(\n",
				"        self,\n",
				"        df: DataFrame,\n",
				"        lookup_id_column: str,\n",
				"        entity_type_column: str,\n",
				"        entity_type_mappings: dict\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Resolve Dynamics 365 polymorphic lookup fields (customerid + customerid_entitytype pattern).\n",
				"        \n",
				"        Dynamics 365 uses polymorphic lookups where one field can reference different entity types:\n",
				"        - lookup_id_column: The ID value (e.g., \"customerid\")\n",
				"        - entity_type_column: Entity type discriminator (e.g., \"customerid_entitytype\")\n",
				"        - entity_type_mappings: Dict mapping entity type values to Silver table names\n",
				"          Example: {\"contact\": \"Contact\", \"account\": \"Account\"}\n",
				"        \n",
				"        Returns:\n",
				"            DataFrame with resolved FK columns added for each entity type.\n",
				"            Column naming: {entity_type_value}Id (e.g., \"ContactId\", \"AccountId\")\n",
				"        \n",
				"        Example usage:\n",
				"            df = resolver.resolve_polymorphic_lookup(\n",
				"                df,\n",
				"                lookup_id_column=\"customerid\",\n",
				"                entity_type_column=\"customerid_entitytype\",\n",
				"                entity_type_mappings={\"contact\": \"Contact\", \"account\": \"Account\"}\n",
				"            )\n",
				"            # Adds: ContactId and AccountId columns\n",
				"        \"\"\"\n",
				"        if not entity_type_mappings:\n",
				"            raise ValueError(\"entity_type_mappings cannot be empty\")\n",
				"        \n",
				"        logging.info(f\"üîó Resolving polymorphic lookup: {lookup_id_column} by {entity_type_column}\")\n",
				"        \n",
				"        result_df = df\n",
				"        \n",
				"        # Resolve each entity type\n",
				"        for entity_type_value, silver_table in entity_type_mappings.items():\n",
				"            # Get mapping for this entity type\n",
				"            mapping = (\n",
				"                spark.read.table(self.mapping_table)\n",
				"                .filter((col(\"SourceId\") == lit(self.source_id)) & \n",
				"                        (lower(col(\"SourceTable\")) == entity_type_value.lower()))\n",
				"                .select(\n",
				"                    col(\"SourceSystemId\").alias(f\"_src_{entity_type_value}\"),\n",
				"                    col(\"SilverRecordId\").alias(f\"{entity_type_value.capitalize()}Id\")\n",
				"                )\n",
				"            )\n",
				"            \n",
				"            # Join when entity type matches\n",
				"            result_df = result_df.join(\n",
				"                mapping,\n",
				"                (col(lookup_id_column).cast(StringType()) == col(f\"_src_{entity_type_value}\")) & \n",
				"                (lower(col(entity_type_column)) == entity_type_value.lower()),\n",
				"                \"left\"\n",
				"            ).drop(f\"_src_{entity_type_value}\")\n",
				"            \n",
				"            logging.info(f\"   ‚úì Resolved {entity_type_value} ‚Üí {entity_type_value.capitalize()}Id\")\n",
				"        \n",
				"        return result_df"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e4c30bcd-d0af-4aa0-97b1-89ff5aa8c10d",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"### Dynamics 365 data writer\n",
				"\n",
				"Component for writing data to Silver lakehouse with MERGE operations and mapping management."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "b0172d0c-4ee9-480f-adb3-eac86410fdea",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from typing import Optional, Callable\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql.functions import col, lit, expr, coalesce\n",
				"from pyspark.sql.types import StringType\n",
				"import logging\n",
				"\n",
				"\n",
				"class Dynamics365DataWriter:\n",
				"    \"\"\"\n",
				"    Component responsible for writing data to Silver lakehouse.\n",
				"    \n",
				"    Capabilities:\n",
				"    - Process deletes (with optional delete policy)\n",
				"    - Process upserts (INSERT/UPDATE via MERGE)\n",
				"    - Generate Silver GUIDs for new records\n",
				"    - Maintain SourceSystemIdMapping table\n",
				"    - Support custom MERGE SQL\n",
				"    \"\"\"\n",
				"    \n",
				"    def __init__(self, source_id: str, target_lakehouse: str, mapping_table: str, enable_hard_delete: bool = True):\n",
				"        \"\"\"\n",
				"        Initialize data writer component.\n",
				"        \n",
				"        Args:\n",
				"            source_id: SourceId GUID for this data source\n",
				"            target_lakehouse: Silver lakehouse name\n",
				"            mapping_table: Full path to SourceSystemIdMapping table\n",
				"            enable_hard_delete: Whether to process hard deletes (default: True)\n",
				"        \"\"\"\n",
				"        self.source_id = source_id\n",
				"        self.target_lakehouse = target_lakehouse\n",
				"        self.mapping_table = mapping_table\n",
				"        self.enable_hard_delete = enable_hard_delete\n",
				"    \n",
				"    def process_deletes(\n",
				"        self,\n",
				"        df_deletes: DataFrame,\n",
				"        source_primary_key: str,\n",
				"        target_table: str,\n",
				"        target_primary_key: str,\n",
				"        delete_policy_func: Optional[Callable[[DataFrame], DataFrame]] = None\n",
				"    ):\n",
				"        \"\"\"\n",
				"        Remove deleted records from Silver layer.\n",
				"        \n",
				"        Sometimes you don't want to delete everything (e.g., keep historical data).\n",
				"        That's what delete_policy_func is for - it can filter which deletes to actually execute.\n",
				"        \n",
				"        Args:\n",
				"            df_deletes: DataFrame with deleted record IDs (from Bronze)\n",
				"            source_primary_key: Bronze PK column name\n",
				"            target_table: Silver table name\n",
				"            target_primary_key: Silver PK column name\n",
				"            delete_policy_func: Optional function to filter deletes before execution\n",
				"        \"\"\"\n",
				"        if not self.enable_hard_delete or df_deletes.isEmpty():\n",
				"            return\n",
				"        \n",
				"        delete_count = df_deletes.count()\n",
				"        logging.info(f\"üóëÔ∏è Processing {delete_count} delete(s) for {target_table}\")\n",
				"        \n",
				"        # Get mapping to resolve Bronze PK ‚Üí Silver PK\n",
				"        mapping_df = self._get_mapping_for_deletes(df_deletes, source_primary_key, target_table, target_primary_key)\n",
				"        \n",
				"        if mapping_df.isEmpty():\n",
				"            logging.info(f\"‚ÑπÔ∏è No matching Silver records to delete\")\n",
				"            return\n",
				"        \n",
				"        # Apply delete policy if provided\n",
				"        if delete_policy_func:\n",
				"            before_count = mapping_df.count()\n",
				"            mapping_df = delete_policy_func(mapping_df)\n",
				"            after_count = mapping_df.count()\n",
				"            excluded = before_count - after_count\n",
				"            if excluded > 0:\n",
				"                logging.info(f\"üõ°Ô∏è Delete policy excluded {excluded} record(s)\")\n",
				"        \n",
				"        # Execute delete\n",
				"        if not mapping_df.isEmpty():\n",
				"            self._execute_delete(mapping_df, target_table, target_primary_key)\n",
				"    \n",
				"    def _get_mapping_for_deletes(\n",
				"        self,\n",
				"        df_deletes: DataFrame,\n",
				"        source_primary_key: str,\n",
				"        target_table: str,\n",
				"        target_primary_key: str\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Find Silver GUIDs for the source records we need to delete.\n",
				"        \n",
				"        Source says \"delete record ID=123\". We look up which Silver GUID\n",
				"        corresponds to that source ID, so we know what to remove.\n",
				"        \"\"\"\n",
				"        mapping = spark.read.table(self.mapping_table) \\\n",
				"            .filter(col(\"SourceId\") == lit(self.source_id)) \\\n",
				"            .filter(col(\"SourceTable\") == lit(target_table)) \\\n",
				"            .select(\n",
				"                col(\"SilverRecordId\").alias(target_primary_key),\n",
				"                col(\"SourceSystemId\")\n",
				"            )\n",
				"        \n",
				"        return df_deletes \\\n",
				"            .withColumn(\"SourceSystemId\", col(source_primary_key).cast(StringType())) \\\n",
				"            .join(mapping, \"SourceSystemId\", \"inner\") \\\n",
				"            .select(target_primary_key)\n",
				"    \n",
				"    def _execute_delete(self, keys_df: DataFrame, target_table: str, target_primary_key: str):\n",
				"        \"\"\"\n",
				"        Physically remove records from Silver and clean up the ID mapping table.\n",
				"        \n",
				"        We delete from TWO places:\n",
				"        1. The target table (e.g., Campaign table)\n",
				"        2. The mapping table (which tracks source ID ‚Üí Silver GUID relationships)\n",
				"        \"\"\"\n",
				"        keys_df.createOrReplaceTempView(\"_delete_keys_temp\")\n",
				"        target_full = f\"{self.target_lakehouse}.{target_table}\"\n",
				"        \n",
				"        # 1. Delete from target table (e.g., Campaign)\n",
				"        delete_sql = f\"\"\"\n",
				"        MERGE INTO {target_full} AS t\n",
				"        USING _delete_keys_temp AS k\n",
				"          ON t.{target_primary_key} = k.{target_primary_key}\n",
				"        WHEN MATCHED THEN DELETE\n",
				"        \"\"\"\n",
				"        \n",
				"        spark.sql(delete_sql)\n",
				"        logging.info(f\"‚úì Deleted records from {target_table}\")\n",
				"        \n",
				"        # 2. Delete corresponding mappings from SourceSystemIdMapping\n",
				"        mapping_delete_sql = f\"\"\"\n",
				"        MERGE INTO {self.mapping_table} AS m\n",
				"        USING _delete_keys_temp AS k\n",
				"          ON m.SilverRecordId = k.{target_primary_key}\n",
				"         AND m.SourceId = '{self.source_id}'\n",
				"         AND m.SourceTable = '{target_table}'\n",
				"        WHEN MATCHED THEN DELETE\n",
				"        \"\"\"\n",
				"        \n",
				"        spark.sql(mapping_delete_sql)\n",
				"        logging.info(f\"‚úì Deleted corresponding mappings from SourceSystemIdMapping\")\n",
				"        \n",
				"        spark.catalog.dropTempView(\"_delete_keys_temp\")\n",
				"    \n",
				"    def process_upserts(\n",
				"        self,\n",
				"        df_active: DataFrame,\n",
				"        source_primary_key: str,\n",
				"        target_table: str,\n",
				"        target_primary_key: str,\n",
				"        transform_func: Optional[Callable[[DataFrame], DataFrame]] = None,\n",
				"        merge_sql: Optional[str] = None,\n",
				"        mapping_table_name: Optional[str] = None\n",
				"    ):\n",
				"        \"\"\"\n",
				"        Add new records and update existing ones in Silver.\n",
				"        \n",
				"        \"Upsert\" = UPDATE if exists, INSERT if new. Smart merge that handles both cases.\n",
				"        This is the core of data synchronization - keeping Silver in sync with source.\n",
				"        \n",
				"        Args:\n",
				"            df_active: DataFrame with active (non-deleted) records\n",
				"            source_primary_key: Bronze PK column name\n",
				"            target_table: Silver table name\n",
				"            target_primary_key: Silver PK column name\n",
				"            transform_func: Optional transformation function\n",
				"            merge_sql: Optional custom MERGE SQL\n",
				"            mapping_table_name: Override SourceTable value in mapping\n",
				"        \"\"\"\n",
				"        if df_active.isEmpty():\n",
				"            return\n",
				"        \n",
				"        upsert_count = df_active.count()\n",
				"        logging.info(f\"‚ûï Processing {upsert_count} upsert(s) for {target_table}\")\n",
				"        \n",
				"        # 1. Transform data (user transformation)\n",
				"        if transform_func:\n",
				"            df_transformed = transform_func(df_active)\n",
				"        else:\n",
				"            df_transformed = df_active\n",
				"        \n",
				"        # 2. Add required Silver columns (SourceId, SourceSystemId) if not present\n",
				"        df_transformed = self._ensure_required_columns(df_transformed, source_primary_key)\n",
				"        \n",
				"        # 3. Resolve existing records (UPDATE) vs new records (INSERT)\n",
				"        df_with_mapping = self._resolve_silver_ids(df_transformed, target_table)\n",
				"        \n",
				"        # 4. Generate GUIDs for new records\n",
				"        df_final = self._generate_silver_ids(df_with_mapping, target_primary_key)\n",
				"        \n",
				"        # 5. Execute MERGE (custom or auto-generated)\n",
				"        self._execute_upsert(df_final, target_table, target_primary_key, merge_sql)\n",
				"        \n",
				"        # 6. Update SourceSystemIdMapping\n",
				"        self._update_mapping(df_final, target_table, target_primary_key, mapping_table_name)\n",
				"    \n",
				"    def _ensure_required_columns(self, df: DataFrame, source_primary_key: str) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Add tracking columns that every Silver record needs.\n",
				"        \n",
				"        Every record in Silver must have:\n",
				"        - SourceId: Which source system it came from (this Dynamics instance)\n",
				"        - SourceSystemId: The original ID from that source system\n",
				"        \n",
				"        This lets us trace any Silver record back to its origin and handle updates correctly.\n",
				"        Transform functions don't need to add these - we do it automatically.\n",
				"        \"\"\"\n",
				"        result_df = df\n",
				"        \n",
				"        # Add SourceId if not present\n",
				"        if \"SourceId\" not in df.columns:\n",
				"            result_df = result_df.withColumn(\"SourceId\", lit(self.source_id))\n",
				"        \n",
				"        # Add SourceSystemId if not present\n",
				"        if \"SourceSystemId\" not in df.columns:\n",
				"            result_df = result_df.withColumn(\"SourceSystemId\", col(source_primary_key).cast(StringType()))\n",
				"        \n",
				"        return result_df\n",
				"    \n",
				"    def _resolve_silver_ids(self, df: DataFrame, target_table: str) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Check which records already exist in Silver vs. which are brand new.\n",
				"        \n",
				"        We look up each source ID in our mapping table. If found, we get back\n",
				"        the Silver GUID to UPDATE. If not found, it's a new record to INSERT.\n",
				"        \"\"\"\n",
				"        mapping = spark.read.table(self.mapping_table) \\\n",
				"            .filter(col(\"SourceId\") == lit(self.source_id)) \\\n",
				"            .filter(col(\"SourceTable\") == lit(target_table)) \\\n",
				"            .select(\n",
				"                col(\"SilverRecordId\"),\n",
				"                col(\"SourceSystemId\").alias(\"_map_SourceSystemId\")\n",
				"            )\n",
				"        \n",
				"        joined = df.join(\n",
				"            mapping,\n",
				"            (df[\"SourceId\"] == lit(self.source_id)) &\n",
				"            (df[\"SourceSystemId\"] == col(\"_map_SourceSystemId\")),\n",
				"            \"left\"\n",
				"        ).drop(\"_map_SourceSystemId\")  \n",
				"        return joined\n",
				"    \n",
				"    def _generate_silver_ids(self, df: DataFrame, target_primary_key: str) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Create unique Silver GUIDs for new records.\n",
				"        \n",
				"        Existing records keep their Silver GUID (from mapping table).\n",
				"        New records get a fresh UUID. This ensures every Silver record\n",
				"        has a stable, unique identifier across all syncs.\n",
				"        \"\"\"\n",
				"        return df.withColumn(\n",
				"            target_primary_key,\n",
				"            coalesce(col(\"SilverRecordId\"), expr(\"uuid()\"))\n",
				"        ).drop(\"SilverRecordId\")\n",
				"    \n",
				"    def _execute_upsert(self, df: DataFrame, target_table: str, target_primary_key: str, merge_sql: Optional[str] = None):\n",
				"        \"\"\"\n",
				"        Run the SQL MERGE command to apply inserts and updates.\n",
				"        \n",
				"        You can provide custom MERGE SQL for special cases (e.g., Country table\n",
				"        with case-insensitive name matching). Otherwise, we auto-generate standard\n",
				"        MERGE logic based on which columns are available.\n",
				"        \"\"\"\n",
				"        \n",
				"        df.createOrReplaceTempView(\"_upsert_source_temp\")\n",
				"        target_full = f\"{self.target_lakehouse}.{target_table}\"\n",
				"        \n",
				"        if merge_sql:            \n",
				"            final_sql = merge_sql.format(\n",
				"                TARGET_TABLE=target_full,\n",
				"                SOURCE_VIEW=\"_upsert_source_temp\",\n",
				"                TARGET_PK=target_primary_key\n",
				"            )\n",
				"            \n",
				"            logging.debug(f\"üìã Final SQL (AFTER substitution):\")\n",
				"            logging.debug(final_sql)\n",
				"        else:\n",
				"            # Auto-generate MERGE SQL based on available columns\n",
				"            target_cols = [f.name for f in spark.read.table(target_full).schema.fields]\n",
				"            available_cols = [c for c in target_cols if c in df.columns]\n",
				"            \n",
				"            # Build UPDATE SET clause - use backticks to preserve case-sensitivity\n",
				"            update_set = \", \".join([f\"`{c}` = s.`{c}`\" for c in available_cols if c != target_primary_key])\n",
				"            \n",
				"            # Build INSERT clause - use backticks to preserve case-sensitivity\n",
				"            insert_cols = \", \".join([f\"`{c}`\" for c in available_cols])\n",
				"            insert_vals = \", \".join([f\"s.`{c}`\" for c in available_cols])\n",
				"            \n",
				"            final_sql = f\"\"\"\n",
				"            MERGE INTO {target_full} AS t\n",
				"            USING _upsert_source_temp AS s\n",
				"              ON t.`{target_primary_key}` = s.`{target_primary_key}`\n",
				"            WHEN MATCHED THEN UPDATE SET {update_set}\n",
				"            WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
				"            \"\"\"\n",
				"        \n",
				"        spark.sql(final_sql)\n",
				"        spark.catalog.dropTempView(\"_upsert_source_temp\")\n",
				"        logging.info(f\"‚úì Upserted records into {target_table}\")\n",
				"    \n",
				"    def _update_mapping(self, df: DataFrame, target_table: str, target_primary_key: str, mapping_table_name: Optional[str] = None):\n",
				"        \"\"\"\n",
				"        Register the source ID ‚Üí Silver GUID relationships in the mapping table.\n",
				"        \n",
				"        This is our \"phone book\" that lets us find Silver records when we know\n",
				"        their source system ID. Critical for updates and deletes in future syncs.\n",
				"        \n",
				"        Args:\n",
				"            mapping_table_name: Override SourceTable value. Use when target differs from logical source.\n",
				"                              If None, uses target_table (default behavior).\n",
				"        \"\"\"\n",
				"        # Use override if specified, otherwise default to target_table\n",
				"        source_table_value = mapping_table_name or target_table\n",
				"        \n",
				"        mapping_df = df.select(\n",
				"            col(target_primary_key).alias(\"SilverRecordId\"),\n",
				"            col(\"SourceId\"),\n",
				"            col(\"SourceSystemId\"),\n",
				"            lit(source_table_value).alias(\"SourceTable\")\n",
				"        )\n",
				"        \n",
				"        mapping_df.createOrReplaceTempView(\"_mapping_source_temp\")\n",
				"        \n",
				"        merge_sql = f\"\"\"\n",
				"        MERGE INTO {self.mapping_table} AS t\n",
				"        USING _mapping_source_temp AS s\n",
				"          ON t.SilverRecordId = s.SilverRecordId\n",
				"        WHEN MATCHED THEN UPDATE SET\n",
				"            t.SourceId = s.SourceId,\n",
				"            t.SourceSystemId = s.SourceSystemId,\n",
				"            t.SourceTable = s.SourceTable\n",
				"        WHEN NOT MATCHED THEN INSERT (SilverRecordId, SourceId, SourceSystemId, SourceTable)\n",
				"        VALUES (s.SilverRecordId, s.SourceId, s.SourceSystemId, s.SourceTable)\n",
				"        \"\"\"\n",
				"        \n",
				"        spark.sql(merge_sql)\n",
				"        spark.catalog.dropTempView(\"_mapping_source_temp\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "85ba5e21",
			"metadata": {},
			"source": [
				"### Dynamics 365 data sync"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "71529053-d9a1-4b5b-88d2-932d62985d14",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from typing import List, Dict, Optional, Callable\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql.functions import col, lit\n",
				"import logging\n",
				"\n",
				"\n",
				"class Dynamics365DataSync(Dynamics365BaseSync):\n",
				"    \"\"\"\n",
				"    Main orchestration class for Dynamics 365 Bronze-to-Silver synchronization.\n",
				"    \n",
				"    Uses composition pattern with specialized components:\n",
				"    - DataReader: Reads data from Bronze lakehouse\n",
				"    - FKResolver: Resolves Bronze FKs to Silver GUIDs  \n",
				"    - DataWriter: Writes data to Silver lakehouse\n",
				"    \n",
				"    Features:\n",
				"    - Watermark-based incremental synchronization\n",
				"    - Automatic GUID generation for Silver IDs\n",
				"    - SourceSystemIdMapping maintenance\n",
				"    - Hard delete support with optional policy\n",
				"    - Custom transformation functions\n",
				"    - Foreign key resolution\n",
				"    \"\"\"\n",
				"    \n",
				"    def __init__(\n",
				"        self, \n",
				"        source_id: str, \n",
				"        source_lakehouse: str, \n",
				"        target_lakehouse: str,\n",
				"        watermark_column: str = \"SinkModifiedOn\",\n",
				"        delete_flag_column: str = \"IsDelete\",\n",
				"        enable_hard_delete: bool = True\n",
				"    ):\n",
				"        \"\"\"\n",
				"        Initialize the Dynamics 365 data synchronizer with composed components.\n",
				"        \n",
				"        Args:\n",
				"            source_id: SourceId GUID for this data source\n",
				"            source_lakehouse: Bronze lakehouse name\n",
				"            target_lakehouse: Silver lakehouse name\n",
				"            watermark_column: Default column for incremental sync tracking (default: \"SinkModifiedOn\")\n",
				"            delete_flag_column: Default column indicating deleted records (default: \"IsDelete\")\n",
				"            enable_hard_delete: Whether to process hard deletes by default (default: True)\n",
				"        \"\"\"\n",
				"        super().__init__(source_id, source_lakehouse, target_lakehouse)\n",
				"        \n",
				"        # Initialize watermark service\n",
				"        self.watermark_service = Dynamics365Watermark(source_lakehouse, target_lakehouse)\n",
				"        self.watermark_column = watermark_column\n",
				"        self.delete_flag_column = delete_flag_column\n",
				"        self.enable_hard_delete = enable_hard_delete\n",
				"        \n",
				"        # Composition: Initialize component objects\n",
				"        self.reader = Dynamics365DataReader(\n",
				"            source_lakehouse=source_lakehouse,\n",
				"            watermark_column=watermark_column,\n",
				"            delete_flag_column=delete_flag_column\n",
				"        )\n",
				"        \n",
				"        self.resolver = Dynamics365ForeignKeyResolver(\n",
				"            source_id=source_id,\n",
				"            source_lakehouse=source_lakehouse,\n",
				"            target_lakehouse=target_lakehouse,\n",
				"            mapping_table=self.mapping_table\n",
				"        )\n",
				"        \n",
				"        self.writer = Dynamics365DataWriter(\n",
				"            source_id=source_id,\n",
				"            target_lakehouse=target_lakehouse,\n",
				"            mapping_table=self.mapping_table,\n",
				"            enable_hard_delete=enable_hard_delete\n",
				"        )\n",
				"    \n",
				"    # region Public API - Main Entry Points\n",
				"    \n",
				"    def sync_table(\n",
				"        self,\n",
				"        source_table: str,\n",
				"        source_primary_key: str,\n",
				"        source_columns: List[str],\n",
				"        target_table: str,\n",
				"        target_primary_key: str,\n",
				"        transform_func: Optional[Callable[[DataFrame], DataFrame]] = None,\n",
				"        delete_policy_func: Optional[Callable[[DataFrame], DataFrame]] = None,\n",
				"        fk_mappings: Optional[Dict[str, str]] = None,\n",
				"        merge_sql: Optional[str] = None,\n",
				"        mapping_source_table: Optional[str] = None,\n",
				"        source_table_lakehouse: Optional[str] = None\n",
				"    ):\n",
				"        \"\"\"\n",
				"        Synchronize a single data table from Bronze to Silver.\n",
				"        \n",
				"        This is the MAIN ENTRY POINT for standard entity synchronization.\n",
				"        Orchestrates the full sync flow using composed components.\n",
				"        \n",
				"        Args:\n",
				"            source_table: Bronze table name (e.g., \"campaign\", \"contact\")\n",
				"            source_primary_key: Bronze PK column name (e.g., \"Id\")\n",
				"            source_columns: List of columns to read from Bronze\n",
				"            target_table: Silver table name (e.g., \"Campaign\", \"Contact\")\n",
				"            target_primary_key: Silver PK column name (e.g., \"CampaignId\", \"ContactId\")\n",
				"            transform_func: Optional function to transform Bronze data before sync.\n",
				"                          Receives DataFrame with Bronze columns PLUS auto-resolved FK columns.\n",
				"                          Auto-resolved columns are named: {source_column}_SilverRecordId\n",
				"                          Should return DataFrame with Silver schema.\n",
				"                          Note: SourceId and SourceSystemId are automatically added if missing.\n",
				"            delete_policy_func: Optional function to filter deletes before execution.\n",
				"                              Receives DataFrame with Silver PKs to delete.\n",
				"                              Should return filtered DataFrame.\n",
				"            fk_mappings: Optional dict mapping Bronze FK columns to Silver tables.\n",
				"                        Format: {\"bronze_column\": \"SilverTableName\"}\n",
				"                        Example: {\"typecode\": \"CampaignType\", \"address1_addressid\": \"Address\"}\n",
				"                        Auto-creates columns: typecode_SilverRecordId, address1_addressid_SilverRecordId\n",
				"            merge_sql: Optional custom MERGE SQL statement for fine-grained control.\n",
				"                      Use placeholders: {TARGET_TABLE}, {SOURCE_VIEW}, {TARGET_PK}\n",
				"                      If not provided, auto-generates MERGE based on columns.\n",
				"            mapping_source_table: Override SourceTable value in SourceSystemIdMapping.\n",
				"                                Use when target table differs from logical source (e.g., Constituent).\n",
				"                                If None, uses target_table (default behavior).\n",
				"            source_table_lakehouse: Lakehouse prefix for source_table. If None, uses configured Bronze lakehouse.\n",
				"                                   Set to empty string \"\" to read without prefix (for TEMPORARY VIEW).\n",
				"        \"\"\"\n",
				"        logging.info(f\"üîÑ Starting sync: {source_table} ‚Üí {target_table}\")\n",
				"        \n",
				"        # Use override if specified, otherwise default to target_table\n",
				"        actual_mapping_table = mapping_source_table or target_table\n",
				"        \n",
				"        # 1. Determine which records need to be processed (only new/modified since last run)\n",
				"        last_watermark = self.watermark_service.get_last_watermark(source_table, target_table)\n",
				"        current_watermark = self.watermark_service.get_current_watermark(source_table, self.watermark_column, source_table_lakehouse)\n",
				"        \n",
				"        if self._is_already_synced(last_watermark, current_watermark):\n",
				"            logging.info(f\"‚úÖ {target_table} is up to date (watermark: {last_watermark})\")\n",
				"            return\n",
				"        \n",
				"        logging.info(f\"üìä Sync window: {last_watermark} ‚Üí {current_watermark}\")\n",
				"        \n",
				"        # 2. Load only records that changed since last sync (READER component)\n",
				"        df_changes = self.reader.read_incremental_changes(\n",
				"            source_table, source_primary_key, source_columns,\n",
				"            last_watermark, current_watermark, source_table_lakehouse\n",
				"        )\n",
				"        \n",
				"        if df_changes.isEmpty():\n",
				"            self._complete_sync_without_changes(source_table, target_table, current_watermark)\n",
				"            return\n",
				"        \n",
				"        # 3. Separate records into two groups: those to delete vs. those to insert/update (READER component)\n",
				"        df_deletes = self.reader.extract_deletes(df_changes, source_primary_key)\n",
				"        df_active = self.reader.extract_active(df_changes, source_primary_key)\n",
				"\n",
				"        # 4. Connect related data - resolve FKs (RESOLVER component)\n",
				"        if not df_active.isEmpty() and fk_mappings:\n",
				"            df_active = self.resolver.resolve_foreign_keys(df_active, fk_mappings)\n",
				"        \n",
				"        # 5. Apply all changes to the target table (WRITER component orchestration)\n",
				"        self._execute_sync_operations(\n",
				"            df_deletes, df_active,\n",
				"            source_primary_key, target_table, target_primary_key,\n",
				"            delete_policy_func, transform_func, merge_sql, actual_mapping_table\n",
				"        )\n",
				"        \n",
				"        # 6. Remember where we finished so next sync starts from here\n",
				"        self._finalize_sync(source_table, target_table, current_watermark)\n",
				"    \n",
				"    def sync_activity_table(\n",
				"        self,\n",
				"        *,\n",
				"        source_table: str,\n",
				"        source_primary_key: str,\n",
				"        source_columns: list,\n",
				"        target_table: str,\n",
				"        target_primary_key: str,\n",
				"        activity_party_join_func: Callable[[DataFrame], DataFrame],\n",
				"        transform_func: Optional[Callable[[DataFrame], DataFrame]] = None,\n",
				"        fk_mappings: Optional[Dict[str, str]] = None,\n",
				"        merge_sql: Optional[str] = None\n",
				"    ):\n",
				"        \"\"\"\n",
				"        Synchronize Dynamics 365 activity tables where one Bronze record ‚Üí N Silver records.\n",
				"        \n",
				"        Activities (letter, phonecall, email) explode into multiple Silver records\n",
				"        based on activityparty relationships. Each activityparty becomes one Silver row.\n",
				"        \n",
				"        Pattern:\n",
				"        1. Bronze activity (letter) ‚Üí incremental slice\n",
				"        2. Join with activityparty (1:N explosion)\n",
				"        3. Resolve FKs (Campaign, Contact/Account ‚Üí Constituent)\n",
				"        4. Transform to Silver schema\n",
				"        5. MERGE with composite key (SourceId, SourceSystemId=ActivityPartyId, ConstituentId)\n",
				"        \n",
				"        Args:\n",
				"            source_table: Bronze activity table name (e.g., \"letter\")\n",
				"            source_primary_key: Bronze PK (e.g., \"Id\")\n",
				"            source_columns: Columns to read from Bronze\n",
				"            target_table: Silver table name (e.g., \"Letter\")\n",
				"            target_primary_key: Silver PK (e.g., \"LetterId\")\n",
				"            activity_party_join_func: Function that joins activity ‚Üí activityparty + enrichment.\n",
				"                                     Input: incremental activity df\n",
				"                                     Output: exploded df with ActivityPartyId column\n",
				"            transform_func: Optional final transformation to Silver schema\n",
				"            fk_mappings: Standard FK mappings (e.g., {\"regardingobjectid\": \"Campaign\"})\n",
				"            merge_sql: Custom MERGE SQL with composite key matching\n",
				"        \n",
				"        Returns:\n",
				"            None\n",
				"        \n",
				"        Example activity_party_join_func:\n",
				"            def enrich_letter(df):\n",
				"                # Join activity ‚Üí activityparty (1:N explosion)\n",
				"                # Resolve Campaign (regardingobjectid ‚Üí Campaign)\n",
				"                # Resolve Contact/Account ‚Üí Constituent\n",
				"                # Return df with ActivityPartyId as SourceSystemId\n",
				"                return df\n",
				"        \"\"\"\n",
				"        logging.info(f\"üîÑ Starting activity sync: {source_table} ‚Üí {target_table}\")\n",
				"        \n",
				"        # 1. Watermark-based incremental read\n",
				"        last_watermark = self.watermark_service.get_last_watermark(source_table, target_table)\n",
				"        current_watermark = self.watermark_service.get_current_watermark(source_table, self.watermark_column, None)\n",
				"        \n",
				"        if self._is_already_synced(last_watermark, current_watermark):\n",
				"            logging.info(f\"‚úÖ {target_table} is up to date (watermark: {last_watermark})\")\n",
				"            return\n",
				"        \n",
				"        logging.info(f\"üìä Sync window: {last_watermark} ‚Üí {current_watermark}\")\n",
				"        \n",
				"        # 2. Load incremental changes (READER component)\n",
				"        df_changes = self.reader.read_incremental_changes(\n",
				"            source_table, source_primary_key, source_columns,\n",
				"            last_watermark, current_watermark\n",
				"        )\n",
				"        \n",
				"        if df_changes.isEmpty():\n",
				"            self._complete_sync_without_changes(source_table, target_table, current_watermark)\n",
				"            return\n",
				"        \n",
				"        # 3. Activity-specific: no delete handling (activities don't typically delete via IsDelete)\n",
				"        # Activities are event-based - once sent, they exist forever\n",
				"        \n",
				"        # 4. Activity party explosion + enrichment\n",
				"        logging.info(f\"üéØ Joining with activityparty (1:N explosion)\")\n",
				"        df_exploded = activity_party_join_func(df_changes)\n",
				"        \n",
				"        if df_exploded.isEmpty():\n",
				"            logging.warning(f\"‚ö†Ô∏è  No activity parties found for {source_table} in sync window\")\n",
				"            self._complete_sync_without_changes(source_table, target_table, current_watermark)\n",
				"            return\n",
				"        \n",
				"        # 5. Resolve FKs (RESOLVER component)\n",
				"        if fk_mappings:\n",
				"            df_exploded = self.resolver.resolve_foreign_keys(df_exploded, fk_mappings)\n",
				"        \n",
				"        # 6. Apply transformation\n",
				"        if transform_func:\n",
				"            df_exploded = transform_func(df_exploded)\n",
				"        \n",
				"        # 7. Add SourceId\n",
				"        df_exploded = df_exploded.withColumn(\"SourceId\", lit(self.source_id))\n",
				"        \n",
				"        # 8. Validate required columns\n",
				"        if \"SourceSystemId\" not in df_exploded.columns:\n",
				"            raise ValueError(f\"Activity join func must produce 'SourceSystemId' column (should be ActivityPartyId)\")\n",
				"        \n",
				"        # 9. Execute MERGE with custom SQL (composite key)\n",
				"        if not merge_sql:\n",
				"            raise ValueError(f\"Activity sync requires custom merge_sql with composite key (SourceId, SourceSystemId, ConstituentId)\")\n",
				"        \n",
				"        # Process upserts using WRITER component\n",
				"        self.writer.process_upserts(\n",
				"            df_active=df_exploded,\n",
				"            source_primary_key=\"SourceSystemId\",  # ActivityPartyId\n",
				"            target_table=target_table,\n",
				"            target_primary_key=target_primary_key,\n",
				"            transform_func=None,  # Already transformed\n",
				"            merge_sql=merge_sql,\n",
				"            mapping_table_name=target_table\n",
				"        )\n",
				"        \n",
				"        # 10. Update watermark\n",
				"        self._finalize_sync(source_table, target_table, current_watermark)\n",
				"    \n",
				"    # endregion\n",
				"    \n",
				"    # region Public API - Convenience Delegates (delegate to components)\n",
				"    \n",
				"    def resolve_foreign_keys(self, df: DataFrame, fk_mappings: Dict[str, str]) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Delegate to FK resolver component.\n",
				"        Public method for custom transformations that need additional FK resolution.\n",
				"        \"\"\"\n",
				"        return self.resolver.resolve_foreign_keys(df, fk_mappings)\n",
				"    \n",
				"    def resolve_constituent_id(\n",
				"        self,\n",
				"        df: DataFrame,\n",
				"        contact_fk_column: Optional[str] = None,\n",
				"        account_fk_column: Optional[str] = None,\n",
				"        output_column: str = \"ConstituentId\"\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Delegate to FK resolver component.\n",
				"        Public method for resolving Constituent from Contact/Account.\n",
				"        \"\"\"\n",
				"        return self.resolver.resolve_constituent_id(df, contact_fk_column, account_fk_column, output_column)\n",
				"    \n",
				"    def resolve_polymorphic_lookup(\n",
				"        self,\n",
				"        df: DataFrame,\n",
				"        lookup_id_column: str,\n",
				"        entity_type_column: str,\n",
				"        entity_type_mappings: dict\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Delegate to FK resolver component.\n",
				"        Public method for resolving polymorphic lookups.\n",
				"        \"\"\"\n",
				"        return self.resolver.resolve_polymorphic_lookup(df, lookup_id_column, entity_type_column, entity_type_mappings)\n",
				"    \n",
				"    def read_activity_parties(\n",
				"        self,\n",
				"        activity_id_column: str = \"activityid\",\n",
				"        participation_types: list = [1, 2]\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Delegate to reader component.\n",
				"        Public method for reading activityparty records.\n",
				"        \"\"\"\n",
				"        return self.reader.read_activity_parties(activity_id_column, participation_types)\n",
				"    \n",
				"    def read_bronze_lookup(\n",
				"        self,\n",
				"        table_name: str,\n",
				"        columns: dict,\n",
				"        filters: Optional[List] = None\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Delegate to reader component.\n",
				"        Public method for reading Bronze lookup tables.\n",
				"        \"\"\"\n",
				"        return self.reader.read_bronze_lookup(table_name, columns, filters)\n",
				"    \n",
				"    # endregion\n",
				"    \n",
				"    # region Private Helpers - Watermark Management\n",
				"    \n",
				"    def _is_already_synced(self, last_watermark, current_watermark) -> bool:\n",
				"        \"\"\"\n",
				"        Check if we've already processed all available data.\n",
				"        \n",
				"        Returns True when the newest record in source is older than what we've already synced.\n",
				"        This means there's nothing new to process.\n",
				"        \"\"\"\n",
				"        return (current_watermark is not None and \n",
				"                last_watermark is not None and \n",
				"                current_watermark <= last_watermark)\n",
				"    \n",
				"    def _complete_sync_without_changes(self, source_table: str, target_table: str, current_watermark):\n",
				"        \"\"\"\n",
				"        Finish the sync when no data changed since last run.\n",
				"        \n",
				"        Even though nothing changed, we still update our bookmark (watermark)\n",
				"        so we know we checked at this point in time.\n",
				"        \"\"\"\n",
				"        logging.info(f\"‚ÑπÔ∏è  No changes detected for {target_table}\")\n",
				"        self.watermark_service.update_watermark(source_table, target_table, current_watermark)\n",
				"    \n",
				"    def _finalize_sync(self, source_table: str, target_table: str, current_watermark):\n",
				"        \"\"\"\n",
				"        Mark this sync as complete and save our progress.\n",
				"        \n",
				"        The watermark acts like a bookmark - it tells the next sync run\n",
				"        where to start reading, so we don't re-process old data.\n",
				"        \"\"\"\n",
				"        self.watermark_service.update_watermark(source_table, target_table, current_watermark)\n",
				"        logging.info(f\"‚úÖ Sync complete: {target_table}\")\n",
				"    \n",
				"    # endregion\n",
				"    \n",
				"    # region Private Helpers - Sync Orchestration\n",
				"    \n",
				"    def _execute_sync_operations(\n",
				"        self,\n",
				"        df_deletes: DataFrame,\n",
				"        df_active: DataFrame,\n",
				"        source_primary_key: str,\n",
				"        target_table: str,\n",
				"        target_primary_key: str,\n",
				"        delete_policy_func: Optional[Callable[[DataFrame], DataFrame]],\n",
				"        transform_func: Optional[Callable[[DataFrame], DataFrame]],\n",
				"        merge_sql: Optional[str],\n",
				"        mapping_table_name: Optional[str] = None\n",
				"    ):\n",
				"        \"\"\"\n",
				"        Apply all data changes to the target table in the correct sequence.\n",
				"        \n",
				"        We always delete first, then insert/update. This order prevents conflicts\n",
				"        when a record is deleted and re-created with the same ID.\n",
				"        \n",
				"        Delegates actual operations to WRITER component.\n",
				"        \"\"\"\n",
				"        # Remove records that were deleted in the source system (WRITER component)\n",
				"        self.writer.process_deletes(\n",
				"            df_deletes, source_primary_key, target_table,\n",
				"            target_primary_key, delete_policy_func\n",
				"        )\n",
				"        \n",
				"        # Add new records and update existing ones with fresh data (WRITER component)\n",
				"        self.writer.process_upserts(\n",
				"            df_active, source_primary_key, target_table,\n",
				"            target_primary_key, transform_func, merge_sql, mapping_table_name\n",
				"        )\n",
				"    \n",
				"    # endregion"
			]
		},
		{
			"cell_type": "markdown",
			"id": "74752e18-2233-41be-abce-28053b155411",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Helper Functions\n",
				"\n",
				"Utility functions for common data transformation patterns in Dynamics 365 synchronization.\n",
				"\n",
				"**D365TransformHelpers Class:**\n",
				"Static utility class containing all reusable helper functions. All methods are static and can be called directly:\n",
				"```python\n",
				"D365TransformHelpers.deduplicate_by_window(df, \"country\", \"id\")\n",
				"D365TransformHelpers.join_case_insensitive(df1, df2, \"Name\", \"CountryName\", \"left\")\n",
				"```\n",
				"\n",
				"**Available Methods:**\n",
				"- `deduplicate_by_window()` - Deduplicate DataFrame using window function with row_number\n",
				"- `normalize_for_matching()` - Normalize string column for case-insensitive matching (lower/upper + trim)\n",
				"- `join_case_insensitive()` - Join DataFrames with automatic case-insensitive string matching\n",
				"\n",
				"**Usage Examples:**\n",
				"```python\n",
				"# Deduplicate by window\n",
				"df_clean = D365TransformHelpers.deduplicate_by_window(\n",
				"    df, \n",
				"    partition_by=\"customer_id\",\n",
				"    order_by=\"modified_date\"\n",
				")\n",
				"\n",
				"# Case-insensitive join (Country lookup)\n",
				"result = D365TransformHelpers.join_case_insensitive(\n",
				"    address_df, country_df,\n",
				"    left_column=\"CountryName\",\n",
				"    right_column=\"Name\",\n",
				"    how=\"left\"\n",
				")\n",
				"\n",
				"# Manual normalization (when join helper doesn't fit)\n",
				"df = D365TransformHelpers.normalize_for_matching(df, \"Name\", case_mode=\"upper\")\n",
				"```\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a58ee22f-7935-475c-a8df-bfe96b8cd7a7",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import DataFrame, Window\n",
				"from pyspark.sql.functions import col, row_number\n",
				"from typing import Union, List, Optional\n",
				"\n",
				"\n",
				"class D365TransformHelpers:\n",
				"    \"\"\"\n",
				"    Static utility class containing reusable helper functions for Dynamics 365 transformations.\n",
				"    \n",
				"    All methods are static and can be called directly:\n",
				"        D365TransformHelpers.deduplicate_by_window(df, \"country\", \"id\")\n",
				"    \n",
				"    This class groups common transformation patterns to:\n",
				"    - Reduce code duplication\n",
				"    - Improve maintainability\n",
				"    - Provide consistent patterns across transformations\n",
				"    \"\"\"\n",
				"    \n",
				"    @staticmethod\n",
				"    def deduplicate_by_window(\n",
				"        df: DataFrame,\n",
				"        partition_by: Union[str, List[str], any],\n",
				"        order_by: Union[str, List[str], any],\n",
				"        keep: str = \"first\"\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Deduplicate DataFrame using window function with row_number.\n",
				"        \n",
				"        This is a common pattern in Dynamics 365 transformations where we need to:\n",
				"        - Pick one record per group (e.g., one Country per name)\n",
				"        - Select based on priority ordering (e.g., prefer active records)\n",
				"        - Remove duplicates after normalization\n",
				"        \n",
				"        Args:\n",
				"            df: Input DataFrame\n",
				"            partition_by: Column(s) to partition by. Can be:\n",
				"                         - String column name: \"country\"\n",
				"                         - List of strings: [\"opportunityid\", \"statecode\"]\n",
				"                         - Column expression: F.upper(F.col(\"Name\"))\n",
				"                         - List of Column expressions\n",
				"            order_by: Column(s) to order by (determines which record to keep). Can be:\n",
				"                     - String column name: \"customeraddressid\"\n",
				"                     - List of strings: [\"statecode\", \"modified\"]\n",
				"                     - Column expression: col(\"modified\").desc()\n",
				"                     - List of Column expressions (complex ordering)\n",
				"            keep: Which record to keep - \"first\" or \"last\" (default: \"first\")\n",
				"                 Note: \"first\" keeps first record after ordering (row_number = 1)\n",
				"        \n",
				"        Returns:\n",
				"            Deduplicated DataFrame with one record per partition\n",
				"        \n",
				"        Examples:\n",
				"            # Simple case: deduplicate by country name, keep first customeraddressid\n",
				"            D365TransformHelpers.deduplicate_by_window(df, \"country_normalized\", \"customeraddressid\")\n",
				"            \n",
				"            # Multiple partition columns\n",
				"            D365TransformHelpers.deduplicate_by_window(df, [\"opportunityid\", \"type\"], \"modified\")\n",
				"            \n",
				"            # Complex ordering with Column expressions\n",
				"            D365TransformHelpers.deduplicate_by_window(\n",
				"                df, \n",
				"                \"opportunityid\",\n",
				"                [when(col(\"statecode\") == 0, 0).otherwise(1).asc(),\n",
				"                 col(\"modified\").desc_nulls_last()]\n",
				"            )\n",
				"            \n",
				"            # Using Column expressions for partition (normalized name matching)\n",
				"            D365TransformHelpers.deduplicate_by_window(\n",
				"                df,\n",
				"                F.upper(F.trim(F.col(\"Name\"))),\n",
				"                \"CreatedDate\"\n",
				"            )\n",
				"        \n",
				"        Technical notes:\n",
				"            - Uses window function with row_number (efficient for large datasets)\n",
				"            - Temporary column __rn__ is used internally and cleaned up\n",
				"            - When keep=\"last\", window ordering is reversed automatically\n",
				"        \"\"\"\n",
				"        # Normalize partition_by to list\n",
				"        if isinstance(partition_by, str):\n",
				"            partition_cols = [col(partition_by)]\n",
				"        elif isinstance(partition_by, list):\n",
				"            # Check if list contains strings or Column objects\n",
				"            partition_cols = [col(c) if isinstance(c, str) else c for c in partition_by]\n",
				"        else:\n",
				"            # Single Column expression\n",
				"            partition_cols = [partition_by]\n",
				"        \n",
				"        # Normalize order_by to list\n",
				"        if isinstance(order_by, str):\n",
				"            order_cols = [col(order_by)]\n",
				"        elif isinstance(order_by, list):\n",
				"            # Check if list contains strings or Column objects\n",
				"            order_cols = [col(c) if isinstance(c, str) else c for c in order_by]\n",
				"        else:\n",
				"            # Single Column expression\n",
				"            order_cols = [order_by]\n",
				"        \n",
				"        # Reverse ordering if keep=\"last\"\n",
				"        if keep == \"last\":\n",
				"            # Reverse each order column (asc ‚Üí desc, desc ‚Üí asc)\n",
				"            reversed_cols = []\n",
				"            for order_col in order_cols:\n",
				"                # This is a simplified approach - for full implementation would need to detect\n",
				"                # current sort direction and reverse it\n",
				"                reversed_cols.append(order_col.desc())\n",
				"            order_cols = reversed_cols\n",
				"        \n",
				"        # Build window specification\n",
				"        w = Window.partitionBy(*partition_cols).orderBy(*order_cols)\n",
				"        \n",
				"        # Apply row_number, filter, and cleanup\n",
				"        return (df.withColumn(\"__rn__\", row_number().over(w))\n",
				"                 .filter(col(\"__rn__\") == 1)\n",
				"                 .drop(\"__rn__\"))\n",
				"    \n",
				"    @staticmethod\n",
				"    def normalize_for_matching(\n",
				"        df: DataFrame,\n",
				"        column: str,\n",
				"        output_column: Optional[str] = None,\n",
				"        case_mode: str = \"lower\"\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Normalize string column for case-insensitive matching.\n",
				"        \n",
				"        Common pattern in Dynamics 365 where entity names, lookup values, and text fields\n",
				"        need case-insensitive comparison. Applies trim() + upper()/lower() normalization.\n",
				"        \n",
				"        Args:\n",
				"            df: Input DataFrame\n",
				"            column: Column name to normalize\n",
				"            output_column: Name for normalized column (default: \"__normalized_{column}\")\n",
				"            case_mode: \"lower\" or \"upper\" normalization (default: \"lower\")\n",
				"        \n",
				"        Returns:\n",
				"            DataFrame with additional normalized column\n",
				"        \n",
				"        Examples:\n",
				"            # Add normalized Name column for matching\n",
				"            df = D365TransformHelpers.normalize_for_matching(df, \"Name\")\n",
				"            # Result: adds column __normalized_Name with lower(trim(Name))\n",
				"            \n",
				"            # Custom output column name\n",
				"            df = D365TransformHelpers.normalize_for_matching(\n",
				"                df, \"CountryName\", \n",
				"                output_column=\"country_normalized\",\n",
				"                case_mode=\"upper\"\n",
				"            )\n",
				"            \n",
				"            # Multiple normalizations for complex matching\n",
				"            df = D365TransformHelpers.normalize_for_matching(df, \"FirstName\")\n",
				"            df = D365TransformHelpers.normalize_for_matching(df, \"LastName\")\n",
				"        \n",
				"        Technical notes:\n",
				"            - Uses lower(trim()) by default for case-insensitive matching\n",
				"            - Temporary normalized columns should be dropped after join/comparison\n",
				"            - Prefer using join_case_insensitive() for direct joins\n",
				"        \"\"\"\n",
				"        from pyspark.sql.functions import upper, lower, trim\n",
				"        \n",
				"        if output_column is None:\n",
				"            output_column = f\"__normalized_{column}\"\n",
				"        \n",
				"        case_func = upper if case_mode == \"upper\" else lower\n",
				"        \n",
				"        return df.withColumn(\n",
				"            output_column,\n",
				"            case_func(trim(col(column)))\n",
				"        )\n",
				"    \n",
				"    @staticmethod\n",
				"    def join_case_insensitive(\n",
				"        left_df: DataFrame,\n",
				"        right_df: DataFrame,\n",
				"        left_column: str,\n",
				"        right_column: str,\n",
				"        how: str = \"inner\"\n",
				"    ) -> DataFrame:\n",
				"        \"\"\"\n",
				"        Join DataFrames with case-insensitive string matching.\n",
				"        \n",
				"        Common pattern in Dynamics 365 lookups where entity names or text fields\n",
				"        need to be matched regardless of case. Automatically normalizes join columns\n",
				"        and cleans up temporary columns.\n",
				"        \n",
				"        Args:\n",
				"            left_df: Left DataFrame\n",
				"            right_df: Right DataFrame\n",
				"            left_column: Column name in left DataFrame to join on\n",
				"            right_column: Column name in right DataFrame to join on\n",
				"            how: Join type - \"inner\", \"left\", \"right\", \"outer\" (default: \"inner\")\n",
				"        \n",
				"        Returns:\n",
				"            Joined DataFrame with temporary normalized columns removed\n",
				"        \n",
				"        Examples:\n",
				"            # Country lookup by name (case-insensitive)\n",
				"            result = D365TransformHelpers.join_case_insensitive(\n",
				"                address_df, country_df,\n",
				"                left_column=\"CountryName\",\n",
				"                right_column=\"Name\",\n",
				"                how=\"left\"\n",
				"            )\n",
				"            \n",
				"            # ConstituentType matching\n",
				"            result = D365TransformHelpers.join_case_insensitive(\n",
				"                source_df, constituent_type_df,\n",
				"                left_column=\"TypeName\",\n",
				"                right_column=\"Name\",\n",
				"                how=\"inner\"\n",
				"            )\n",
				"            \n",
				"            # Multiple case-insensitive joins in sequence\n",
				"            df = D365TransformHelpers.join_case_insensitive(df, lookup1, \"Field1\", \"Name\")\n",
				"            df = D365TransformHelpers.join_case_insensitive(df, lookup2, \"Field2\", \"Code\")\n",
				"        \n",
				"        Technical notes:\n",
				"            - Uses lower(trim()) for normalization\n",
				"            - Automatically cleans up __left_norm__ and __right_norm__ columns\n",
				"            - More efficient than manual normalize + join + cleanup pattern\n",
				"        \"\"\"\n",
				"        from pyspark.sql.functions import lower, trim\n",
				"        \n",
				"        # Normalize both sides\n",
				"        left_norm = left_df.withColumn(\"__left_norm__\", lower(trim(col(left_column))))\n",
				"        right_norm = right_df.withColumn(\"__right_norm__\", lower(trim(col(right_column))))\n",
				"        \n",
				"        # Join on normalized columns\n",
				"        result = left_norm.join(\n",
				"            right_norm,\n",
				"            col(\"__left_norm__\") == col(\"__right_norm__\"),\n",
				"            how=how\n",
				"        )\n",
				"        \n",
				"        # Cleanup temporary columns\n",
				"        return result.drop(\"__left_norm__\", \"__right_norm__\")"
			]
		}
	],
	"metadata": {
		"dependencies": {},
		"kernel_info": {
			"name": "synapse_pyspark"
		},
		"kernelspec": {
			"display_name": "synapse_pyspark",
			"name": "synapse_pyspark"
		},
		"language_info": {
			"name": "python"
		},
		"microsoft": {
			"language": "python",
			"language_group": "synapse_pyspark",
			"ms_spell_check": {
				"ms_spell_check_language": "en"
			}
		},
		"nteract": {
			"version": "nteract-front-end@1.0.0"
		},
		"spark_compute": {
			"compute_id": "/trident/default",
			"session_options": {
				"conf": {
					"spark.synapse.nbs.session.timeout": "1200000"
				}
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
