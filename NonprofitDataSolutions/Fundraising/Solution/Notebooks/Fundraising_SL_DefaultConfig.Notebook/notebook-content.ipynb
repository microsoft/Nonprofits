{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3515d48d-dd5f-46e0-b645-f92e513e0e99",
			"metadata": {
				"editable": false,
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"%run <Fundraising_Config>"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1181c0db-450f-4cca-b90e-85cce4a1e561",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"# Default configuration"
			]
		},
		{
			"cell_type": "markdown",
			"id": "7d33d73e-f8a9-46e0-a442-dbe3e6d2fdde",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Cofiguration"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0d129fc1-7f13-48ee-be40-f5526df4aa2f",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import Row\n",
				"from pyspark.sql.functions import current_timestamp\n",
				"import logging\n",
				"\n",
				"# ------------------------------------------------------------\n",
				"# Configuration Data\n",
				"# ------------------------------------------------------------\n",
				"configuration_data = [\n",
				"    (\"e5d8f734-b17f-4cc8-b8c2-fc9738add78f\", \"Currency\", \"USD\"),\n",
				"    (\"d0da94c0-c22c-46f8-a958-0f03cff6a69f\", \"CurrencySymbol\", \"$\"),\n",
				"    (\"aec9bc8e-d40d-45c6-8def-46eb088b250b\", \"FiscalYearStartMonth\", \"7\"),\n",
				"    (\"5cb4d467-ffd6-4370-a8c5-c1e5e802fd46\", \"IsCurrencySymbolPrefix\", \"true\")\n",
				"]\n",
				"\n",
				"# Create DataFrame with schema and timestamps\n",
				"df_configuration = spark.createDataFrame(\n",
				"    [Row(ConfigurationId=id, Name=name, Value=value) for id, name, value in configuration_data]\n",
				").withColumn(\"CreatedDate\", current_timestamp()) \\\n",
				" .withColumn(\"ModifiedDate\", current_timestamp())\n",
				"\n",
				"# Register temp view and execute MERGE\n",
				"df_configuration.createOrReplaceTempView(\"default_configuration\")\n",
				"\n",
				"# MERGE values\n",
				"configuration_table_name = get_full_table_name(silver_lakehouse_name, \"Configuration\")\n",
				"spark.sql(f\"\"\"\n",
				"    MERGE INTO {configuration_table_name} AS target\n",
				"    USING default_configuration AS source\n",
				"      ON target.Name = source.Name\n",
				"    WHEN NOT MATCHED THEN\n",
				"      INSERT (\n",
				"        ConfigurationId,\n",
				"        CreatedDate,\n",
				"        ModifiedDate,\n",
				"        Name,\n",
				"        Value\n",
				"      )\n",
				"      VALUES (\n",
				"        source.ConfigurationId,\n",
				"        source.CreatedDate,\n",
				"        source.ModifiedDate,\n",
				"        source.Name,\n",
				"        source.Value\n",
				"      )\n",
				"\"\"\")\n",
				"\n",
				"# Clean up temporary view\n",
				"spark.sql(\"DROP VIEW IF EXISTS default_configuration\")\n",
				"\n",
				"logging.info(\"✅ Default Configuration created\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "536b8a3b-2d1f-4740-8804-e29713aa4e6d",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Channels"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d3c60ba6-d921-45be-aae6-e1289402ddd8",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import Row\n",
				"from pyspark.sql.functions import current_timestamp\n",
				"import logging\n",
				"\n",
				"# ------------------------------------------------------------\n",
				"# Channel Data\n",
				"# ------------------------------------------------------------\n",
				"channel_names = [\n",
				"    (\"Email\",          \"4ba4a473-aeb9-4743-b4b5-1a2ec92f2323\"),\n",
				"    (\"Social Media\",   \"a13b7e00-c20d-448b-b056-4b38d3e4efc4\"),\n",
				"    (\"Direct Mail\",    \"5d79d9d1-35ec-407b-bb7c-c108dc463f00\"),\n",
				"    (\"Phone Call\",     \"ce8121cb-ad89-4dd6-8dd2-c6897c99bf57\"),\n",
				"]\n",
				"\n",
				"# Create DataFrame with schema and timestamps\n",
				"df_channel = spark.createDataFrame(\n",
				"    [Row(ChannelId=id, Name=name) for name, id in channel_names]\n",
				").withColumn(\"CreatedDate\", current_timestamp()) \\\n",
				" .withColumn(\"ModifiedDate\", current_timestamp())\n",
				"\n",
				"# Register temp view and execute MERGE\n",
				"df_channel.createOrReplaceTempView(\"default_channel\")\n",
				"\n",
				"# MERGE values\n",
				"channel_table_name = get_full_table_name(silver_lakehouse_name, \"Channel\")\n",
				"spark.sql(f\"\"\"\n",
				"    MERGE INTO {channel_table_name} AS target\n",
				"    USING default_channel AS source\n",
				"      ON target.Name = source.Name\n",
				"    WHEN NOT MATCHED THEN\n",
				"      INSERT (\n",
				"        ChannelId,\n",
				"        CreatedDate,\n",
				"        ModifiedDate,\n",
				"        Name\n",
				"      )\n",
				"      VALUES (\n",
				"        source.ChannelId,\n",
				"        source.CreatedDate,\n",
				"        source.ModifiedDate,\n",
				"        source.Name\n",
				"      )\n",
				"\"\"\")\n",
				"\n",
				"# Clean up temporary view\n",
				"spark.sql(\"DROP VIEW IF EXISTS default_channel\")\n",
				"\n",
				"logging.info(\"✅ Default Channels created\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "5abe8b48-ace2-462b-82b2-12f6b1687b12",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"# Default segmentation"
			]
		},
		{
			"cell_type": "markdown",
			"id": "d39a4d0c-4096-4616-a779-c2cca4f2a853",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Constituent Segment Type"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e20ed505-fbe8-4a1c-9503-a031afaaa7cc",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import Row\n",
				"from pyspark.sql.functions import current_timestamp\n",
				"import logging\n",
				"\n",
				"# ------------------------------------------------------------\n",
				"# ConstituentSegmentType Data\n",
				"# ------------------------------------------------------------\n",
				"segment_type_data = [\n",
				"    (\"000550c9-e37e-4750-a7b3-0720bfdd8755\", \"Gift Recurrance\"),\n",
				"    (\"64502acd-04d7-43ce-9fc2-8fe469bbe80b\", \"Lifetime Giving Range\"),\n",
				"    (\"89da35f8-d031-491b-be2b-00e42a2a58f5\", \"Gift Significance\"),\n",
				"    (\"9ea4b1dd-08cb-42b1-ad90-ab452f21b11f\", \"Age Range\")\n",
				"]\n",
				"\n",
				"# Create DataFrame with schema and timestamps\n",
				"df_segment_type = spark.createDataFrame(\n",
				"    [Row(ConstituentSegmentTypeId=id, Name=name) for id, name in segment_type_data]\n",
				").withColumn(\"CreatedDate\", current_timestamp()) \\\n",
				" .withColumn(\"ModifiedDate\", current_timestamp())\n",
				"\n",
				"# Register temp view and execute MERGE\n",
				"df_segment_type.createOrReplaceTempView(\"default_segment_types\")\n",
				"\n",
				"# MERGE values\n",
				"segment_type_table_name = get_full_table_name(silver_lakehouse_name, \"ConstituentSegmentType\")\n",
				"spark.sql(f\"\"\"\n",
				"    MERGE INTO {segment_type_table_name} AS target\n",
				"    USING default_segment_types AS source\n",
				"      ON target.Name = source.Name\n",
				"    WHEN NOT MATCHED THEN\n",
				"      INSERT (\n",
				"        ConstituentSegmentTypeId,\n",
				"        CreatedDate,\n",
				"        ModifiedDate,\n",
				"        Name\n",
				"      )\n",
				"      VALUES (\n",
				"        source.ConstituentSegmentTypeId,\n",
				"        source.CreatedDate,\n",
				"        source.ModifiedDate,\n",
				"        source.Name\n",
				"      )\n",
				"\"\"\")\n",
				"\n",
				"# Clean up temporary view\n",
				"spark.sql(\"DROP VIEW IF EXISTS default_segment_types\")\n",
				"\n",
				"logging.info(\"✅ Default ConstituentSegmentTypes created\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "4ae32d43-4ad1-4f39-b5c4-2bc836021a11",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Constituent Segment"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "b4f78582-4b9e-42e2-818a-5d17a55d4c98",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import Row\n",
				"from pyspark.sql.functions import current_timestamp\n",
				"import logging\n",
				"\n",
				"# ------------------------------------------------------------\n",
				"# ConstituentSegment Data\n",
				"# ------------------------------------------------------------\n",
				"\n",
				"gift_recurrence_segments = [\n",
				"    (\"f963f3d1-92a2-48f5-8000-a6a73687025f\", \"Prospect\", 1),\n",
				"    (\"f61288ed-b087-40cf-a9b9-06714a664f23\", \"New Donor\", 2),\n",
				"    (\"dc43a4c9-5942-4a86-9f1a-4ab7cec6f7c8\", \"LYBNT FY\", 3),\n",
				"    (\"2de18bf7-a352-4540-a238-fdfe97177a11\", \"LYBNT CY\", 4),\n",
				"    (\"2c87a98f-f72a-42b2-a2df-8c77741f0181\", \"LYBNT T12M\", 5),\n",
				"    (\"f9eb97e9-9210-435a-ad2d-82c771907102\", \"Active\", 6),\n",
				"    (\"810eb5c3-a898-4bfe-bc2d-67e7021deef9\", \"Recurring Donor\", 7),\n",
				"    (\"2b1280dc-574b-40b0-ad10-dc2b8e9cc6aa\", \"Lapsed Donor\", 8)\n",
				"]\n",
				"\n",
				"lifetime_giving_range_segments = [\n",
				"    (\"6584d841-17c2-4720-853d-719d5bfffea2\", \"<$250\", 101),\n",
				"    (\"87e6e616-0e60-495d-af24-0ca869c51f78\", \"$250–$999\", 102),\n",
				"    (\"122aa8f6-1081-47d7-bd1b-7241aa5784d7\", \"$1,000–$4,999\", 103),\n",
				"    (\"8df6054a-1c8f-4db0-8208-742ca294bcd1\", \"$5,000–$9,999\", 104),\n",
				"    (\"e6921693-e1ce-404e-9572-bd3316acd9a0\", \"$10,000–$24,999\", 105),\n",
				"    (\"ef2e3c01-63e6-4fc7-b85d-bbe56cd4efeb\", \"$25,000–$49,999\", 106),\n",
				"    (\"dcb11147-29de-46c6-a242-01f778cb9327\", \"$50,000–$99,999\", 107),\n",
				"    (\"9dee3abd-3fc7-4f34-9eef-6f370f9c7e73\", \"$100,000–$499,999\", 108),\n",
				"    (\"41c3481e-3acf-4861-a6bc-469eb94b5c39\", \"$500,000–$999,999\", 109),\n",
				"    (\"a061b8b9-2aee-484a-92ba-0cef6f153e77\", \"$1,000,000+\", 110)\n",
				"]\n",
				"\n",
				"gift_significance_segments = []\n",
				"\n",
				"age_range_segments = [\n",
				"    (\"54da67f7-3db1-4337-8051-9241467f93ef\", \"<18\", 301),\n",
				"    (\"69401d7c-9505-4c60-8879-3ca11d9dd482\", \"18-24\", 302),\n",
				"    (\"5960fba8-0b78-420f-8730-e5c78a8d9c51\", \"25-34\", 303),\n",
				"    (\"c806d565-6390-4c88-8c0e-5396dff48794\", \"35-44\", 304),\n",
				"    (\"4bc89ab7-c3bc-4063-995b-ca25cf3819d4\", \"45-54\", 305),\n",
				"    (\"4c372788-6417-42a1-b079-0aed5dc05f4a\", \"55-64\", 306),\n",
				"    (\"7014a1e8-07cb-4b94-867a-d156b0a14938\", \"65-74\", 307),\n",
				"    (\"3a7ee966-bda0-46be-a4cd-5c109d548b84\", \">75\", 308),\n",
				"    (\"6d421a90-b50a-4dac-b230-2dbebb4e3623\", \"Unclassified\", None)\n",
				"]\n",
				"\n",
				"# Define segment types and their corresponding segments\n",
				"segment_types_data = {\n",
				"    \"Gift Recurrance\": gift_recurrence_segments,\n",
				"    \"Lifetime Giving Range\": lifetime_giving_range_segments,\n",
				"    \"Gift Significance\": gift_significance_segments,\n",
				"    \"Age Range\": age_range_segments\n",
				"}\n",
				"\n",
				"# Create segment data with type names more efficiently\n",
				"segment_data = [\n",
				"    Row(\n",
				"        ConstituentSegmentId=segment_id,\n",
				"        ConstituentSegmentTypeName=type_name,\n",
				"        Name=segment_name,\n",
				"        Order=sort_order\n",
				"    )\n",
				"    for type_name, segments in segment_types_data.items()\n",
				"    for segment in segments\n",
				"    for segment_id, segment_name, sort_order in [segment if len(segment) == 3 else (*segment, None)]\n",
				"]\n",
				"\n",
				"# Create DataFrame with timestamps in one operation\n",
				"df_segment = spark.createDataFrame(segment_data) \\\n",
				"    .withColumn(\"CreatedDate\", current_timestamp()) \\\n",
				"    .withColumn(\"ModifiedDate\", current_timestamp())\n",
				"\n",
				"# Register temp view and join in one operation\n",
				"df_segment.createOrReplaceTempView(\"default_segments_raw\")\n",
				"\n",
				"segment_type_table_name = get_full_table_name(silver_lakehouse_name, \"ConstituentSegmentType\")\n",
				"spark.sql(f\"\"\"\n",
				"    CREATE OR REPLACE TEMPORARY VIEW default_segments AS\n",
				"    SELECT \n",
				"        s.ConstituentSegmentId,\n",
				"        st.ConstituentSegmentTypeId,\n",
				"        s.CreatedDate,\n",
				"        s.ModifiedDate,\n",
				"        s.Name,\n",
				"        s.Order\n",
				"    FROM default_segments_raw s\n",
				"    INNER JOIN {segment_type_table_name} st \n",
				"        ON s.ConstituentSegmentTypeName = st.Name\n",
				"\"\"\")\n",
				"\n",
				"# MERGE values\n",
				"full_table_name = get_full_table_name(silver_lakehouse_name, \"ConstituentSegment\")\n",
				"spark.sql(f\"\"\"\n",
				"    MERGE INTO {full_table_name} AS target\n",
				"    USING default_segments AS source\n",
				"      ON target.Name = source.Name \n",
				"         AND target.ConstituentSegmentTypeId = source.ConstituentSegmentTypeId\n",
				"    WHEN NOT MATCHED THEN\n",
				"      INSERT (\n",
				"        ConstituentSegmentId,\n",
				"        ConstituentSegmentTypeId,\n",
				"        CreatedDate,\n",
				"        ModifiedDate,\n",
				"        Name,\n",
				"        Order\n",
				"      )\n",
				"      VALUES (\n",
				"        source.ConstituentSegmentId,\n",
				"        source.ConstituentSegmentTypeId,\n",
				"        source.CreatedDate,\n",
				"        source.ModifiedDate,\n",
				"        source.Name,\n",
				"        source.Order\n",
				"      )\n",
				"\"\"\")\n",
				"\n",
				"# Clean up temporary views\n",
				"spark.sql(\"DROP VIEW IF EXISTS default_segments\")\n",
				"spark.sql(\"DROP VIEW IF EXISTS default_segments_raw\")\n",
				"\n",
				"logging.info(\"✅ Default ConstituentSegments created\")"
			]
		}
	],
	"metadata": {
		"dependencies": {
			"lakehouse": {
				"default_lakehouse": "{SILVER_LAKEHOUSE_ID}",
				"default_lakehouse_name": "{SILVER_LAKEHOUSE_NAME}",
				"known_lakehouses": [
					{
						"id": "{SILVER_LAKEHOUSE_ID}"
					}
				]
			}
		},
		"kernel_info": {
			"name": "synapse_pyspark"
		},
		"kernelspec": {
			"display_name": "synapse_pyspark",
			"name": "synapse_pyspark"
		},
		"microsoft": {
			"language": "python",
			"language_group": "synapse_pyspark",
			"ms_spell_check": {
				"ms_spell_check_language": "en"
			}
		},
		"nteract": {
			"version": "nteract-front-end@1.0.0"
		},
		"spark_compute": {
			"compute_id": "/trident/default",
			"session_options": {
				"conf": {
					"spark.synapse.nbs.session.timeout": "1200000"
				}
			}
		},
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}