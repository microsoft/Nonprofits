{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab04eed6-8de2-4baa-aae2-5cbfa7fecf7d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dfffc0-854b-4d62-9ae4-beee6342d9be",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "%run <Fundraising_SalesforceNPSP_Config>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19a4cd-9eee-4f30-b79c-19a62f077ce9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f28cbf-20c0-4054-b543-fad850e8de27",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd1cff-0ad3-4924-80e2-8a2b1d393336",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "merge_sql = f\"\"\"\n",
    "MERGE INTO {silver_lakehouse_name}.Source AS target\n",
    "USING (\n",
    "    SELECT \n",
    "        '{source_id}' AS SourceId,\n",
    "        '{source_name}' AS Name,\n",
    "        current_timestamp() AS CreatedDate,\n",
    "        current_timestamp() AS ModifiedDate\n",
    ") AS source\n",
    "ON target.SourceId = source.SourceId\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    SourceId, CreatedDate, ModifiedDate, Name\n",
    ") VALUES (\n",
    "    source.SourceId, source.CreatedDate, source.ModifiedDate, source.Name\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(merge_sql)\n",
    "row = result.collect()[0]\n",
    "\n",
    "logging.info(f\"✅ Rows processed {row['num_affected_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0a6fa-2301-4361-8857-04df2b470aa9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fc369-ff97-4393-a813-ebeb11e5f4c3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def EnrichCountry(df):\n",
    "    return (\n",
    "        df\n",
    "        .select(col(\"npsp__MailingCountry__c\").alias(\"Name\"))\n",
    "        .dropna(subset=[\"Name\"])\n",
    "        .distinct()\n",
    "        .withColumn(\"CountryId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"CreatedDate\", current_timestamp())\n",
    "        .withColumn(\"ModifiedDate\", current_timestamp())\n",
    "        .withColumn(\"CountryCode\", lit(None).cast(StringType()))  \n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            \"CountryId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"CountryCode\",\n",
    "            \"Name\",\n",
    "            \"SourceId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "countryTable = CdfTable(\n",
    "    source_table_name=\"Address\",\n",
    "    source_primary_key=\"npsp__MailingCountry__c\",\n",
    "    target_table_name=\"Country\",\n",
    "    columns=[\"npsp__MailingCountry__c\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Country AS target\n",
    "    USING latestSnapshot_Address AS source\n",
    "    ON target.Name = source.Name\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        CountryId, CreatedDate, ModifiedDate, CountryCode, Name, SourceId\n",
    "    ) VALUES (\n",
    "        source.CountryId, source.CreatedDate, source.ModifiedDate, source.CountryCode, source.Name, source.SourceId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichCountry\n",
    ")\n",
    "\n",
    "ProcessCdfTable(countryTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a29bc-45c9-4254-83db-b97518f5230c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: CampaignType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852e3b4-fe2a-4723-b2fe-af97dffb8ce1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichCampaignType(df):\n",
    "    new_df =  (\n",
    "        df\n",
    "        .filter(col(\"SobjectType\") == \"Campaign\")  # Filter only Campaign record types\n",
    "        .dropna(subset=[\"Name\"])  # Ensure valid Name\n",
    "        .select(\n",
    "            col(\"Name\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceSystemId\"]) \n",
    "        .withColumn(\"CampaignTypeId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            \"CampaignTypeId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"Name\",\n",
    "            \"SourceId\",\n",
    "            \"SourceSystemId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "campaignTypeTable = CdfTable(\n",
    "    source_table_name=\"RecordType\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"CampaignType\",\n",
    "    columns=[\"Id\", \"Name\", \"CreatedDate\", \"LastModifiedDate\", \"SobjectType\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.CampaignType AS target\n",
    "    USING latestSnapshot_RecordType AS source\n",
    "    ON target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.Name = source.Name\n",
    "\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        CampaignTypeId, CreatedDate, ModifiedDate, Name, SourceId, SourceSystemId\n",
    "    ) VALUES (\n",
    "        source.CampaignTypeId, source.CreatedDate, source.ModifiedDate,\n",
    "        source.Name, source.SourceId, source.SourceSystemId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichCampaignType\n",
    ")\n",
    "\n",
    "ProcessCdfTable(campaignTypeTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e75b9d-fe86-4533-86df-be3a06e89dba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a09c35-983c-40f5-8bf7-1cd0a11fcaa1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def EnrichChannel(df):\n",
    "    new_df =  (\n",
    "        df\n",
    "        .select(col(\"Type\").alias(\"Name\"))\n",
    "        .distinct()\n",
    "        .dropna(subset=[\"Name\"])\n",
    "        .withColumn(\"ChannelId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"CreatedDate\", current_timestamp())\n",
    "        .withColumn(\"ModifiedDate\", current_timestamp())\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            \"ChannelId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"Name\",\n",
    "            \"SourceId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "channelTable = CdfTable(\n",
    "    source_table_name=\"Campaign\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Channel\",\n",
    "    columns=[\"Type\", \"Id\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Channel AS target\n",
    "    USING latestSnapshot_Campaign AS source\n",
    "    ON target.Name = source.Name\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        ChannelId, CreatedDate, ModifiedDate, Name, SourceId\n",
    "    ) VALUES (\n",
    "        source.ChannelId, source.CreatedDate, source.ModifiedDate, source.Name, source.SourceId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichChannel\n",
    ")\n",
    "\n",
    "ProcessCdfTable(channelTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025af0f-c9af-46a6-8689-3ba2a9ae64dd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: ConstituentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e7398-c84f-4085-94f0-3e75279eb3ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def EnrichConstituentType(df_account):\n",
    "    df_types = (\n",
    "        df_account\n",
    "        .select(col(\"Type\").alias(\"Name\"))\n",
    "        .distinct()\n",
    "        .dropna(subset=[\"Name\"])\n",
    "    )\n",
    "\n",
    "    # Check if there is at least one contact\n",
    "    df_contact = get_bronze_table(\"Contact\")\n",
    "    if not df_contact.limit(1).isEmpty():\n",
    "        # Add 'Individual' as a synthetic row\n",
    "        df_individual = spark.createDataFrame([Row(Name=\"Individual\")])\n",
    "        df_types = df_types.unionByName(df_individual).dropDuplicates([\"Name\"])\n",
    "\n",
    "    new_df = (\n",
    "        df_types\n",
    "        .withColumn(\"ConstituentTypeId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"CreatedDate\", current_timestamp())\n",
    "        .withColumn(\"ModifiedDate\", current_timestamp())\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            \"ConstituentTypeId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"Name\",\n",
    "            \"SourceId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "constituentTypeTable = CdfTable(\n",
    "    source_table_name=\"Account\",\n",
    "    source_primary_key=\"Type\",\n",
    "    target_table_name=\"ConstituentType\",\n",
    "    columns=[\"Type\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.ConstituentType AS target\n",
    "    USING latestSnapshot_Account AS source\n",
    "    ON target.Name = source.Name\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        ConstituentTypeId, CreatedDate, ModifiedDate, Name, SourceId\n",
    "    ) VALUES (\n",
    "        source.ConstituentTypeId, source.CreatedDate, source.ModifiedDate, source.Name, source.SourceId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichConstituentType\n",
    ")\n",
    "\n",
    "ProcessCdfTable(constituentTypeTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4230a5-f359-4e24-8cc2-0eb933fdf1e0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: EventType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac6cd4-5074-42b2-ba7e-e551b3754e57",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def EnrichEventType(df):\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(col(\"Subject\").alias(\"Name\"))\n",
    "        .distinct()\n",
    "        .dropna(subset=[\"Name\"])\n",
    "        .withColumn(\"EventTypeId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"CreatedDate\", current_timestamp())\n",
    "        .withColumn(\"ModifiedDate\", current_timestamp())\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            \"EventTypeId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"Name\",\n",
    "            \"SourceId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "eventTypeTable = CdfTable(\n",
    "    source_table_name=\"Event\",\n",
    "    source_primary_key=\"Subject\",\n",
    "    target_table_name=\"EventType\",\n",
    "    columns=[\"Subject\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.EventType AS target\n",
    "    USING latestSnapshot_Event AS source\n",
    "    ON target.Name = source.Name and target.SourceId = source.SourceId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        EventTypeId, CreatedDate, ModifiedDate, Name, SourceId\n",
    "    ) VALUES (\n",
    "        source.EventTypeId, source.CreatedDate, source.ModifiedDate, source.Name, source.SourceId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichEventType\n",
    ")\n",
    "\n",
    "ProcessCdfTable(eventTypeTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a2292-22e0-4363-a5f4-a7e7f18113a7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: OpportunityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52d8fb-5fea-4210-a716-083bd760bb58",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def EnrichOpportunityType(df):\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(col(\"Type\").alias(\"Name\"))\n",
    "        .distinct()\n",
    "        .dropna(subset=[\"Name\"])\n",
    "        .withColumn(\"OpportunityTypeId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"CreatedDate\", current_timestamp())\n",
    "        .withColumn(\"ModifiedDate\", current_timestamp())\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            \"OpportunityTypeId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"Name\",\n",
    "            \"SourceId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "opportunityTypeTable = CdfTable(\n",
    "    source_table_name=\"Opportunity\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"OpportunityType\",\n",
    "    columns=[\"Type\", \"Id\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.OpportunityType AS target\n",
    "    USING latestSnapshot_Opportunity AS source\n",
    "    ON target.Name = source.Name\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        OpportunityTypeId, CreatedDate, ModifiedDate, Name, SourceId\n",
    "    ) VALUES (\n",
    "        source.OpportunityTypeId, source.CreatedDate, source.ModifiedDate, source.Name, source.SourceId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichOpportunityType\n",
    ")\n",
    "\n",
    "ProcessCdfTable(opportunityTypeTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e0724-49f4-4d9e-b10b-d7e3dbc36104",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09fc70-5e32-4718-a1ea-4d66cf2f4aea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def EnrichCampaign(df):\n",
    "    # Read CampaignType table\n",
    "    campaign_type_df = get_silver_table(\"CampaignType\").select(\n",
    "        \"CampaignTypeId\",\n",
    "        col(\"SourceSystemId\").alias(\"CampaignTypeSourceSystemId\")\n",
    "    )\n",
    "\n",
    "    new_df = (\n",
    "        df\n",
    "        .join(campaign_type_df, df[\"RecordTypeId\"] == campaign_type_df[\"CampaignTypeSourceSystemId\"], how=\"left\")\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .withColumn(\"CreatedDate\", current_timestamp()) # Optional override\n",
    "        .withColumn(\"ModifiedDate\", current_timestamp()) # Optional override\n",
    "        .withColumn(\"CampaignId\",  expr(\"uuid()\"))\n",
    "        .select(\n",
    "            \"CampaignId\",\n",
    "            \"CampaignTypeId\",\n",
    "            \"ActualCost\",\n",
    "            \"CreatedDate\",\n",
    "            \"EndDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"Name\",\n",
    "            \"SourceId\",\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            \"StartDate\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "campaignTable = CdfTable(\n",
    "    source_table_name=\"Campaign\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Campaign\",\n",
    "    columns=[\n",
    "        \"Id\", \"RecordTypeId\", \"ActualCost\", \"CreatedDate\", \"EndDate\",\n",
    "        \"Name\", \"StartDate\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Campaign AS target\n",
    "    USING latestSnapshot_Campaign AS source\n",
    "    ON target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.Cost = source.ActualCost,\n",
    "        target.EndDate = source.EndDate,\n",
    "        target.Name = source.Name,\n",
    "        target.StartDate = source.StartDate\n",
    "\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        CampaignId, CampaignTypeId, Cost, CreatedDate, EndDate, ModifiedDate, Name, \n",
    "        SourceId, SourceSystemId, StartDate\n",
    "    ) VALUES (\n",
    "        source.CampaignId, source.CampaignTypeId, source.ActualCost, source.CreatedDate, source.EndDate, source.ModifiedDate, source.Name, \n",
    "        source.SourceId, source.SourceSystemId, source.StartDate\n",
    "        )\n",
    "    \n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichCampaign,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(campaignTable, source_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcaf4c-8569-4353-b00f-78843b361e99",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e522a0-c2cf-42a8-8953-597e90856fc4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichAddress(df_addr_src):\n",
    "    df_union = (\n",
    "        df_addr_src\n",
    "          .select(\n",
    "              lit(None).cast(\"string\").alias(\"AddressId\"),\n",
    "              col(\"npsp__MailingCity__c\").alias(\"MailingCity\"),\n",
    "              col(\"npsp__MailingState__c\").alias(\"MailingState\"),\n",
    "              col(\"npsp__MailingPostalCode__c\").alias(\"MailingPostalCode\"),\n",
    "              col(\"npsp__MailingCountry__c\").alias(\"MailingCountry\"),\n",
    "              \"CreatedDate\",\n",
    "              \"LastModifiedDate\",\n",
    "              col(\"Id\").alias(\"SourceSystemId\"),\n",
    "              lit(source_id).alias(\"SourceId\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    # Assign AddressId where missing\n",
    "    df_union = df_union.withColumn(\"AddressId\", expr(\"coalesce(AddressId, uuid())\"))\n",
    "\n",
    "    df_country = get_silver_table(\"Country\").select(\"CountryId\", \"Name\")\n",
    "    df_enriched = df_union.join(\n",
    "        df_country,\n",
    "        df_union[\"MailingCountry\"] == df_country[\"Name\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    new_df =  (\n",
    "        df_enriched.select(\n",
    "            \"AddressId\",\n",
    "            col(\"MailingCity\").alias(\"City\"),\n",
    "            \"CountryId\",\n",
    "            \"CreatedDate\",\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            \"SourceId\",\n",
    "            \"SourceSystemId\",\n",
    "            col(\"MailingState\").alias(\"State\"),\n",
    "            col(\"MailingPostalCode\").alias(\"ZipCode\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "addressTable = CdfTable(\n",
    "    source_table_name=\"Address\",              \n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Address\",\n",
    "    columns=[\n",
    "        \"Id\", \"npsp__MailingCity__c\", \"npsp__MailingState__c\",\n",
    "        \"npsp__MailingPostalCode__c\", \"npsp__MailingCountry__c\",\n",
    "        \"CreatedDate\", \"LastModifiedDate\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Address AS tgt\n",
    "    USING latestSnapshot_Address AS src\n",
    "    ON  tgt.SourceId      = src.SourceId\n",
    "    AND tgt.SourceSystemId = src.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "         tgt.ModifiedDate = src.ModifiedDate,\n",
    "         tgt.City         = src.City,\n",
    "         tgt.State        = src.State,\n",
    "         tgt.ZipCode      = src.ZipCode,\n",
    "         tgt.CountryId    = src.CountryId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "         AddressId, City, CountryId, CreatedDate, ModifiedDate,\n",
    "         SourceId, SourceSystemId, State, ZipCode\n",
    "    ) VALUES (\n",
    "         src.AddressId, src.City, src.CountryId, src.CreatedDate,\n",
    "         src.ModifiedDate, src.SourceId, src.SourceSystemId, src.State, src.ZipCode\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichAddress,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(addressTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c8b2e-42aa-42fc-aa4e-15d78de0f091",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: OpportunityStage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8b397-b0b9-41c2-ae61-2451f2249ff2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def EnrichOpportunityStage(df):\n",
    "    new_df = (\n",
    "        df\n",
    "        .dropna(subset=[\"MasterLabel\"])\n",
    "        .select(\"Id\", \"MasterLabel\", \"CreatedDate\", \"LastModifiedDate\")\n",
    "        .withColumn(\"OpportunityStageId\", expr(\"uuid()\"))\n",
    "        .withColumn(\"ModifiedDate\", col(\"LastModifiedDate\"))\n",
    "        .withColumn(\"SourceId\", lit(source_id))\n",
    "        .select(\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            \"OpportunityStageId\",\n",
    "            \"CreatedDate\",\n",
    "            \"ModifiedDate\",\n",
    "            \"SourceId\",\n",
    "            col(\"MasterLabel\").alias(\"Name\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "opportunityStageTable = CdfTable(\n",
    "    source_table_name=\"OpportunityStage\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"OpportunityStage\",\n",
    "    columns=[\"Id\", \"MasterLabel\", \"CreatedDate\", \"LastModifiedDate\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.OpportunityStage AS target\n",
    "    USING latestSnapshot_OpportunityStage AS source\n",
    "    ON target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        OpportunityStageId, Name, CreatedDate, ModifiedDate, SourceId, SourceSystemId\n",
    "    ) VALUES (\n",
    "        source.OpportunityStageId, source.Name, source.CreatedDate, source.ModifiedDate,\n",
    "        source.SourceId, source.SourceSystemId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichOpportunityStage\n",
    ")\n",
    "\n",
    "ProcessCdfTable(opportunityStageTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91f8ed-f1d4-4d8c-a695-beaf3f044085",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transfrom: CampaignChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2da0be-6c78-443d-8090-6c546727c8f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, lit, monotonically_increasing_id\n",
    "\n",
    "def EnrichCampaignChannel(df):\n",
    "    channel_df = get_silver_table(\"Channel\").select(col(\"Name\").alias(\"ChannelName\"), \"ChannelId\")\n",
    "    silver_campaign_df = get_silver_table(\"Campaign\").select(\"SourceSystemId\", \"CampaignId\")\n",
    "\n",
    "    new_df =  (\n",
    "        df\n",
    "        .join(\n",
    "            silver_campaign_df,\n",
    "            df[\"Id\"] == silver_campaign_df[\"SourceSystemId\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .join(\n",
    "            channel_df,\n",
    "            df[\"Type\"] == channel_df[\"ChannelName\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .withColumn(\"CampaignChannelId\", expr(\"uuid()\"))\n",
    "        .select(\n",
    "            \"ChannelId\",\n",
    "            \"CampaignChannelId\",\n",
    "            \"CampaignId\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "campaignChannelTable = CdfTable(\n",
    "    source_table_name=\"Campaign\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"CampaignChannel\",\n",
    "    columns=[\n",
    "        \"Id\", \"Type\"  # Type is used to match Channel.Name\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.CampaignChannel AS target\n",
    "    USING latestSnapshot_Campaign AS source\n",
    "    ON target.CampaignId = source.CampaignId AND target.ChannelId = source.ChannelId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        CampaignChannelId, ChannelId, CampaignId\n",
    "    ) VALUES (\n",
    "        source.CampaignChannelId, source.ChannelId, source.CampaignId\n",
    "    )\n",
    "    WHEN NOT MATCHED BY SOURCE THEN DELETE\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichCampaignChannel\n",
    ")\n",
    "\n",
    "ProcessCdfTable(campaignChannelTable, source_name)\n",
    "log_merge_metrics(\n",
    "    f\"{silver_lakehouse_name}.CampaignChannel\",\n",
    "    \"CampaignChannel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe45bcd-dcce-48b5-8b8b-60862b6ebf3f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: EmailEngagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef536b75-366a-4909-aaa8-b98996d884be",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichEmailEngagement(df):\n",
    "    # Silver Campaign lookup (SourceSystemId to CampaignId)\n",
    "    df_campaign = get_silver_table(\"Campaign\").select(\n",
    "        col(\"CampaignId\"), col(\"SourceSystemId\").alias(\"CampaignSourceSystemId\")\n",
    "    )\n",
    "\n",
    "    # ChannelId by resolving ChannelName = 'Email' \n",
    "    df_channel = get_silver_table(\"Channel\").filter(col(\"Name\") == \"Email\") \\\n",
    "        .select(col(\"ChannelId\").alias(\"EmailChannelId\")).limit(1)\n",
    "\n",
    "    # Join EmailMessage.RelatedToId → Campaign.SourceSystemId\n",
    "    df = df.join(\n",
    "        df_campaign,\n",
    "        df[\"RelatedToId\"] == df_campaign[\"CampaignSourceSystemId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # join to add EmailChannelId\n",
    "    df = df.join(df_channel, how=\"left\")\n",
    "\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(\n",
    "            col(\"CampaignId\"),\n",
    "            col(\"EmailChannelId\").alias(\"ChannelId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            expr(\"uuid()\").alias(\"EmailEngagementId\"),\n",
    "            col(\"Id\").alias(\"EmailId\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            col(\"MessageDate\").alias(\"SendDate\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            col(\"Subject\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "emailEngagementTable = CdfTable(\n",
    "    source_table_name=\"EmailMessage\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"EmailEngagement\",\n",
    "    columns=[\n",
    "        \"Id\", \"CreatedDate\", \"LastModifiedDate\", \"MessageDate\", \"Subject\", \"RelatedToId\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.EmailEngagement AS target\n",
    "    USING latestSnapshot_EmailMessage AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.CampaignId = source.CampaignId,\n",
    "        target.ChannelId = source.ChannelId,\n",
    "        target.Subject = source.Subject,\n",
    "        target.SendDate = source.SendDate,\n",
    "        target.EmailId = source.EmailId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        CampaignId, ChannelId, CreatedDate, EmailEngagementId, EmailId, ModifiedDate,\n",
    "        SendDate, SourceId, SourceSystemId, Subject\n",
    "    ) VALUES (\n",
    "        source.CampaignId, source.ChannelId, source.CreatedDate, source.EmailEngagementId, source.EmailId, source.ModifiedDate,\n",
    "        source.SendDate, source.SourceId, source.SourceSystemId, source.Subject\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichEmailEngagement,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(emailEngagementTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8a38a-a29f-4327-9427-5ff6ec40ae9f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69455177-89a6-4f4c-8ff7-229f12de5e70",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichAccount(df):\n",
    "    # Bronze Address: for direct match\n",
    "    df_addr_bronze = get_bronze_table(\"Address\").select(\n",
    "        col(\"Id\").alias(\"AddressSourceSystemId_bronze\"),\n",
    "        col(\"npsp__Household_Account__c\").alias(\"HouseholdAccountId_bronze\")\n",
    "    )\n",
    "\n",
    "    # Silver Address: for translation to AddressId\n",
    "    df_addr_silver = get_silver_table(\"Address\").select(\n",
    "        \"AddressId\", \"SourceSystemId\", \"City\", \"State\", \"ZipCode\", \"CountryId\", \"SourceId\"\n",
    "    )\n",
    "\n",
    "    # Silver Country for country name (for fallback matching)\n",
    "    df_country = get_silver_table(\"Country\").select(\"CountryId\", \"Name\")\n",
    "\n",
    "    # -------------------- Direct Match --------------------\n",
    "    df_direct = (\n",
    "        df\n",
    "        .join(\n",
    "            df_addr_bronze,\n",
    "            df[\"Id\"] == df_addr_bronze[\"HouseholdAccountId_bronze\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .join(\n",
    "            df_addr_silver.withColumnRenamed(\"AddressId\", \"DirectAddressId\"),\n",
    "            df_addr_bronze[\"AddressSourceSystemId_bronze\"] == col(\"SourceSystemId\"),\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -------------------- Fallback Match --------------------\n",
    "    df_addr_silver_country = (\n",
    "        df_addr_silver\n",
    "        .join(df_country, df_addr_silver[\"CountryId\"] == df_country[\"CountryId\"], \"left\")\n",
    "        .withColumnRenamed(\"AddressId\", \"FallbackAddressId\")\n",
    "        .withColumnRenamed(\"City\", \"fb_city\")\n",
    "        .withColumnRenamed(\"State\", \"fb_state\")\n",
    "        .withColumnRenamed(\"ZipCode\", \"fb_zip\")\n",
    "        .withColumnRenamed(\"Name\", \"fb_country_name\")\n",
    "    )\n",
    "\n",
    "    df_fallback = (\n",
    "        df\n",
    "        .join(\n",
    "            df_addr_silver_country,\n",
    "            (df[\"BillingCity\"] == col(\"fb_city\")) &\n",
    "            (df[\"BillingState\"] == col(\"fb_state\")) &\n",
    "            (df[\"BillingPostalCode\"] == col(\"fb_zip\")) &\n",
    "            (df[\"BillingCountry\"] == col(\"fb_country_name\")) &\n",
    "            (lit(source_id) == df_addr_silver_country[\"SourceId\"]),\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    from pyspark.sql.functions import coalesce\n",
    "\n",
    "    result = (\n",
    "        df_direct\n",
    "        .join(\n",
    "            df_fallback.select(col(\"Id\").alias(\"fallback_Id\"), col(\"FallbackAddressId\")),\n",
    "            df_direct[\"Id\"] == col(\"fallback_Id\"),\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .withColumn(\"AddressId\", coalesce(col(\"DirectAddressId\"), col(\"FallbackAddressId\")))\n",
    "    )\n",
    "\n",
    "    new_df = (\n",
    "        result.select(\n",
    "            expr(\"uuid()\").alias(\"AccountId\"),\n",
    "            col(\"AddressId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"npsp__Matching_Gift_Email__c\").alias(\"Email\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            col(\"Name\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "accountTable = CdfTable(\n",
    "    source_table_name=\"Account\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Account\",\n",
    "    columns=[\n",
    "    \"Id\", \"Name\", \"CreatedDate\", \"LastModifiedDate\", \"npsp__Matching_Gift_Email__c\",\n",
    "    \"BillingCity\", \"BillingState\", \"BillingPostalCode\", \"BillingCountry\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Account AS target\n",
    "    USING latestSnapshot_Account AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.Email = source.Email,\n",
    "        target.AddressId = source.AddressId,\n",
    "        target.Name = source.Name\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        AccountId, AddressId, CreatedDate, Email, ModifiedDate,\n",
    "        Name, SourceId, SourceSystemId\n",
    "    ) VALUES (\n",
    "        source.AccountId, source.AddressId, source.CreatedDate, source.Email,\n",
    "        source.ModifiedDate, source.Name, source.SourceId, source.SourceSystemId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichAccount,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(accountTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415adae-6a85-486d-8a2d-4933dddb5988",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afaa0b-8f9c-4cd5-8b4f-e6f66d50932b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichContact(df):\n",
    "    df_contact = df.alias(\"c\")\n",
    "    df_address = get_silver_table(\"Address\") \\\n",
    "        .select(\"AddressId\", \"City\", \"State\", \"ZipCode\", \"SourceId\", \"SourceSystemId\") \\\n",
    "        .alias(\"a\")\n",
    "\n",
    "    # First try to match by npsp__Current_Address__c → Address.SourceSystemId\n",
    "    df_direct = df_contact.join(\n",
    "        df_address,\n",
    "        col(\"c.npsp__Current_Address__c\") == col(\"a.SourceSystemId\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # If not matched, fall back to address fields (City, State, Zip, SourceId)\n",
    "    df_fallback = df_contact.join(\n",
    "        df_address,\n",
    "        (col(\"c.MailingCity\") == col(\"a.City\")) &\n",
    "        (col(\"c.MailingState\") == col(\"a.State\")) &\n",
    "        (col(\"c.MailingPostalCode\") == col(\"a.ZipCode\")) &\n",
    "        (lit(source_id) == col(\"a.SourceId\")),\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Use AddressId from direct match if available, otherwise from fallback\n",
    "    from pyspark.sql.functions import coalesce\n",
    "\n",
    "    df_joined = df_direct \\\n",
    "        .withColumn(\"FallbackAddressId\", df_fallback[\"a.AddressId\"]) \\\n",
    "        .withColumn(\"AddressId\", coalesce(df_direct[\"a.AddressId\"], col(\"FallbackAddressId\")))\n",
    "\n",
    "    new_df = df_joined.select(\n",
    "        expr(\"uuid()\").alias(\"ContactId\"),\n",
    "        col(\"AddressId\"),\n",
    "        col(\"c.CreatedDate\"),\n",
    "        col(\"c.LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "        col(\"c.Email\"),\n",
    "        col(\"c.FirstName\"),\n",
    "        col(\"c.LastName\"),\n",
    "        col(\"c.Id\").alias(\"SourceSystemId\"),\n",
    "        col(\"c.Birthdate\").alias(\"BirthDate\"),\n",
    "        lit(source_id).alias(\"SourceId\")\n",
    "    ).dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "contactTable = CdfTable(\n",
    "    source_table_name=\"Contact\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Contact\",\n",
    "    columns=[\n",
    "        \"Id\", \"FirstName\", \"LastName\", \"CreatedDate\", \"LastModifiedDate\", \n",
    "        \"MailingCity\", \"MailingState\", \"MailingPostalCode\", \"Email\", \"Birthdate\", \"npsp__Current_Address__c\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Contact AS target\n",
    "    USING latestSnapshot_Contact AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.Email        = source.Email,\n",
    "        target.FirstName    = source.FirstName,\n",
    "        target.LastName     = source.LastName,\n",
    "        target.AddressId    = source.AddressId,\n",
    "        target.BirthDate    = source.BirthDate\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        ContactId, AddressId, CreatedDate, ModifiedDate,\n",
    "        Email, FirstName, LastName, SourceSystemId, SourceId, BirthDate\n",
    "    ) VALUES (\n",
    "        source.ContactId, source.AddressId, source.CreatedDate, source.ModifiedDate,\n",
    "        source.Email, source.FirstName, source.LastName, source.SourceSystemId, source.SourceId, source.BirthDate\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichContact,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(contactTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c40fa-ee1d-46c1-a607-3a607cebf7c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Constituent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942545c-229d-44cb-b590-c63aee784be9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, expr, when\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as Ff\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 1.  LOAD SOURCE TABLES\n",
    "# ------------------------------------------------------------------#\n",
    "df_contact_silver = get_silver_table(\"Contact\") \\\n",
    "    .select(\"ContactId\", \"SourceId\")\n",
    "\n",
    "df_account_silver = get_silver_table(\"Account\") \\\n",
    "    .select(\"AccountId\", \"SourceSystemId\", \"SourceId\")\n",
    "\n",
    "df_account_bronze = get_bronze_table(\"Account\") \\\n",
    "    .select(col(\"Id\").alias(\"SourceSystemId\"), col(\"Type\"))\n",
    "\n",
    "df_const_type = get_silver_table(\"ConstituentType\") \\\n",
    "    .select(\"Name\", \"ConstituentTypeId\")\n",
    "\n",
    "type_map = {r[\"Name\"]: r[\"ConstituentTypeId\"] for r in df_const_type.collect()}\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 2.  CONTACT-BASED CONSTITUENTS \n",
    "# ------------------------------------------------------------------#\n",
    "individual_type_id = type_map[\"Individual\"]\n",
    "\n",
    "df_contact_constituents = (\n",
    "    df_contact_silver\n",
    "    .withColumn(\"ConstituentId\", expr(\"uuid()\"))\n",
    "    .withColumn(\"AccountId\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"ConstituentTypeId\", lit(individual_type_id))\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 3.  ACCOUNT-BASED CONSTITUENTS\n",
    "# ------------------------------------------------------------------#\n",
    "\n",
    "df_account_enriched = (\n",
    "    df_account_silver\n",
    "      .join(df_account_bronze, \"SourceSystemId\")            # adds Type\n",
    "      .join(df_const_type, df_account_bronze[\"Type\"] == df_const_type[\"Name\"], \"left\")  # adds ConstituentTypeId\n",
    ")\n",
    "\n",
    "df_account_constituents = (\n",
    "    df_account_enriched\n",
    "      .withColumn(\"ConstituentId\", expr(\"uuid()\"))\n",
    "      .withColumn(\"ContactId\", lit(None).cast(\"string\"))\n",
    "      .withColumn(\"ConstituentTypeId\", col(\"ConstituentTypeId\"))\n",
    "      .select(\"ConstituentId\", \"AccountId\", \"ContactId\", \"ConstituentTypeId\", \"SourceId\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 4.  UNION  & TEMP VIEW\n",
    "# ------------------------------------------------------------------#\n",
    "df_union = (\n",
    "    df_contact_constituents\n",
    "      .select(\"ConstituentId\", \"AccountId\", \"ContactId\", \"ConstituentTypeId\", \"SourceId\")\n",
    "      .unionByName(df_account_constituents)\n",
    ")\n",
    "\n",
    "window = Window.partitionBy(\"ContactId\", \"AccountId\").orderBy(F.col(\"ConstituentId\"))\n",
    "df_union = df_union.withColumn(\"row_number\", F.row_number().over(window)).filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "\n",
    "df_union.createOrReplaceTempView(\"staged_Constituent\")\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 5.  MERGE INTO SILVER.Constituent\n",
    "# ------------------------------------------------------------------#\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {silver_lakehouse_name}.Constituent AS tgt\n",
    "USING staged_Constituent AS src\n",
    "ON   tgt.ContactId  <=> src.ContactId             \n",
    "AND  tgt.AccountId  <=> src.AccountId\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "      tgt.ConstituentTypeId = src.ConstituentTypeId\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "      ConstituentId, AccountId, ContactId, ConstituentTypeId\n",
    ") VALUES (\n",
    "      src.ConstituentId, src.AccountId, src.ContactId, src.ConstituentTypeId\n",
    ")\n",
    "WHEN NOT MATCHED BY SOURCE THEN DELETE \n",
    "\"\"\")\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 6.  LOG\n",
    "# ------------------------------------------------------------------#\n",
    "\n",
    "log_merge_metrics(f\"{silver_lakehouse_name}.Constituent\", \"Constituent\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 7.  CLEAN UP TEMP VIEW\n",
    "# ------------------------------------------------------------------#\n",
    "spark.catalog.dropTempView(\"staged_Constituent\")\n",
    "\n",
    "logging.info(\"✅ Constituent table refreshed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbbea3-e15c-4158-901a-99a345c39ddf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: ConstituentEmailEngagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5648fd-79de-4cf2-90d0-682432ba9cc1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichConstituentEmailEngagement(df):\n",
    "    df = df.filter(\n",
    "        (col(\"RelationType\") == \"ToAddress\") &\n",
    "        (col(\"RelationObjectType\").isin(\"Account\", \"Contact\"))\n",
    "    )\n",
    "\n",
    "    df_contact = spark.read.table(f\"{silver_lakehouse_name}.Contact\") \\\n",
    "        .select(col(\"ContactId\"), col(\"SourceSystemId\").alias(\"ContactSourceSystemId\"), col(\"Email\"))\n",
    "\n",
    "    df_constituent = spark.read.table(f\"{silver_lakehouse_name}.Constituent\") \\\n",
    "        .select(\"ConstituentId\", \"ContactId\")\n",
    "\n",
    "    df_email_engagement = spark.read.table(f\"{silver_lakehouse_name}.EmailEngagement\") \\\n",
    "        .select(col(\"EmailEngagementId\"), col(\"SourceSystemId\").alias(\"EmailMessageId_silver\"))\n",
    "\n",
    "\n",
    "    # Join RelationId (Salesforce ContactId) to Contact.SourceSystemId\n",
    "    df = df.join(\n",
    "        df_contact,\n",
    "        df[\"RelationId\"] == df_contact[\"ContactSourceSystemId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Join ContactId to Constituent\n",
    "    df = df.join(\n",
    "        df_constituent,\n",
    "        df_contact[\"ContactId\"] == df_constituent[\"ContactId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Join EmailMessageId (bronze) to EmailEngagement.SourceSystemId (silver)\n",
    "    df = df.join(\n",
    "        df_email_engagement,\n",
    "        df[\"EmailMessageId\"] == df_email_engagement[\"EmailMessageId_silver\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(\n",
    "            expr(\"uuid()\").alias(\"ConstituentEmailEngagementId\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"EmailEngagementId\"),   # from join\n",
    "            col(\"EmailMessageId\").alias(\"EmailId\"),\n",
    "            col(\"SystemModstamp\").alias(\"ModifiedDate\"),\n",
    "            col(\"CreatedDate\").alias(\"SendDate\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "constituentEmailEngagementTable = CdfTable(\n",
    "    source_table_name=\"EmailMessageRelation\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"ConstituentEmailEngagement\",\n",
    "    columns=[\n",
    "        \"Id\", \"RelationId\", \"RelationType\", \"RelationObjectType\", \"RelationAddress\", \"EmailMessageId\", \"CreatedDate\", \"SystemModstamp\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.ConstituentEmailEngagement AS target\n",
    "    USING latestSnapshot_EmailMessageRelation AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.EmailEngagementId = source.EmailEngagementId,\n",
    "        target.ConstituentId = source.ConstituentId,\n",
    "        target.SendDate = source.SendDate\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        ConstituentEmailEngagementId, ConstituentId, CreatedDate, SourceSystemId, SourceId,\n",
    "        SendDate, EmailEngagementId, ModifiedDate, EmailId\n",
    "    ) VALUES (\n",
    "        source.ConstituentEmailEngagementId, source.ConstituentId, source.CreatedDate, source.SourceSystemId, source.SourceId,\n",
    "        source.SendDate, source.EmailEngagementId, source.ModifiedDate, source.EmailId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichConstituentEmailEngagement,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(constituentEmailEngagementTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d45684-db18-41b2-b621-d4860963b583",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65255a-bb63-41a1-88af-53182c122d75",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, expr\n",
    "from pyspark.sql.types import StringType, DecimalType\n",
    "\n",
    "def EnrichEvent(df):\n",
    "    # Silver EventType lookup\n",
    "    df_eventtype = get_silver_table(\"EventType\").select(\n",
    "        col(\"EventTypeId\"), col(\"Name\")\n",
    "    )\n",
    "\n",
    "    # Silver Channel lookup – we assume ChannelName = 'Events'\n",
    "    df_channel = get_silver_table(\"Channel\").filter(col(\"Name\") == \"Events\") \\\n",
    "        .select(col(\"ChannelId\").alias(\"EventsChannelId\")).limit(1)\n",
    "\n",
    "    # Join EventSubtype to EventType.Name\n",
    "    df = df.join(df_eventtype, df[\"EventSubtype\"] == df_eventtype[\"Name\"], \"left\")\n",
    "\n",
    "    # join to add ChannelId = 'Events'\n",
    "    df = df.join(df_channel, how=\"left\")\n",
    "\n",
    "    # Final output\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(\n",
    "            expr(\"uuid()\").alias(\"EventId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"EventTypeId\"),\n",
    "            col(\"EventsChannelId\").alias(\"ChannelId\"),  \n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            col(\"Subject\").alias(\"Name\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            col(\"StartDateTime\").alias(\"StartDate\"),\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "eventTable = CdfTable(\n",
    "    source_table_name=\"Event\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Event\",\n",
    "    columns=[\n",
    "        \"Id\", \"EventSubtype\", \"CreatedDate\", \"LastModifiedDate\", \"Subject\", \"StartDateTime\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Event AS target\n",
    "    USING latestSnapshot_Event AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate   = source.ModifiedDate,\n",
    "        target.EventTypeId    = source.EventTypeId,\n",
    "        target.Name           = source.Name,\n",
    "        target.StartDate      = source.StartDate,\n",
    "        target.ChannelId      = source.ChannelId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        EventId, CreatedDate, EventTypeId, ModifiedDate, Name, SourceId, SourceSystemId, StartDate, ChannelId\n",
    "    ) VALUES (\n",
    "        source.EventId, source.CreatedDate,\n",
    "        source.EventTypeId, source.ModifiedDate, source.Name, source.SourceId, source.SourceSystemId, source.StartDate, source.ChannelId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichEvent,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(eventTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb6e56-4280-4fb3-914a-31980e54bc74",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: ConstituentOpportunityStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5a09a-91ec-4dcd-a83b-f6557e63e781",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "import logging\n",
    "\n",
    "def EnrichConstituentOpportunityStage(df):\n",
    "    # Load reference tables\n",
    "    # Prepare lookup with aliased columns immediately\n",
    "    constituent_lookup = get_silver_table(\"Constituent\").select(\n",
    "        col(\"ConstituentId\"),\n",
    "        col(\"ContactId\").alias(\"Lkp_ContactId\"),\n",
    "        col(\"AccountId\").alias(\"Lkp_AccountId\")\n",
    "    )\n",
    "\n",
    "    stage_df = get_silver_table(\"OpportunityStage\") \\\n",
    "        .select(\"Name\", \"OpportunityStageId\")\n",
    "\n",
    "    account_df = get_silver_table(\"Account\") \\\n",
    "        .selectExpr(\"SourceSystemId as AccountSourceSystemId\", \"AccountId as AccountGuid\")\n",
    "\n",
    "    contact_df = get_silver_table(\"Contact\") \\\n",
    "        .selectExpr(\"SourceSystemId as ContactSourceSystemId\", \"ContactId as ContactGuid\")\n",
    "\n",
    "    logging.info(f\"🔎 Constituent rows: {constituent_lookup.count()}\")\n",
    "    logging.info(f\"🔎 OpportunityStage rows: {stage_df.count()}\")\n",
    "    logging.info(f\"🔎 Account rows: {account_df.count()}\")\n",
    "    logging.info(f\"🔎 Contact rows: {contact_df.count()}\")\n",
    "\n",
    "    # Join with OpportunityStage\n",
    "    df = df.join(stage_df, df[\"StageName\"] == stage_df[\"Name\"], how=\"left\")\n",
    "    logging.info(f\"🔄 After join with stage: {df.count()}\")\n",
    "\n",
    "    # Translate ContactId from source system ID to internal GUID\n",
    "    df = df.join(contact_df, df[\"ContactId\"] == contact_df[\"ContactSourceSystemId\"], how=\"left\")\n",
    "\n",
    "    # Translate AccountId from source system ID to internal GUID\n",
    "    df = df.join(account_df, df[\"AccountId\"] == account_df[\"AccountSourceSystemId\"], how=\"left\")\n",
    "    logging.info(f\"🔄 After join with contact and account: {df.count()}\")\n",
    "\n",
    "    # Join with Constituent using resolved internal GUIDs\n",
    "\n",
    "    # 1. Match via Contact (Primary)\n",
    "    df_with_contact = df.filter(col(\"ContactGuid\").isNotNull()) \\\n",
    "        .join(constituent_lookup, df[\"ContactGuid\"] == constituent_lookup[\"Lkp_ContactId\"], how=\"left\") \\\n",
    "        .drop(\"Lkp_ContactId\", \"Lkp_AccountId\")\n",
    "\n",
    "    # 2. Match via Account (Fallback)\n",
    "    df_with_account = df.filter(col(\"ContactGuid\").isNull()) \\\n",
    "        .join(constituent_lookup, df[\"AccountGuid\"] == constituent_lookup[\"Lkp_AccountId\"], how=\"left\") \\\n",
    "        .drop(\"Lkp_ContactId\", \"Lkp_AccountId\")\n",
    "\n",
    "    # Union results\n",
    "    df = df_with_contact.unionByName(df_with_account)\n",
    "    \n",
    "    logging.info(f\"🔄 After join with constituent: {df.count()}\")\n",
    "\n",
    "    # Select and filter final result\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(\n",
    "            expr(\"uuid()\").alias(\"ConstituentOpportunityStageId\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"OpportunityStageId\")\n",
    "        )\n",
    "        .dropDuplicates([\"ConstituentId\", \"OpportunityStageId\"])\n",
    "        .filter(col(\"ConstituentId\").isNotNull() & col(\"OpportunityStageId\").isNotNull())\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "constituentOpportunityStageTable = CdfTable(\n",
    "    source_table_name=\"Opportunity\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"ConstituentOpportunityStage\",\n",
    "    columns=[\"Id\", \"AccountId\", \"ContactId\", \"StageName\"],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.ConstituentOpportunityStage AS target\n",
    "    USING latestSnapshot_Opportunity AS source\n",
    "    ON target.ConstituentId = source.ConstituentId AND target.OpportunityStageId = source.OpportunityStageId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        ConstituentOpportunityStageId, ConstituentId, OpportunityStageId\n",
    "    ) VALUES (\n",
    "        source.ConstituentOpportunityStageId, source.ConstituentId, source.OpportunityStageId\n",
    "    )\n",
    "    WHEN NOT MATCHED BY SOURCE THEN DELETE\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichConstituentOpportunityStage,\n",
    "\n",
    ")\n",
    "\n",
    "ProcessCdfTable(constituentOpportunityStageTable, source_name)\n",
    "log_merge_metrics(\n",
    "    f\"{silver_lakehouse_name}.ConstituentOpportunityStage\",\n",
    "    \"ConstituentOpportunityStage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e17541-c1bc-40cc-a87b-2ef21cbd3f79",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ce89e-b4c4-4fa2-be17-ccbdc62bc11e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, lit, current_timestamp, when\n",
    "\n",
    "def EnrichLetter(df):\n",
    "    # Silver lookups\n",
    "    df_campaign = get_silver_table(\"Campaign\").select(\n",
    "        col(\"CampaignId\").alias(\"CampaignId_silver\"),\n",
    "        col(\"SourceSystemId\").alias(\"CampaignSourceSystemId\")\n",
    "    )\n",
    "\n",
    "    df_contact = get_silver_table(\"Contact\").select(\n",
    "        col(\"ContactId\").alias(\"ContactId_silver\"),\n",
    "        col(\"SourceSystemId\").alias(\"ContactSourceSystemId\")\n",
    "    )\n",
    "\n",
    "    df_constituent = get_silver_table(\"Constituent\").select(\n",
    "        \"ConstituentId\",\n",
    "        col(\"ContactId\").alias(\"ConstituentContactId\")\n",
    "    )\n",
    "\n",
    "    df_channel = get_silver_table(\"Channel\").select(\n",
    "        col(\"ChannelId\").alias(\"ChannelId_silver\"),\n",
    "        col(\"Name\").alias(\"ChannelName\")\n",
    "    )\n",
    "\n",
    "    # Filter: only Letter tasks (Subject = 'Send Letter')\n",
    "    df = df.filter(col(\"Subject\") == \"Send Letter\")\n",
    "\n",
    "    # Step 1: WhoId → Contact\n",
    "    df = df.join(df_contact, df[\"WhoId\"] == df_contact[\"ContactSourceSystemId\"], \"left\")\n",
    "\n",
    "    # Step 2: ContactId → Constituent\n",
    "    df = df.join(df_constituent, df[\"ContactId_silver\"] == df_constituent[\"ConstituentContactId\"], \"left\")\n",
    "\n",
    "    # Step 3: WhatId → Campaign\n",
    "    df = df.join(df_campaign, df[\"WhatId\"] == df_campaign[\"CampaignSourceSystemId\"], \"left\")\n",
    "\n",
    "    # Step 4: Map TaskSubtype → ChannelName (e.g., 'Task' → 'Direct Mail')\n",
    "    df = df.withColumn(\"ResolvedChannelName\",\n",
    "        when(col(\"TaskSubtype\") == \"Task\", lit(\"Direct Mail\"))\n",
    "    )\n",
    "\n",
    "    # Step 5: ResolvedChannelName → Channel to get ChannelId\n",
    "    df = df.join(df_channel, df[\"ResolvedChannelName\"] == df_channel[\"ChannelName\"], \"left\")\n",
    "\n",
    "    # Final output\n",
    "    new_df = (\n",
    "        df.select(\n",
    "            expr(\"uuid()\").alias(\"LetterId\"),\n",
    "            col(\"CampaignId_silver\").alias(\"CampaignId\"),\n",
    "            col(\"ChannelId_silver\").alias(\"ChannelId\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CompletedDateTime\").alias(\"SentDate\"),\n",
    "            current_timestamp().alias(\"CreatedDate\"),\n",
    "            col(\"Subject\"),\n",
    "            current_timestamp().alias(\"ModifiedDate\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "letterTable = CdfTable(\n",
    "    source_table_name=\"Task\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Letter\",\n",
    "    columns=[\n",
    "        \"Id\", \"WhoId\", \"WhatId\", \"Subject\", \"TaskSubtype\", \"CompletedDateTime\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Letter AS target\n",
    "    USING latestSnapshot_Task AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.SentDate     = source.SentDate,\n",
    "        target.Subject      = source.Subject,\n",
    "        target.CampaignId   = source.CampaignId,\n",
    "        target.ChannelId    = source.ChannelId,\n",
    "        target.ConstituentId= source.ConstituentId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        LetterId, CampaignId, ChannelId, ConstituentId, CreatedDate,\n",
    "        Subject, ModifiedDate, SourceId, SourceSystemId, SentDate\n",
    "    ) VALUES (\n",
    "        source.LetterId, source.CampaignId, source.ChannelId, source.ConstituentId, source.CreatedDate,\n",
    "        source.Subject, source.ModifiedDate, source.SourceId, source.SourceSystemId, source.SentDate\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichLetter,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(letterTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022d9a5-2f38-4bf0-ac5f-9ef060b5ce0c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e7961-bfcd-461a-b602-69cac6deb027",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, broadcast\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "def TransformOpportunity(df):\n",
    "    # Lookup Campaign\n",
    "    campaign_df = get_silver_table(\"Campaign\") \\\n",
    "        .select(col(\"CampaignId\").alias(\"CampaignGuid\"), col(\"SourceSystemId\").alias(\"CampaignSfdcId\"))\n",
    "\n",
    "    # Lookup Contact\n",
    "    contact_df = get_silver_table(\"Contact\") \\\n",
    "        .select(col(\"ContactId\").alias(\"ContactGuid\"), col(\"SourceSystemId\").alias(\"ContactSfdcId\"))\n",
    "\n",
    "    # Lookup Account\n",
    "    account_df = get_silver_table(\"Account\") \\\n",
    "        .select(col(\"AccountId\").alias(\"AccountGuid\"), col(\"SourceSystemId\").alias(\"AccountSfdcId\"))\n",
    "\n",
    "    # Lookup Constituent (Aliased for split-join optimization)\n",
    "    constituent_lookup = get_silver_table(\"Constituent\") \\\n",
    "        .select(\n",
    "            col(\"ConstituentId\"), \n",
    "            col(\"ContactId\").alias(\"Lkp_ContactId\"), \n",
    "            col(\"AccountId\").alias(\"Lkp_AccountId\")\n",
    "        )\n",
    "\n",
    "    # Lookup OpportunityStage\n",
    "    stage_df = get_silver_table(\"OpportunityStage\") \\\n",
    "        .select(col(\"Name\").alias(\"StageName_lookup\"), \"OpportunityStageId\")\n",
    "\n",
    "    # Lookup OpportunityType\n",
    "    type_df = get_silver_table(\"OpportunityType\") \\\n",
    "        .select(col(\"Name\").alias(\"TypeName_lookup\"), \"OpportunityTypeId\")\n",
    "\n",
    "    # 1. Base Enrichment (Standard Joins)\n",
    "    # Using broadcast for small reference tables (Stage, Type) to avoid skew\n",
    "    df_base = (\n",
    "        df.join(broadcast(stage_df), df[\"StageName\"] == stage_df[\"StageName_lookup\"], how=\"left\")\n",
    "          .join(broadcast(type_df), df[\"Type\"] == type_df[\"TypeName_lookup\"], how=\"left\") \n",
    "          .join(campaign_df, df[\"CampaignId\"] == campaign_df[\"CampaignSfdcId\"], how=\"left\")\n",
    "          .join(contact_df, df[\"ContactId\"] == contact_df[\"ContactSfdcId\"], how=\"left\")\n",
    "          .join(account_df, df[\"AccountId\"] == account_df[\"AccountSfdcId\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    # 2. Split Join for Constituent (Optimization for OR condition)\n",
    "    \n",
    "    # Branch A: Match via Contact (Primary)\n",
    "    df_contact_match = df_base.filter(col(\"ContactGuid\").isNotNull())\n",
    "    df_contact_match = df_contact_match.join(\n",
    "        constituent_lookup,\n",
    "        df_contact_match[\"ContactGuid\"] == constituent_lookup[\"Lkp_ContactId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Branch B: Match via Account (Fallback, only if Contact is missing)\n",
    "    df_account_match = df_base.filter(col(\"ContactGuid\").isNull())\n",
    "    df_account_match = df_account_match.join(\n",
    "        constituent_lookup,\n",
    "        df_account_match[\"AccountGuid\"] == constituent_lookup[\"Lkp_AccountId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # 3. Union results\n",
    "    df_enriched = df_contact_match.unionByName(df_account_match)\n",
    "\n",
    "    new_df = (\n",
    "        df_enriched.select(\n",
    "            expr(\"uuid()\").alias(\"OpportunityId\"),\n",
    "            col(\"CampaignGuid\").alias(\"CampaignId\"),\n",
    "            col(\"CloseDate\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"ExpectedRevenue\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            col(\"OpportunityStageId\"),\n",
    "            col(\"OpportunityTypeId\"),\n",
    "            col(\"Name\").alias(\"OpportunityName\") \n",
    "        )\n",
    "        .dropDuplicates([\"SourceSystemId\", \"SourceId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "opportunityTable = CdfTable(\n",
    "    source_table_name=\"Opportunity\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Opportunity\",\n",
    "    columns=[\n",
    "        \"Id\", \"AccountId\", \"Amount\", \"CampaignId\", \"CloseDate\", \"ContactId\",\n",
    "        \"CreatedDate\", \"ExpectedRevenue\", \"LastModifiedDate\", \"Name\",\n",
    "        \"RecordTypeId\", \"StageName\", \"SystemModstamp\", \"Type\", \"npe03__Recurring_Donation__c\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Opportunity AS target\n",
    "    USING latestSnapshot_Opportunity AS source\n",
    "    ON target.SourceSystemId = source.SourceSystemId AND target.SourceId = source.SourceId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.CampaignId = source.CampaignId,\n",
    "        target.CloseDate = source.CloseDate,\n",
    "        target.ConstituentId = source.ConstituentId,\n",
    "        target.ExpectedRevenue = source.ExpectedRevenue,\n",
    "        target.OpportunityStageId = source.OpportunityStageId,\n",
    "        target.OpportunityTypeId = source.OpportunityTypeId,\n",
    "        target.OpportunityName = source.OpportunityName  \n",
    "\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        OpportunityId, CampaignId, CloseDate,\n",
    "        ConstituentId, CreatedDate, ExpectedRevenue, ModifiedDate,\n",
    "        SourceId, SourceSystemId, OpportunityStageId, OpportunityTypeId, OpportunityName\n",
    "    ) VALUES (\n",
    "        source.OpportunityId, source.CampaignId, source.CloseDate,\n",
    "        source.ConstituentId, source.CreatedDate, source.ExpectedRevenue, source.ModifiedDate,\n",
    "        source.SourceId, source.SourceSystemId, source.OpportunityStageId, source.OpportunityTypeId, source.OpportunityName\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=TransformOpportunity,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(opportunityTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a1318-20bd-48ad-89e3-75ceb137f557",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144447ef-0536-4477-935a-27fbfc4271d7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, lit\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def TransformParticipation(df):\n",
    "    constituent_df = get_silver_table(\"Constituent\") \\\n",
    "        .select(\"ConstituentId\", \"ContactId\")\n",
    "\n",
    "    contact_df = get_silver_table(\"Contact\") \\\n",
    "        .select(\"ContactId\", \"SourceSystemId\")\n",
    "    \n",
    "    contact_constituent_df = contact_df.join(\n",
    "        constituent_df,\n",
    "        on=\"ContactId\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Load/Insert ParticipationType if not exists\n",
    "    participation_type_df = get_silver_table(\"ParticipationType\") \\\n",
    "        .filter(col(\"Name\") == \"Volunteering\") \\\n",
    "        .select(\"ParticipationTypeId\")\n",
    "\n",
    "    if participation_type_df.isEmpty():\n",
    "        new_type_df = spark.createDataFrame([(\n",
    "            str(uuid.uuid4()), \"Volunteering\", source_id, \"Volunteering\",\n",
    "            datetime.now(timezone.utc), datetime.now(timezone.utc)\n",
    "        )], [\"ParticipationTypeId\", \"Name\", \"SourceId\", \"SourceSystemId\", \"CreatedDate\", \"ModifiedDate\"])\n",
    "\n",
    "        new_type_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{silver_lakehouse_name}.ParticipationType\")\n",
    "        participation_type_df = new_type_df.select(\"ParticipationTypeId\")\n",
    "\n",
    "    participation_type_id = participation_type_df.collect()[0][0]\n",
    "\n",
    "    # Join with contact → constituent\n",
    "    df = df.join(contact_constituent_df, df[\"GW_Volunteers__Contact__c\"] == contact_constituent_df[\"SourceSystemId\"], \"left\")\n",
    "\n",
    "\n",
    "    new_df = (\n",
    "        df.select(\n",
    "            expr(\"uuid()\").alias(\"ParticipationId\"),\n",
    "            lit(True).alias(\"AttendedEvent\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"GW_Volunteers__End_Date__c\").alias(\"EndDate\"),\n",
    "            col(\"GW_Volunteers__Hours_Worked__c\").alias(\"Hours\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            lit(participation_type_id).alias(\"ParticipationTypeId\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            col(\"GW_Volunteers__Start_Date__c\").alias(\"StartDate\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "participationTable = CdfTable(\n",
    "    source_table_name=\"VolunteerHours\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Participation\",\n",
    "    columns=[\n",
    "        \"Id\",\n",
    "        \"GW_Volunteers__Contact__c\",\n",
    "        \"GW_Volunteers__Start_Date__c\",\n",
    "        \"GW_Volunteers__End_Date__c\",\n",
    "        \"GW_Volunteers__Hours_Worked__c\",\n",
    "        \"GW_Volunteers__Volunteer_Campaign__c\",\n",
    "        \"CreatedDate\",\n",
    "        \"LastModifiedDate\",\n",
    "        \"SystemModstamp\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Participation AS target\n",
    "    USING latestSnapshot_VolunteerHours AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate = source.ModifiedDate,\n",
    "        target.AttendedEvent = source.AttendedEvent,\n",
    "        target.Hours = source.Hours,\n",
    "        target.EndDate = source.EndDate,\n",
    "        target.StartDate = source.StartDate\n",
    "\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        ParticipationId, AttendedEvent, ConstituentId, CreatedDate,\n",
    "        EndDate, Hours, ModifiedDate, ParticipationTypeId,\n",
    "        SourceId, SourceSystemId, StartDate\n",
    "    ) VALUES (\n",
    "        source.ParticipationId, source.AttendedEvent, source.ConstituentId, source.CreatedDate,\n",
    "        source.EndDate, source.Hours, source.ModifiedDate, source.ParticipationTypeId,\n",
    "        source.SourceId, source.SourceSystemId, source.StartDate\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=TransformParticipation,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(participationTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e2f3e-409d-4384-81d5-633ad0276188",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Phonecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e863687-5e41-4da5-856d-ac60c647841c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, lit, current_timestamp, when\n",
    "\n",
    "def EnrichPhoneCall(df):\n",
    "    # Load Silver lookups\n",
    "    df_campaign = get_silver_table(\"Campaign\").select(\n",
    "        col(\"CampaignId\").alias(\"CampaignId_silver\"),\n",
    "        col(\"SourceSystemId\").alias(\"CampaignSourceSystemId\")\n",
    "    )\n",
    "\n",
    "    df_contact = get_silver_table(\"Contact\").select(\n",
    "        col(\"ContactId\").alias(\"ContactId_silver\"),\n",
    "        col(\"SourceSystemId\").alias(\"ContactSourceSystemId\")\n",
    "    )\n",
    "\n",
    "    df_constituent = get_silver_table(\"Constituent\").select(\n",
    "        col(\"ConstituentId\"),\n",
    "        col(\"ContactId\").alias(\"ConstituentContactId\")\n",
    "    )\n",
    "\n",
    "    df_channel = get_silver_table(\"Channel\").select(\n",
    "        col(\"ChannelId\").alias(\"ChannelId_silver\"),\n",
    "        col(\"Name\").alias(\"ChannelName\")\n",
    "    )\n",
    "\n",
    "    # Filter: only Call-type tasks\n",
    "    df = df.filter(col(\"TaskSubtype\") == \"Call\")\n",
    "\n",
    "    # Step 1: Join WhoId → Contact\n",
    "    df = df.join(df_contact, df[\"WhoId\"] == df_contact[\"ContactSourceSystemId\"], \"left\")\n",
    "\n",
    "    # Step 2: Join ContactId → Constituent\n",
    "    df = df.join(df_constituent, df[\"ContactId_silver\"] == df_constituent[\"ConstituentContactId\"], \"left\")\n",
    "\n",
    "    # Step 3: Join WhatId → Campaign (Silver)\n",
    "    df = df.join(df_campaign, df[\"WhatId\"] == df_campaign[\"CampaignSourceSystemId\"], \"left\")\n",
    "\n",
    "    # Step 4: Resolve TaskSubtype = 'Call' → ChannelName = 'Phone Call'\n",
    "    df = df.withColumn(\"ResolvedChannelName\",\n",
    "        when(col(\"TaskSubtype\") == \"Call\", lit(\"Phone Call\"))\n",
    "    )\n",
    "\n",
    "    # Step 5: ResolvedChannelName → Channel to get ChannelId\n",
    "    df = df.join(df_channel, df[\"ResolvedChannelName\"] == df_channel[\"ChannelName\"], \"left\")\n",
    "\n",
    "    # Final output\n",
    "    new_df = (\n",
    "        df.select(\n",
    "            expr(\"uuid()\").alias(\"PhonecallId\"),\n",
    "            col(\"CampaignId_silver\").alias(\"CampaignId\"),\n",
    "            col(\"ChannelId_silver\").alias(\"ChannelId\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CompletedDateTime\").alias(\"CallDate\"),\n",
    "            current_timestamp().alias(\"CreatedDate\"),\n",
    "            col(\"Description\"),\n",
    "            current_timestamp().alias(\"ModifiedDate\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "phonecallTable = CdfTable(\n",
    "    source_table_name=\"Task\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Phonecall\",\n",
    "    columns=[\n",
    "        \"Id\", \"WhoId\", \"WhatId\", \"Subject\", \"Description\", \"TaskSubtype\", \"CompletedDateTime\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Phonecall AS target\n",
    "    USING latestSnapshot_Task AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate   = source.ModifiedDate,\n",
    "        target.CallDate       = source.CallDate,\n",
    "        target.CampaignId     = source.CampaignId,\n",
    "        target.ChannelId      = source.ChannelId,\n",
    "        target.ConstituentId  = source.ConstituentId,\n",
    "        target.Description    = source.Description\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        PhonecallId, CampaignId, ChannelId, ConstituentId, CreatedDate,\n",
    "        Description, ModifiedDate, SourceId, SourceSystemId, CallDate\n",
    "    ) VALUES (\n",
    "        source.PhonecallId, source.CampaignId, source.ChannelId, source.ConstituentId, source.CreatedDate,\n",
    "        source.Description, source.ModifiedDate, source.SourceId, source.SourceSystemId, source.CallDate\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichPhoneCall,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(phonecallTable, source_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12216b-15e3-4a9f-8fc9-21c06fad3efc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88a982-cdc5-43fc-8f6f-f90f540d8655",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichTransaction(df):\n",
    "    # Silver lookups\n",
    "    df_account = spark.read.table(f\"{silver_lakehouse_name}.Account\").select(\"AccountId\", \"SourceSystemId\")\n",
    "    df_contact = spark.read.table(f\"{silver_lakehouse_name}.Contact\").select(\"ContactId\", \"SourceSystemId\")\n",
    "    df_constituent = spark.read.table(f\"{silver_lakehouse_name}.Constituent\").select(\n",
    "        col(\"ConstituentId\"),\n",
    "        col(\"AccountId\").alias(\"AccountId_constituent\"),\n",
    "        col(\"ContactId\").alias(\"ContactId_constituent\")\n",
    "    )\n",
    "    df_campaign_silver = spark.read.table(f\"{silver_lakehouse_name}.Campaign\").select(\"CampaignId\", \"SourceSystemId\")\n",
    "    df_channel_silver = spark.read.table(f\"{silver_lakehouse_name}.Channel\").select(\"ChannelId\", col(\"Name\").alias(\"ChannelName\"))\n",
    "    df_campaign_bronze = spark.read.table(f\"{bronze_lakehouse_name}.Campaign\").select(\n",
    "        col(\"Id\").alias(\"CampaignId_bronze\"),\n",
    "        col(\"Type\").alias(\"CampaignType_bronze\")\n",
    "    )\n",
    "    df_recordtype = spark.read.table(f\"{bronze_lakehouse_name}.RecordType\").select(\n",
    "        col(\"Id\").alias(\"RecordTypeId_bronze\"),\n",
    "        col(\"Name\").alias(\"RecordTypeName_bronze\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumnRenamed(\"Name\", \"OpportunityName\")\n",
    "\n",
    "    # Filter by allowed record types\n",
    "    df = (\n",
    "        df.join(\n",
    "            df_recordtype,\n",
    "            df[\"RecordTypeId\"] == df_recordtype[\"RecordTypeId_bronze\"],\n",
    "            \"left\"\n",
    "        )\n",
    "        .filter(col(\"RecordTypeName_bronze\").isin([\"Donation\", \"Grant\", \"In Kind\"]))\n",
    "    )\n",
    "\n",
    "    # Join to Account and Contact to get GUIDs\n",
    "    df = (\n",
    "        df\n",
    "        .join(df_account.withColumnRenamed(\"AccountId\", \"AccountId_silver\"), df[\"AccountId\"] == df_account[\"SourceSystemId\"], \"left\")\n",
    "        .join(df_contact.withColumnRenamed(\"ContactId\", \"ContactId_silver\"), df[\"ContactId\"] == df_contact[\"SourceSystemId\"], \"left\")\n",
    "    )\n",
    "\n",
    "    # Join to Constituent: Contact and Account separately, use unique suffixes\n",
    "    df = (\n",
    "        df\n",
    "        # Contact Constituent (with alias)\n",
    "        .join(\n",
    "            df_constituent\n",
    "                .withColumnRenamed(\"ConstituentId\", \"ConstituentId_Contact\")\n",
    "                .withColumnRenamed(\"ContactId_constituent\", \"ContactId_constituent_contact\")\n",
    "                .withColumnRenamed(\"AccountId_constituent\", \"AccountId_constituent_contact\"),\n",
    "            col(\"ContactId_silver\") == col(\"ContactId_constituent_contact\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        # Account Constituent (with alias)\n",
    "        .join(\n",
    "            df_constituent\n",
    "                .withColumnRenamed(\"ConstituentId\", \"ConstituentId_Account\")\n",
    "                .withColumnRenamed(\"ContactId_constituent\", \"ContactId_constituent_account\")\n",
    "                .withColumnRenamed(\"AccountId_constituent\", \"AccountId_constituent_account\"),\n",
    "            col(\"AccountId_silver\") == col(\"AccountId_constituent_account\"),\n",
    "            \"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join Silver Campaign\n",
    "    df = df.join(\n",
    "        df_campaign_silver.withColumnRenamed(\"CampaignId\", \"CampaignId_silver\"),\n",
    "        df[\"CampaignId\"] == df_campaign_silver[\"SourceSystemId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Join Bronze Campaign to get Type\n",
    "    df = df.join(\n",
    "        df_campaign_bronze,\n",
    "        df[\"CampaignId\"] == df_campaign_bronze[\"CampaignId_bronze\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Join Silver Channel by matching Campaign.Type to Channel.Name\n",
    "    df = df.join(\n",
    "        df_channel_silver.withColumnRenamed(\"ChannelId\", \"ChannelId_silver\"),\n",
    "        df[\"CampaignType_bronze\"] == df_channel_silver[\"ChannelName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(\n",
    "            expr(\"uuid()\").alias(\"TransactionId\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"CampaignId_silver\").alias(\"CampaignId\"),\n",
    "            col(\"ChannelId_silver\").alias(\"ChannelId\"),\n",
    "            # Prefer Contact ConstituentId, fallback to Account ConstituentId (unique aliases)\n",
    "            coalesce(col(\"ConstituentId_Contact\"), col(\"ConstituentId_Account\")).alias(\"ConstituentId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            (col(\"npe03__Recurring_Donation__c\").isNotNull() & (col(\"npe03__Recurring_Donation__c\") != \"\") ).alias(\"IsRecurring\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            col(\"OpportunityName\").alias(\"Name\"),\n",
    "            expr(\"null\").cast(\"string\").alias(\"OpportunityId\"),  # Adjust if needed\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"Id\").alias(\"SourceSystemId\"),\n",
    "            col(\"CloseDate\").alias(\"TransactionDate\"),\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# ---- CdfTable ----\n",
    "transactionTable = CdfTable(\n",
    "    source_table_name=\"Opportunity\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"Transaction\",\n",
    "    columns=[\n",
    "        \"Id\", \"Name\", \"AccountId\", \"ContactId\", \"Amount\", \"CampaignId\", \"RecordTypeId\",\n",
    "        \"CreatedDate\", \"LastModifiedDate\", \"Type\", \"CloseDate\", \"npe03__Recurring_Donation__c\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.Transaction AS target\n",
    "    USING latestSnapshot_Opportunity AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate     = source.ModifiedDate,\n",
    "        target.Amount           = source.Amount,\n",
    "        target.CampaignId       = source.CampaignId,\n",
    "        target.ChannelId        = source.ChannelId,\n",
    "        target.ConstituentId    = source.ConstituentId,\n",
    "        target.Name             = source.Name,\n",
    "        target.TransactionDate  = source.TransactionDate,\n",
    "        target.IsRecurring      = source.IsRecurring\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        TransactionId, Amount, CampaignId, ChannelId, ConstituentId, CreatedDate, IsRecurring,\n",
    "        ModifiedDate, Name, OpportunityId, SourceId, SourceSystemId, TransactionDate\n",
    "    ) VALUES (\n",
    "        source.TransactionId, source.Amount, source.CampaignId, source.ChannelId, source.ConstituentId, source.CreatedDate,\n",
    "        source.IsRecurring, source.ModifiedDate, source.Name, source.OpportunityId, source.SourceId,\n",
    "        source.SourceSystemId, source.TransactionDate\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichTransaction,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(transactionTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfbe43-c0dd-48d7-b27d-d5afff6124eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### Transform: SoftCredit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b7bb4-4831-4c62-b934-fa66d922bee1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def EnrichSoftCredit(df):\n",
    "    # Only keep rows where Role == \"Soft Credit\"\n",
    "    df = df.filter(col(\"Role\") == \"Soft Credit\")\n",
    "\n",
    "    # Silver Transaction lookup (get TransactionId by matching OpportunityId)\n",
    "    df_transaction = get_silver_table(\"Transaction\") \\\n",
    "        .select(col(\"TransactionId\"), col(\"SourceSystemId\"), col(\"Amount\"))  # SourceSystemId is Opportunity.Id\n",
    "\n",
    "    # Silver Contact lookup\n",
    "    df_contact = get_silver_table(\"Contact\") \\\n",
    "        .select(col(\"ContactId\"), col(\"SourceSystemId\"))\n",
    "\n",
    "    # Silver Constituent lookup\n",
    "    df_constituent = get_silver_table(\"Constituent\") \\\n",
    "        .select(\"ConstituentId\", \"ContactId\")\n",
    "\n",
    "    # Join OCR.OpportunityId → Transaction.SourceSystemId\n",
    "    df = df.join(\n",
    "        df_transaction,\n",
    "        df[\"OpportunityId\"] == df_transaction[\"SourceSystemId\"],   # THIS IS THE CRUCIAL JOIN\n",
    "        \"left\"\n",
    "    )\n",
    "    # Join OCR.ContactId → Contact.SourceSystemId\n",
    "    df = df.join(\n",
    "        df_contact.withColumnRenamed(\"ContactId\", \"ContactId_silver\"),\n",
    "        df[\"ContactId\"] == df_contact[\"SourceSystemId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    # Join ContactId_silver → Constituent.ContactId\n",
    "    df = df.join(\n",
    "        df_constituent,\n",
    "        df[\"ContactId_silver\"] == df_constituent[\"ContactId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    new_df = (\n",
    "        df\n",
    "        .select(\n",
    "            expr(\"uuid()\").alias(\"SoftCreditId\"),\n",
    "            col(\"ConstituentId\"),\n",
    "            col(\"CreatedDate\"),\n",
    "            col(\"LastModifiedDate\").alias(\"ModifiedDate\"),\n",
    "            lit(source_id).alias(\"SourceId\"),\n",
    "            col(\"OpportunityId\").alias(\"SourceSystemId\"),  # SourceSystemId comes from OpportunityContactRole\n",
    "            col(\"TransactionId\"),\n",
    "            col(\"Amount\")\n",
    "        )\n",
    "        .dropDuplicates([\"SourceId\", \"SourceSystemId\"])\n",
    "    )\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "softCreditTable = CdfTable(\n",
    "    source_table_name=\"OpportunityContactRole\",\n",
    "    source_primary_key=\"Id\",\n",
    "    target_table_name=\"SoftCredit\",\n",
    "    columns=[\n",
    "        \"Id\", \"OpportunityId\", \"ContactId\", \"CreatedDate\", \"LastModifiedDate\", \"Role\"\n",
    "    ],\n",
    "    merge_sql_template=f\"\"\"\n",
    "    MERGE INTO {silver_lakehouse_name}.SoftCredit AS target\n",
    "    USING latestSnapshot_OpportunityContactRole AS source\n",
    "    ON target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.ModifiedDate   = source.ModifiedDate,\n",
    "        target.ConstituentId  = source.ConstituentId,\n",
    "        target.Amount         = source.Amount,\n",
    "        target.TransactionId  = source.TransactionId\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        SoftCreditId, Amount, ConstituentId, CreatedDate,\n",
    "        ModifiedDate, SourceId, SourceSystemId, TransactionId\n",
    "    ) VALUES (\n",
    "        source.SoftCreditId, source.Amount, source.ConstituentId, source.CreatedDate,\n",
    "        source.ModifiedDate, source.SourceId, source.SourceSystemId, source.TransactionId\n",
    "    )\n",
    "    \"\"\",\n",
    "    source_lakehouse=bronze_lakehouse_name,\n",
    "    target_lakehouse=silver_lakehouse_name,\n",
    "    enrich_func=EnrichSoftCredit,\n",
    "    hard_delete=True\n",
    ")\n",
    "\n",
    "ProcessCdfTable(softCreditTable, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80395b2-3843-4ad6-b039-53e7dd431086",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "### UpdateSourceSystemIdMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b11b91-7f43-439b-b619-0a7842c07da5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, broadcast\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "\n",
    "def UpdateSourceSystemIdMapping(mapping_table_full_name: str):\n",
    "    logging.info(f\"🔍 Loading existing records from {mapping_table_full_name}...\")\n",
    "    existing_mappings = spark.table(mapping_table_full_name) \\\n",
    "        .select(\"SourceId\", \"SourceSystemId\", \"SourceTable\") \\\n",
    "        .distinct() \\\n",
    "        .persist()\n",
    "\n",
    "    logging.info(f\"📋 Listing all tables in {silver_lakehouse_name} lakehouse...\")\n",
    "    tables = [\n",
    "        t.name for t in spark.catalog.listTables(silver_lakehouse_name)\n",
    "        if t.tableType.lower() == \"managed\"\n",
    "    ]\n",
    "\n",
    "    logging.info(f\"✅ Found {len(tables)} tables for analysis.\")\n",
    "\n",
    "    for table_name in tables:\n",
    "        try:\n",
    "            df = get_silver_table(table_name)\n",
    "            schema_fields = [f.name for f in df.schema.fields]\n",
    "\n",
    "            required_columns = {\"SourceId\", \"SourceSystemId\"}\n",
    "            id_column = f\"{table_name}Id\"\n",
    "\n",
    "            if not required_columns.issubset(schema_fields):\n",
    "                continue\n",
    "            if id_column not in schema_fields:\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"🔄 Processing table: {table_name}\")\n",
    "\n",
    "            df_ids = df.select(\n",
    "                col(\"SourceId\"),\n",
    "                col(\"SourceSystemId\"),\n",
    "                col(id_column).alias(\"SilverRecordId\")\n",
    "            ) \\\n",
    "            .dropna(subset=[\"SourceId\", \"SourceSystemId\"]) \\\n",
    "            .dropDuplicates([\"SourceId\", \"SourceSystemId\"]) \\\n",
    "            .withColumn(\"SourceTable\", lit(table_name))\n",
    "\n",
    "            df_new = df_ids.join(\n",
    "                broadcast(existing_mappings),\n",
    "                on=[\"SourceId\", \"SourceSystemId\", \"SourceTable\"],\n",
    "                how=\"left_anti\"\n",
    "            )\n",
    "\n",
    "            count_new = df_new.count()\n",
    "            if count_new > 0:\n",
    "                logging.info(f\"➕ Inserting {count_new} new records into {mapping_table_full_name}\")\n",
    "                df_new.write.format(\"delta\").mode(\"append\").saveAsTable(mapping_table_full_name)\n",
    "            else:\n",
    "                logging.info(\"0️⃣ No new records to insert.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"⛔ Error while processing table {table_name}: {e}\")\n",
    "\n",
    "    existing_mappings.unpersist()\n",
    "    logging.info(\"✅ Done. SourceSystemIdMapping is up to date.\")\n",
    "\n",
    "# [TEMPORARY SOLUTION] Truncate table\n",
    "mapping_table_full_name = get_full_table_name(silver_lakehouse_name, \"SourceSystemIdMapping\")\n",
    "spark.sql(f\"TRUNCATE TABLE {mapping_table_full_name}\")\n",
    "UpdateSourceSystemIdMapping(mapping_table_full_name)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "{BRONZE_LAKEHOUSE_ID}",
    "default_lakehouse_name": "{BRONZE_LAKEHOUSE_NAME}",
    "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
    "known_lakehouses": [
     {
      "id": "{BRONZE_LAKEHOUSE_ID}"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "layout": "standard",
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
