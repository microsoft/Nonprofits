{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0401695e-9201-481b-b520-442e96f33b83",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# SampleData Import To Silver Layer\n",
        "Use this notebook for importing sample data from csv files stored in /Files to silver layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c3c615-9224-4aca-a1b8-c373107a055e",
      "metadata": {
        "editable": true,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "%run <Fundraising_SL_CreateSchema> { \"enable_create_tables\": false }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8fdd24a-911c-4b64-b2e9-d39c9dcae89d",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd088dd-7e76-4a56-acb7-fc591d00f2e8",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# Define lakehouse where the data should be stored\n",
        "target_lakehouse = silver_lakehouse_name\n",
        "\n",
        "# Feature flags\n",
        "enable_delete_sample_data = False\n",
        "enable_import_sample_data = True\n",
        "\n",
        "# Define paths\n",
        "input_folder = \"file:/lakehouse/default/Files/nds-silver-sampledata/\"\n",
        "output_folder = \"/lakehouse/default/Tables/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9419a21a-6eb5-49f8-a4f6-5b2cf4e4ed16",
      "metadata": {
        "editable": true,
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Delete sample data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f888c5ee-60a6-4521-9be6-a4973eadff41",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "if enable_delete_sample_data:\n",
        "    # Get all table names (same logic as import)\n",
        "    files = mssparkutils.fs.ls(input_folder)\n",
        "    csv_files = [f.path for f in files if f.path.endswith(\".csv\")]\n",
        "    table_names = [os.path.basename(f).replace(\".csv\", \"\") for f in csv_files]\n",
        "\n",
        "    # Track failures\n",
        "    failures = []\n",
        "\n",
        "    print(\"üßπ Starting table data deletion...\\n\")\n",
        "\n",
        "    for table_name in table_names:\n",
        "        print(f\"üóëÔ∏è Deleting data from table: {table_name}\")\n",
        "        \n",
        "        try:\n",
        "            full_table_name = get_full_table_name(target_lakehouse, table_name)\n",
        "            spark.sql(f\"DELETE FROM {full_table_name}\")\n",
        "            # not tested yet\n",
        "            #spark.sql(f\"DELETE FROM {full_table_name} USING Source WHERE {full_table_name}.SourceId = Source.SourceId AND Source.Name = 'SampleData'\")\n",
        "            print(f\"‚úÖ Cleared: {table_name}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to clear: {table_name}\")\n",
        "            print(traceback.format_exc())\n",
        "            failures.append((table_name, str(e)))\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\nüßæ Deletion Summary:\")\n",
        "    print(f\"‚úîÔ∏è Cleared: {len(table_names) - len(failures)} tables\")\n",
        "    print(f\"‚ùå Failed: {len(failures)} tables\")\n",
        "\n",
        "    if failures:\n",
        "        print(\"\\nüìå Failures:\")\n",
        "        for tbl, err in failures:\n",
        "            print(f\" - {tbl}: {err}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f0eebf2-dcab-462c-864b-f92e92e49465",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "## Sample Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2e7db9-e2f7-4dd9-b365-b6f208a96278",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, broadcast, lit\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from delta.tables import DeltaTable\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime, timezone\n",
        "import traceback\n",
        "\n",
        "# ==============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def get_csv_files(input_folder):\n",
        "    \"\"\"\n",
        "    Get list of CSV files from the input folder.\n",
        "    \n",
        "    Returns:\n",
        "        list: List of CSV file paths\n",
        "    \"\"\"\n",
        "    files = mssparkutils.fs.ls(input_folder)\n",
        "    csv_files = [f.path for f in files if f.path.endswith(\".csv\")]\n",
        "    csv_files.sort()  # Sort alphabetically\n",
        "    \n",
        "    return csv_files\n",
        "\n",
        "def process_table(file_path, failures_list):\n",
        "    \"\"\"\n",
        "    Process a single table from CSV file to lakehouse.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the CSV file\n",
        "        failures_list: List to append failures to\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    table_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
        "    logging.info(f\"\\nüöÄ Processing table: {table_name}\")\n",
        "    \n",
        "    try:\n",
        "        full_table_name = get_full_table_name(target_lakehouse, table_name)\n",
        "        logging.info(f\"üìÑ Reading file: {file_path}\")\n",
        "\n",
        "        method_name = f\"get_{table_name.lower()}_schema\"\n",
        "        expected_schema = getattr(NonprofitSilverModel, method_name)()\n",
        "\n",
        "        if not expected_schema:\n",
        "            raise ValueError(f\"‚ùå No schema defined for table: {table_name}\")\n",
        "\n",
        "        # Step 1: Read CSV without applying schema\n",
        "        raw_df = spark.read \\\n",
        "            .option(\"header\", True) \\\n",
        "            .csv(file_path)\n",
        "\n",
        "        # Step 2: Select and cast columns according to schema (by name, order-independent)\n",
        "        casted_cols = []\n",
        "        for field in expected_schema:\n",
        "            if field.name in raw_df.columns:\n",
        "                casted_cols.append(col(field.name).cast(field.dataType).alias(field.name))\n",
        "            else:\n",
        "                raise ValueError(f\"‚ùå Missing expected column: {field.name} in {file_path}\")\n",
        "\n",
        "        # Generate dynamic UPDATE clause for WHEN MATCHED\n",
        "        update_assignments = []\n",
        "        for field in expected_schema:\n",
        "            if field.name in raw_df.columns:\n",
        "                update_assignments.append(f\"target.{field.name} = source.{field.name}\")\n",
        "\n",
        "        df = raw_df.select(casted_cols)\n",
        "\n",
        "        # Generalized key detection\n",
        "        key_col = f\"{table_name}Id\"\n",
        "        if key_col in df.columns:\n",
        "            on_condition = f\"target.{key_col} = source.{key_col}\"\n",
        "        elif \"SourceId\" in df.columns and \"SourceSystemId\" in df.columns:\n",
        "            on_condition = \"target.SourceId = source.SourceId AND target.SourceSystemId = source.SourceSystemId\"\n",
        "        else:\n",
        "            raise ValueError(f\"‚ùå Cannot determine deduplication key for {table_name} (looked for '{key_col}' or ('SourceId','SourceSystemId'))\")\n",
        "\n",
        "        logging.info(f\"üíæ Writing to Lakehouse table: {full_table_name}\")\n",
        "\n",
        "        if not table_exists(full_table_name):\n",
        "            raise ValueError(f\"‚ùå Target table does not exist: {full_table_name}\")\n",
        "\n",
        "        # Upsert logic using Delta Lake MERGE\n",
        "        delta_table = DeltaTable.forName(spark, full_table_name)\n",
        "        staging_view = f\"staging_{table_name.lower()}\"\n",
        "        df.createOrReplaceTempView(staging_view)\n",
        "\n",
        "        merge_sql = f\"\"\"\n",
        "            MERGE INTO {full_table_name} AS target\n",
        "            USING {staging_view} AS source\n",
        "            ON {on_condition}\n",
        "            WHEN MATCHED THEN UPDATE SET {\", \".join(update_assignments)}\n",
        "            WHEN NOT MATCHED THEN INSERT *\n",
        "        \"\"\"\n",
        "\n",
        "        spark.sql(merge_sql)\n",
        "        logging.info(f\"üÜó Merge complete for {table_name}\")\n",
        "        logging.info(f\"‚úÖ Done: {table_name}\")\n",
        "        return True\n",
        "    \n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Failed to process {table_name}\")\n",
        "        if isinstance(e, ValueError):\n",
        "            logging.error(f\"Error: {e}\")\n",
        "        else:\n",
        "            logging.error(traceback.format_exc())\n",
        "        failures_list.append((table_name, str(e)))\n",
        "        return False\n",
        "\n",
        "def print_summary(total_files, failures_list):\n",
        "    \"\"\"\n",
        "    Print processing summary and handle failures.\n",
        "    \n",
        "    Args:\n",
        "        total_files: Total number of files processed\n",
        "        failures_list: List of failed tables\n",
        "    \"\"\"\n",
        "    logging.info(\"\\nüßæ Summary:\")\n",
        "    logging.info(f\"‚úîÔ∏è Successfully processed: {len(total_files) - len(failures_list)} tables\")\n",
        "    logging.info(f\"‚ùå Failed: {len(failures_list)} tables\")\n",
        "\n",
        "    if failures_list:\n",
        "        logging.info(\"\\nüìå Failures:\")\n",
        "        for tbl, err in failures_list:\n",
        "            logging.error(f\" - {tbl}: {err}\")\n",
        "        \n",
        "        # Fail the notebook if there were any failures\n",
        "        failure_summary = f\"Failed to process {len(failures_list)} table(s): {', '.join([tbl for tbl, _ in failures_list])}\"\n",
        "        raise Exception(f\"‚ùå Data import completed with failures. {failure_summary}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "if enable_import_sample_data:\n",
        "    # Initialize\n",
        "    logging.info(\"üöÄ Starting sample data import process...\")\n",
        "    \n",
        "    # Get CSV files\n",
        "    csv_files = get_csv_files(input_folder)\n",
        "    \n",
        "    logging.info(f\"üìÇ Found {len(csv_files)} CSV files to process\")\n",
        "    \n",
        "    # Track failures\n",
        "    failures = []\n",
        "    \n",
        "    # Process all CSV files\n",
        "    for file_path in csv_files:\n",
        "        process_table(file_path, failures)\n",
        "    \n",
        "    # Summary and cleanup\n",
        "    print_summary(csv_files, failures)"
      ]
    }
  ],
  "metadata": {
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "{SILVER_LAKEHOUSE_ID}",
        "default_lakehouse_name": "{SILVER_LAKEHOUSE_NAME}",
        "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
        "known_lakehouses": [
          {
            "id": "{SILVER_LAKEHOUSE_ID}"
          }
        ]
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
