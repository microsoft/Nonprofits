{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "403649a3-f735-4f3a-92ff-5f489c47e233",
			"metadata": {
				"editable": false,
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				},
				"tags": []
			},
			"outputs": [],
			"source": [
				"%run <Fundraising_SalesforceNPSP_Config>"
			]
		},
		{
			"cell_type": "markdown",
			"id": "41640c8a-29b2-49bf-99f6-b9585e99595b",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"source": [
				"## Parameters"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f2a2aa4f-d22e-49ce-8238-493c61add151",
			"metadata": {
				"editable": true,
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				},
				"tags": [
					"parameters"
				]
			},
			"outputs": [],
			"source": [
				"# Feature flags\n",
				"enable_delete_watermark_table = False\n",
				"enable_delete_data_tables = False\n",
				"enable_merge_data = True\n",
				"\n",
				"# Configuration\n",
				"main_table_names = [\n",
				"    \"Account\",\n",
				"    \"Address\",\n",
				"    \"Campaign\",\n",
				"    \"CampaignMember\",\n",
				"    \"Contact\",\n",
				"    \"EmailMessage\",\n",
				"    \"EmailMessageRelation\",\n",
				"    \"Event\",\n",
				"    \"Opportunity\",\n",
				"    \"OpportunityContactRole\",\n",
				"    \"OpportunityStage\",\n",
				"    \"RecordType\",\n",
				"    \"Task\",\n",
				"    \"VolunteerHours\",\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0a56f3ac-3c8e-4add-9ce5-3444f3ed991c",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"if enable_delete_watermark_table:\n",
				"    # Delete Watermark table\n",
				"    logging.info(f\"Deleting table: {WATERMARK_TABLE_NAME}\")\n",
				"    spark.sql(f\"DROP TABLE IF EXISTS {get_full_table_name(bronze_lakehouse_name, WATERMARK_TABLE_NAME)}\")\n",
				"    logging.info(f\"✅ Table deleted: {WATERMARK_TABLE_NAME}.\")\n",
				"\n",
				"if enable_delete_data_tables:\n",
				"    # Delete data tables\n",
				"    for table_name in main_table_names:\n",
				"        staging_table_name = f\"{table_name}_stg\"\n",
				"        logging.info(f\"Deleting tables: {table_name}, {staging_table_name}.\")\n",
				"        spark.sql(f\"DROP TABLE IF EXISTS {get_full_table_name(bronze_lakehouse_name, table_name)}\")\n",
				"        spark.sql(f\"DROP TABLE IF EXISTS {get_full_table_name(bronze_lakehouse_name, staging_table_name)}\")\n",
				"        logging.info(f\"✅ Tables deleted: {table_name}, {staging_table_name}.\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "942e8ce7-349f-4fac-a6c9-db4e257717a4",
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql import SparkSession, DataFrame\n",
				"from pyspark.sql.functions import col\n",
				"from delta.tables import DeltaTable\n",
				"from datetime import datetime\n",
				"import logging\n",
				"from typing import Tuple, Optional\n",
				"\n",
				"def process_table(target_table_name: str, key_column_name: str = \"Id\") -> Tuple[int, bool, Optional[datetime]]:\n",
				"    staging_table_name = f\"{target_table_name}_stg\"\n",
				"    staging_full_table_name = get_full_table_name(bronze_lakehouse_name, staging_table_name)\n",
				"    target_full_table_name = get_full_table_name(bronze_lakehouse_name, target_table_name)\n",
				"    \n",
				"    # Tables that do not have IsDeleted column\n",
				"    reference_tables  = [\"OpportunityStage\", \"RecordType\"]\n",
				"\n",
				"    try:\n",
				"        logging.info(f\"Processing data from {staging_full_table_name}\")\n",
				"        \n",
				"        # Read data from staging table\n",
				"        staging_df: DataFrame = get_bronze_table(staging_table_name)\n",
				"        staging_stats = staging_df.agg({\n",
				"            \"*\": \"count\",\n",
				"            \"SystemModstamp\": \"max\"\n",
				"        }).collect()[0]\n",
				"        staging_count: int = staging_stats[0]\n",
				"        staging_max_timestamp: Optional[datetime] = staging_stats[1]\n",
				"        has_watermark: bool = (\"SystemModstamp\" in staging_df.columns)\n",
				"        logging.info(f\"Read {staging_count} records from staging table: {staging_full_table_name}, with max_SystemModstamp: {staging_max_timestamp}.\")\n",
				"\n",
				"        # Check if target table exists\n",
				"        if not table_exists(target_full_table_name):\n",
				"            # Initial load - create target table and load data\n",
				"            logging.info(f\"Target table does not exist. Performing initial load\")\n",
				"\n",
				"            if \"IsDeleted\" in staging_df.columns and target_table_name not in reference_tables:\n",
				"                staging_df = staging_df.filter(col(\"IsDeleted\") == False)\n",
				"                logging.info(f\"Excluded deleted records from initial load for table: {target_table_name}\")\n",
				"\n",
				"            staging_df.write.option(\"delta.enableChangeDataFeed\", \"true\").format(\"delta\").mode(\"overwrite\").saveAsTable(target_full_table_name)\n",
				"\n",
				"            logging.info(f\"✅ Initial load completed successfully.\")\n",
				"\n",
				"        else:\n",
				"            # Incremental load\n",
				"            logging.info(f\"Target table exists. Performing incremental load\")\n",
				"            \n",
				"            target_df: DataFrame = get_bronze_table(target_table_name)\n",
				"            \n",
				"            # if the target doesn't have same columns -> resync all\n",
				"            if not have_same_columns(staging_df, target_df):\n",
				"                logging.warn(f\"⚠️ The target table {target_full_table_name} has different schema than {staging_full_table_name}. The target table will be overriden and resynchronize in a next run.\")\n",
				"\n",
				"                # Override target table (incl. schema)\n",
				"                staging_df.write.option(\"delta.enableChangeDataFeed\", \"true\").format(\"delta\").mode(\"overwrite\").saveAsTable(target_full_table_name)\n",
				"                \n",
				"                # Update last SystemModstamp to NULL\n",
				"                return (staging_count, has_watermark, None)\n",
				"            \n",
				"            # ---------- CASE 1: Reference Tables ----------\n",
				"            if target_table_name in reference_tables:\n",
				"                logging.info(f\"Processing reference table: {target_table_name}\")\n",
				"                staging_df.createOrReplaceTempView(\"staging_reference\")\n",
				"                spark.sql(f\"\"\"\n",
				"                    MERGE INTO {target_full_table_name} AS main\n",
				"                    USING staging_reference AS staging\n",
				"                    ON main.{key_column_name} = staging.{key_column_name}\n",
				"                    WHEN MATCHED THEN UPDATE SET *\n",
				"                    WHEN NOT MATCHED THEN INSERT *\n",
				"                \"\"\")\n",
				"                logging.info(f\"✅ Full MERGE completed for metadata table: {target_table_name}\")\n",
				"\n",
				"            # ---------- CASE 2: Transactional tables ----------\n",
				"            else:\n",
				"                # Split staging into active/deleted\n",
				"                active_staging_df = staging_df.filter(col(\"IsDeleted\") == False)\n",
				"                deleted_staging_df = staging_df.filter(col(\"IsDeleted\") == True)\n",
				"\n",
				"                # Upsert active records\n",
				"                active_staging_df.createOrReplaceTempView(\"staging_active\")\n",
				"                spark.sql(f\"\"\"\n",
				"                    MERGE INTO {target_full_table_name} AS main\n",
				"                    USING staging_active AS staging\n",
				"                    ON main.{key_column_name} = staging.{key_column_name}\n",
				"                    WHEN MATCHED THEN UPDATE SET *\n",
				"                    WHEN NOT MATCHED THEN INSERT *\n",
				"                \"\"\")\n",
				"                logging.info(f\"✅ Upsert of active records completed for: {target_table_name}\")\n",
				"\n",
				"                # Delete deleted records\n",
				"                if deleted_staging_df.count() > 0:\n",
				"                    delta_target_table = DeltaTable.forName(spark, target_full_table_name)\n",
				"                    delta_target_table.alias(\"target\").merge(\n",
				"                        deleted_staging_df.alias(\"staging\"),\n",
				"                        f\"target.{key_column_name} = staging.{key_column_name}\"\n",
				"                    ).whenMatchedDelete().execute()\n",
				"                    logging.info(f\"✅ Deleted {deleted_staging_df.count()} records from: {target_table_name}\")\n",
				"\n",
				"        return (staging_count, has_watermark, staging_max_timestamp)\n",
				"    except Exception as e:\n",
				"        logging.error(f\"⛔ Error processing {staging_full_table_name}: {str(e)}\")\n",
				"        raise\n",
				"\n",
				"def add_watermark(table_name: str, has_watermark: bool, latest_watermark: None | datetime):\n",
				"    try:\n",
				"        if has_watermark:\n",
				"            spark.sql(f\"\"\"\n",
				"                MERGE INTO {WATERMARK_TABLE_NAME} target\n",
				"                USING (\n",
				"                    SELECT '{table_name}' AS ObjectName,\n",
				"                            {(\"NULL\" if latest_watermark is None else f\"TIMESTAMP('{latest_watermark}')\" )} AS LastWatermark,\n",
				"                            current_timestamp() AS UpdatedAt\n",
				"                ) source\n",
				"                ON target.ObjectName = source.ObjectName\n",
				"                WHEN MATCHED THEN UPDATE SET LastWatermark = source.LastWatermark, UpdatedAt = source.UpdatedAt\n",
				"                WHEN NOT MATCHED THEN INSERT *\n",
				"            \"\"\")\n",
				"            logging.info(f\"✅ Watermark for table: '{table_name}' updated to {latest_watermark}\")\n",
				"        else:\n",
				"            logging.info(f\"⚠️ `SystemModstamp` column not found for table: '{table_name}' — skipping watermark update.\")\n",
				"\n",
				"    except Exception as e:\n",
				"        logging.error(f\"⛔ Error adding watermark for {table_name}: {str(e)}\")\n",
				"        raise\n",
				"\n",
				"if enable_merge_data:\n",
				"    # Process each table\n",
				"    for table_name in main_table_names:\n",
				"        logging.info(f\"Starting processing for table: {table_name}.\")\n",
				"\n",
				"        updated_records_count, has_watermark, max_watermark = process_table(table_name)\n",
				"        add_watermark(table_name, has_watermark, max_watermark)\n",
				"        \n",
				"        logging.info(f\"✅ Completed processing for table: {table_name}.\")\n",
				"        logging.info(f\"-------------------------------------------\")"
			]
		}
	],
	"metadata": {
		"dependencies": {
			"lakehouse": {
				"default_lakehouse": "{BRONZE_LAKEHOUSE_ID}",
				"default_lakehouse_name": "{BRONZE_LAKEHOUSE_NAME}",
				"default_lakehouse_workspace_id": "{WORKSPACE_ID}",
				"known_lakehouses": [
					{
						"id": "{BRONZE_LAKEHOUSE_ID}"
					}
				]
			}
		},
		"kernel_info": {
			"name": "synapse_pyspark"
		},
		"kernelspec": {
			"display_name": "synapse_pyspark",
			"name": "synapse_pyspark"
		},
		"language_info": {
			"name": "python"
		},
		"microsoft": {
			"language": "python",
			"language_group": "synapse_pyspark",
			"ms_spell_check": {
				"ms_spell_check_language": "en"
			}
		},
		"nteract": {
			"version": "nteract-front-end@1.0.0"
		},
		"spark_compute": {
			"compute_id": "/trident/default",
			"session_options": {
				"conf": {
					"spark.synapse.nbs.session.timeout": "1200000"
				}
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
